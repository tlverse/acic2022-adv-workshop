[{"path":"index.html","id":"welcome","chapter":"Welcome!","heading":"Welcome!","text":"open source, reproducible vignette full-day workshop \nTargeted Learning framework statistical causal inference machine\nlearning. Beyond introducing Targeted Learning, workshop focuses \napplying methodology practice using tlverse software\necosystem. materials based working\ndraft book Targeted Learning R: Causal Data Science \ntlverse Software Ecosystem, \nincludes -depth discussion topics much , may serve \nuseful reference accompany workshop materials.\n","code":""},{"path":"index.html","id":"important-links","chapter":"Welcome!","heading":"Important links","text":"Load R environment: Please set R virtual environment using theinstructions.Load R environment: Please set R virtual environment using theinstructions.probably exceed GitHub API rate limit installation,\nthrow error. issue solution addressed\n.probably exceed GitHub API rate limit installation,\nthrow error. issue solution addressed\n.Code: R script files section workshop available via\nGitHub repository workshop \nhttps://github.com/tlverse/tlverse-workshops/tree/master/R_codeCode: R script files section workshop available via\nGitHub repository workshop \nhttps://github.com/tlverse/tlverse-workshops/tree/master/R_code","code":""},{"path":"index.html","id":"about-this-workshop","chapter":"Welcome!","heading":"About this workshop","text":"workshop provide comprehensive overview field \nTargeted Learning causal inference, corresponding tlverse software\necosystem, particular focus placed \nsophisticated intervention regimes (dynamic, optimal dynamic, stochastic).\nEmphasis placed targeted minimum loss-based estimators causal\neffects single timepoint interventions, including extensions missing\ncovariate outcome data. multiply robust, efficient plug-estimators\nuse state---art, ensemble machine learning tools flexibly adjust \nconfounding yielding valid statistical inference.addition discussion, workshop incorporate interactive\nactivities hands-, guided R programming exercises, allow participants\nopportunity familiarize methodology tools \ntranslate real-world data analysis. highly recommended participants\nunderstanding basic statistical concepts confounding,\nprobability distributions, confidence intervals, hypothesis testing, \nregression. Advanced knowledge mathematical statistics useful \nnecessary. Familiarity R programming language essential.","code":""},{"path":"index.html","id":"outline","chapter":"Welcome!","heading":"Outline","text":"9–10: Intro talk Mark Alan\n10–11: sl3\n11–12: tmle3mopttx\n12-1: lunch\n1-2: tmle3shift\n2-2:30: coffee break\n2:30-3:30: tmle3mediate\n3:30-4: Q&\n\n\n\n* 09:00-10:00A: Introduction Targeted Learning\n* 10:00-11:00A: Super learning sl3 R\npackage\n* 11:00-12:00P: Optimal treatment regimes tmle3mopttx R\npackage\n* 12:00-01:00P: Lunch Break\n* 1:00-02:00P: Stochastic treatment regimes tmle3shift R\npackage\n* 02:00-02:30P: Coffee Break\n* 02:30-03:15P: Causal mediation analysis tmle3mediate R\npackage\n* 03:15-04:00P: Concluding remarks discussion","code":""},{"path":"index.html","id":"about-the-instructors","chapter":"Welcome!","heading":"About the instructors","text":"","code":""},{"path":"index.html","id":"mark-van-der-laan","chapter":"Welcome!","heading":"Mark van der Laan","text":"Mark van der Laan, PhD, Professor Biostatistics Statistics UC\nBerkeley. research interests include statistical methods computational\nbiology, survival analysis, censored data, adaptive designs, targeted maximum\nlikelihood estimation, causal inference, data-adaptive loss-based learning, \nmultiple testing. research group developed loss-based super learning \nsemiparametric models, based cross-validation, generic optimal tool \nestimation infinite-dimensional parameters, nonparametric density\nestimation prediction censored uncensored data. Building \nwork, research group developed targeted maximum likelihood estimation\ntarget parameter data-generating distribution arbitrary\nsemiparametric nonparametric models, generic optimal methodology \nstatistical causal inference. recently, Mark’s group focused \npart development centralized, principled set software tools \ntargeted learning, tlverse.","code":""},{"path":"index.html","id":"alan-hubbard","chapter":"Welcome!","heading":"Alan Hubbard","text":"Alan Hubbard Professor Biostatistics, former head Division \nBiostatistics UC Berkeley, head data analytics core UC Berkeley’s\nSuperFund research program. current research interests include causal\ninference, variable importance analysis, statistical machine learning,\nestimation inference data-adaptive statistical target parameters, \ntargeted minimum loss-based estimation. Research group generally\nmotivated applications problems computational biology, epidemiology,\nprecision medicine.","code":""},{"path":"index.html","id":"jeremy-coyle","chapter":"Welcome!","heading":"Jeremy Coyle","text":"Jeremy Coyle, PhD, consulting data scientist statistical programmer,\ncurrently leading software development effort produced \ntlverse ecosystem R packages related software tools. Jeremy earned \nPhD Biostatistics UC Berkeley 2016, primarily supervision\nAlan Hubbard.","code":""},{"path":"index.html","id":"nima-hejazi","chapter":"Welcome!","heading":"Nima Hejazi","text":"Nima Hejazi, PhD, incoming Assistant Professor\nBiostatistics Harvard T.H. Chan School Public\nHealth. received PhD \nbiostatistics UC Berkeley, working supervision Mark van der Laan\nAlan Hubbard, afterwards held NSF postdoctoral research fellowship.\nNima’s research interests blend causal inference, machine learning,\nsemiparametric estimation, computational statistics – areas recent\nemphasis include causal mediation analysis, efficiency biased sampling\ndesigns, non/semi-parametric sieve estimation machine learning, \ntargeted loss-based estimation. work primarily driven applications \nclinical trials (esp. vaccine efficacy trials), infectious disease epidemiology,\ncomputational biology. Nima passionate statistical computing\nopen source software design standards statistical data science, \nco-led contributed significantly many tlverse packages (hal9001,\nsl3, tmle3, origami, tmle3shift, tmle3mediate).","code":""},{"path":"index.html","id":"ivana-malenica","chapter":"Welcome!","heading":"Ivana Malenica","text":"Ivana Malenica PhD student biostatistics advised Mark van der Laan.\nIvana currently fellow Berkeley Institute Data Science, \nserving NIH Biomedical Big Data Freeport-McMoRan Genomic Engine fellow.\nearned Master’s Biostatistics Bachelor’s Mathematics, \nspent time Translational Genomics Research Institute. broadly,\nresearch interests span non/semi-parametric theory, probability theory,\nmachine learning, causal inference high-dimensional statistics. \ncurrent work involves complex dependent settings (dependence time \nnetwork) adaptive sequential designs.","code":""},{"path":"index.html","id":"rachael-phillips","chapter":"Welcome!","heading":"Rachael Phillips","text":"Rachael Phillips PhD student biostatistics, advised Alan Hubbard \nMark van der Laan. MA Biostatistics, BS Biology, BA \nMathematics. student targeted learning causal inference, Rachael’s\nresearch integrates semiparametric statistical estimation inference. \nmotivated applied projects current work involves personalized\nonline learning data streams vital signs, human-computer interaction,\nautomated machine learning, developing statistical analysis plans using\ntargeted learning.","code":""},{"path":"index.html","id":"repro","chapter":"Welcome!","heading":"0.1 Reproduciblity with the tlverse","text":"tlverse software ecosystem growing collection packages, several \nquite early software lifecycle. team best \nmaintain backwards compatibility. work reaches completion, \nspecific versions tlverse packages used archived tagged \nproduce .book written using bookdown, complete\nsource available GitHub.\nversion book built R version 4.2.0 (2022-04-22),\npandoc version 2.7.3, \nfollowing packages:","code":""},{"path":"index.html","id":"setup","chapter":"Welcome!","heading":"0.2 Setup instructions","text":"","code":""},{"path":"index.html","id":"r-and-rstudio","chapter":"Welcome!","heading":"0.2.1 R and RStudio","text":"R RStudio separate downloads installations. R \nunderlying statistical computing environment. RStudio graphical integrated\ndevelopment environment (IDE) makes using R much easier \ninteractive. need install R install RStudio.","code":""},{"path":"index.html","id":"windows","chapter":"Welcome!","heading":"0.2.1.1 Windows","text":"","code":""},{"path":"index.html","id":"if-you-already-have-r-and-rstudio-installed","chapter":"Welcome!","heading":"0.2.1.1.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears console indicates version R \nrunning. Alternatively, can type sessionInfo(), also display\nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install . \ncan check \ninformation remove old versions system \nwish .","code":""},{"path":"index.html","id":"if-you-dont-have-r-and-rstudio-installed","chapter":"Welcome!","heading":"0.2.1.1.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Run .exe file just downloadedGo RStudio download pageUnder Installers select RStudio x.yy.zzz - Windows\nXP/Vista/7/8 (x, y, z represent version numbers)Double click file install itOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"index.html","id":"macos-mac-os-x","chapter":"Welcome!","heading":"0.2.1.2 macOS / Mac OS X","text":"","code":""},{"path":"index.html","id":"if-you-already-have-r-and-rstudio-installed-1","chapter":"Welcome!","heading":"0.2.1.2.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears terminal indicates version R running.\nAlternatively, can type sessionInfo(), also display \nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install .","code":""},{"path":"index.html","id":"if-you-dont-have-r-and-rstudio-installed-1","chapter":"Welcome!","heading":"0.2.1.2.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Select .pkg file latest R versionDouble click downloaded file install RIt also good idea install XQuartz (needed\npackages)Go RStudio download\npageUnder Installers select RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit)\n(x, y, z represent version numbers)Double click file install RStudioOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"index.html","id":"linux","chapter":"Welcome!","heading":"0.2.1.3 Linux","text":"Follow instructions distribution\nCRAN, provide information\nget recent version R common distributions. \ndistributions, use package manager (e.g., Debian/Ubuntu run\nsudo apt-get install r-base, Fedora sudo yum install R), \ndon’t recommend approach versions provided \nusually date. case, make sure least R 3.3.1.Go RStudio download\npageUnder Installers select version matches distribution, \ninstall preferred method (e.g., Debian/Ubuntu sudo dpkg -rstudio-x.yy.zzz-amd64.deb terminal).’s installed, open RStudio make sure works don’t get \nerror messages.setup instructions adapted written Data Carpentry: R\nData Analysis Visualization Ecological\nData.","code":""},{"path":"motivation.html","id":"motivation","chapter":"Motivation","heading":"Motivation","text":"“One enemy robust science humanity — appetite \nright, tendency find patterns noise, see supporting\nevidence already believe true, ignore facts \nfit.”— Nature Editorial (2015b)Scientific research unique point history. need improve rigor\nreproducibility field greater ever; corroboration moves\nscience forward, yet growing alarm results \nreproduced report false discoveries (Baker 2016). Consequences \nmeeting need result decline rate scientific\nprogression, reputation sciences, public’s trust \nfindings (Munafò et al. 2017; Nature Editorial 2015a).“key question want answer seeing results scientific\nstudy whether can trust data analysis.”— Peng (2015)Unfortunately, current state culture data analysis statistics\nactually enables human bias improper model selection. hypothesis\ntests estimators derived statistical models, obtain valid\nestimates inference critical statistical model contains \nprocess generated data. Perhaps treatment randomized \ndepended small number baseline covariates; knowledge \ncan incorporated model. Alternatively, maybe data \nobservational, knowledge data-generating process (DGP).\ncase, statistical model contain data\ndistributions. practice; however, models selected based knowledge\nDGP, instead models often selected based (1) p-values \nyield, (2) convenience implementation, /(3) analysts loyalty\nparticular model. practice “cargo-cult statistics — \nritualistic miming statistics rather conscientious practice,”\n(Stark Saltelli 2018) characterized arbitrary modeling choices, even though\nchoices often result different answers research question.\n, “increasingly often, [statistics] used instead aid \nabet weak science, role can perform well used mechanically \nritually,” opposed original purpose safeguarding weak\nscience (Stark Saltelli 2018). presents fundamental drive behind epidemic\nfalse findings scientific research suffering (van der Laan Starmans 2014).“suggest weak statistical understanding probably due \ninadequate”statistics lite\" education. approach build \nappropriate mathematical fundamentals provide scientifically\nrigorous introduction statistics. Hence, students’ knowledge may remain\nimprecise, patchy, prone serious misunderstandings. approach\nachieves, however, providing students false confidence able\nuse inferential tools whereas usually interpret p-value\nprovided black box statistical software. educational problem\nremains unaddressed, poor statistical practices prevail regardless \nprocedures measures may favored /banned editorials.\"— Szucs Ioannidis (2017)team University California, Berkeley, uniquely positioned \nprovide education. Spearheaded Professor Mark van der Laan, \nspreading rapidly many students colleagues greatly\nenriched field, aptly named “Targeted Learning” methodology targets \nscientific question hand counter current culture \n“convenience statistics” opens door biased estimation, misleading\nresults, false discoveries. Targeted Learning restores fundamentals \nformalized field statistics, facts statistical\nmodel represents real knowledge experiment generated data,\ntarget parameter represents seeking learn data \nfeature distribution generated (van der Laan Starmans 2014). way,\nTargeted Learning defines truth establishes principled standard \nestimation, thereby inhibiting --human biases (e.g., hindsight bias,\nconfirmation bias, outcome bias) infiltrating analysis.“key effective classical [statistical] inference \nwell-defined questions analysis plan tests questions.”— Nosek et al. (2018)objective provide training students, researchers, industry professionals, faculty science, public health, statistics, \nfields empower necessary knowledge skills utilize \nsound methodology Targeted Learning — technique provides tailored\npre-specified machines answering queries, data analysis \ncompletely reproducible, estimators efficient, minimally biased, \nprovide formal statistical inference.Just conscientious use modern statistical methodology necessary \nensure scientific practice thrives, remains critical acknowledge \nrole robust software plays allowing practitioners direct access \npublished results. recall “article…scientific publication \nscholarship , merely advertising scholarship. \nactual scholarship complete software development environment \ncomplete set instructions generated figures,” thus making \navailability adoption robust statistical software key enhancing \ntransparency inherent aspect science (Buckheit Donoho 1995).statistical methodology readily accessible practice, \ncrucial accompanied robust user-friendly software\n(Pullenayegum et al. 2016; Stromberg others 2004). tlverse software\necosystem developed fulfill need Targeted Learning\nmethodology. software facilitate computationally reproducible\nefficient analyses, also tool Targeted Learning education since\nworkflow mirrors methodology. particular, tlverse\nparadigm focus implementing specific estimator small set \nrelated estimators. Instead, focus exposing statistical framework\nTargeted Learning — R packages tlverse ecosystem\ndirectly model key objects defined mathematical theoretical\nframework Targeted Learning. ’s , tlverse R packages share \ncore set design principles centered extensibility, allowing \nused conjunction built upon one cohesive\nfashion.workshop, reader embark journey tlverse\necosystem. Guided R programming exercises, case studies, \nintuitive explanation readers build toolbox applying Targeted\nLearning statistical methodology, translate real-world causal\ninference analyses. Participants need fully trained statistician \nbegin understanding applying methods. However, highly\nrecommended participants understanding basic statistical\nconcepts confounding, probability distributions, confidence intervals,\nhypothesis tests, regression. Advanced knowledge mathematical statistics\nmay useful necessary. Familiarity R programming\nlanguage essential. also recommend understanding introductory\ncausal inference.introductory materials learning R programming language recommend following free resources:Software Carpentry’s Programming \nRSoftware Carpentry’s R Reproducible Scientific\nAnalysisGrolemund Wickham’s R Data\nScienceFor causal inference learning materials recommend following resources:Hernán MA, Robins JM (2019). Causal\nInference.Jason . Roy’s coursera Course Crash Course Causality: Inferring\nCausal Effects Observational Data","code":""},{"path":"tlverse.html","id":"tlverse","chapter":"1 Welcome to the tlverse","heading":"1 Welcome to the tlverse","text":"","code":""},{"path":"tlverse.html","id":"learning-objectives","chapter":"1 Welcome to the tlverse","heading":"1.1 Learning Objectives","text":"Understand tlverse ecosystem conceptuallyIdentify core components tlverseInstall tlverse R packagesUnderstand Targeted Learning roadmapLearn WASH Benefits example data","code":""},{"path":"tlverse.html","id":"what-is-the-tlverse","chapter":"1 Welcome to the tlverse","heading":"1.2 What is the tlverse?","text":"tlverse new framework Targeted Learning R, inspired \ntidyverse ecosystem R packages.analogy tidyverse:tidyverse opinionated collection R packages designed data\nscience. packages share underlying design philosophy, grammar, data\nstructures., tlverse isan opinionated collection R packages Targeted Learningsharing underlying philosophy, grammar, set data structures","code":""},{"path":"tlverse.html","id":"tlverse-components","chapter":"1 Welcome to the tlverse","heading":"1.3 tlverse components","text":"main packages represent core tlverse:sl3: Modern Super Learning Pipelines\n? modern object-oriented re-implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.\n? design leverages modern tools fast computation, \nforward-looking, can form one cornerstones tlverse.\n? modern object-oriented re-implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.? design leverages modern tools fast computation, \nforward-looking, can form one cornerstones tlverse.tmle3: Engine Targeted Learning\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.\n? common interface engine accommodates current algorithmic\napproaches Targeted Learning still flexible enough remain \nengine even new techniques developed.\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.? common interface engine accommodates current algorithmic\napproaches Targeted Learning still flexible enough remain \nengine even new techniques developed.addition engines drive development tlverse, \nsupporting packages – particular, two…origami: Generalized Framework \nCross-Validation\n? generalized framework flexible cross-validation\n? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner algorithm Targeted Learning.\n? generalized framework flexible cross-validationWhy? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner algorithm Targeted Learning.delayed: Parallelization Framework \nDependent Tasks\n? framework delayed computations (futures) based task\ndependencies.\n? Efficient allocation compute resources essential deploying\nlarge-scale, computationally intensive algorithms.\n? framework delayed computations (futures) based task\ndependencies.? Efficient allocation compute resources essential deploying\nlarge-scale, computationally intensive algorithms.key principle tlverse extensibility. , want support\nnew Targeted Learning estimators developed. model \nnew estimators implemented additional packages using core packages\n. currently two featured examples :tmle3mopttx: Optimal Treatments\ntlverse\n? Learn optimal rule estimate mean outcome rule\n? Optimal Treatment powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.\n? Learn optimal rule estimate mean outcome ruleWhy? Optimal Treatment powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.tmle3shift: Shift Interventions \ntlverse\n? Shift interventions continuous treatments\n? treatment variables discrete. able estimate \neffects continuous treatment represents powerful extension \nTargeted Learning approach.\n? Shift interventions continuous treatmentsWhy? treatment variables discrete. able estimate \neffects continuous treatment represents powerful extension \nTargeted Learning approach.","code":""},{"path":"tlverse.html","id":"installtlverse","chapter":"1 Welcome to the tlverse","heading":"1.4 Installation","text":"tlverse ecosystem packages currently hosted \nhttps://github.com/tlverse, yet CRAN. \ncan use devtools package install :tlverse depends large number packages also hosted\nGitHub. , may see following error:just means R tried install many packages GitHub \nshort window. fix , need tell R use GitHub \nuser (’ll need GitHub user account). Follow two steps:Type usethis::browse_github_pat() R console, direct\nGitHub’s page create New Personal Access Token.Create Personal Access Token simply clicking “Generate token” \nbottom page.Copy Personal Access Token, long string lowercase letters \nnumbers.Type usethis::edit_r_environ() R console, open \n.Renviron file source window RStudio. able \naccess .Renviron file command, try inputting\nSys.setenv(GITHUB_PAT = ) Personal Access Token inserted \nstring equals symbol; error, skip \nstep 8..Renviron file, type GITHUB_PAT= paste Personal\nAccess Token equals symbol space..Renviron file, press enter key ensure .Renviron\nends newline.Save .Renviron file.Restart R changes take effect. can restart R via drop-\nmenu “Session” tab. “Session” tab top RStudio\ninterface.following steps, able successfully install \npackage threw error .","code":"\ninstall.packages(\"devtools\")\ndevtools::install_github(\"tlverse/tlverse\")Error: HTTP error 403.\n  API rate limit exceeded for 71.204.135.82. (But here's the good news:\n  Authenticated requests get a higher rate limit. Check out the documentation\n  for more details.)\n\n  Rate limit remaining: 0/60\n  Rate limit reset at: 2019-03-04 19:39:05 UTC\n\n  To increase your GitHub API rate limit\n  - Use `usethis::browse_github_pat()` to create a Personal Access Token.\n  - Use `usethis::edit_r_environ()` and add the token as `GITHUB_PAT`."},{"path":"intro.html","id":"intro","chapter":"2 The Roadmap for Targeted Learning","heading":"2 The Roadmap for Targeted Learning","text":"","code":""},{"path":"intro.html","id":"learning-objectives-1","chapter":"2 The Roadmap for Targeted Learning","heading":"2.1 Learning Objectives","text":"end chapter able :Translate scientific questions statistical questions.Define statistical model based knowledge experiment \ngenerated data.Identify causal parameter function observed data distribution.Explain following causal statistical assumptions \nimplications: ..d., consistency, interference, positivity, SUTVA.","code":""},{"path":"intro.html","id":"introduction","chapter":"2 The Roadmap for Targeted Learning","heading":"2.2 Introduction","text":"roadmap statistical learning concerned translation \nreal-world data applications mathematical statistical formulation \nrelevant estimation problem. involves data random variable \nprobability distribution, scientific knowledge represented statistical\nmodel, statistical target parameter representing answer question \ninterest, notion estimator sampling distribution \nestimator.","code":""},{"path":"intro.html","id":"the-roadmap","chapter":"2 The Roadmap for Targeted Learning","heading":"2.3 The Roadmap","text":"Following roadmap process five stages.Data random variable probability distribution, \\(O \\sim P_0\\).statistical model \\(\\mathcal{M}\\) \\(P_0 \\\\mathcal{M}\\).statistical target parameter \\(\\Psi\\) estimand \\(\\Psi(P_0)\\).estimator \\(\\hat{\\Psi}\\) estimate \\(\\hat{\\Psi}(P_n)\\).measure uncertainty estimate \\(\\hat{\\Psi}(P_n)\\).","code":""},{"path":"intro.html","id":"data-as-a-random-variable-with-a-probability-distribution-o-sim-p_0","chapter":"2 The Roadmap for Targeted Learning","heading":"(1) Data as a random variable with a probability distribution, \\(O \\sim P_0\\)","text":"data set ’re confronted result experiment can\nview data random variable, \\(O\\), repeat experiment\ndifferent realization experiment. particular, \nrepeat experiment many times learn probability distribution,\n\\(P_0\\), data. , observed data \\(O\\) probability distribution\n\\(P_0\\) \\(n\\) independent identically distributed (..d.) observations \nrandom variable \\(O; O_1, \\ldots, O_n\\). Note data ..d.,\nways handle non-..d. data, establishing conditional\nindependence, stratifying data create sets identically distributed data,\netc. crucial researchers absolutely clear actually\nknow data-generating distribution given problem interest.\nUnfortunately, communication statisticians researchers often\nfraught misinterpretation. roadmap provides mechanism \nensure clear communication research statistician – truly helps\ncommunication!","code":""},{"path":"intro.html","id":"the-empirical-probability-measure-p_n","chapter":"2 The Roadmap for Targeted Learning","heading":"The empirical probability measure, \\(P_n\\)","text":"\\(n\\) ..d. observations empirical probability\nmeasure, \\(P_n\\). empirical probability measure approximation \ntrue probability measure \\(P_0\\), allowing us learn data. \nexample, can define empirical probability measure set, \\(\\), \nproportion observations end \\(\\). ,\n\\[\\begin{equation*}\n  P_n() = \\frac{1}{n}\\sum_{=1}^{n} \\mathbb{}(O_i \\)\n\\end{equation*}\\]order start learning something, need ask “know \nprobability distribution data?” brings us Step 2.","code":""},{"path":"intro.html","id":"the-statistical-model-mathcalm-such-that-p_0-in-mathcalm","chapter":"2 The Roadmap for Targeted Learning","heading":"(2) The statistical model \\(\\mathcal{M}\\) such that \\(P_0 \\in \\mathcal{M}\\)","text":"statistical model \\(\\mathcal{M}\\) defined question asked \nend . defined set possible probability\ndistributions observed data. Often \\(\\mathcal{M}\\) large (possibly\ninfinite-dimensional), reflect fact statistical knowledge \nlimited. case \\(\\mathcal{M}\\) infinite-dimensional, deem \nnonparametric statistical model.Alternatively, probability distribution data hand described\nfinite number parameters, statistical model parametric. \ncase, prescribe belief random variable \\(O\\) \nobserved , e.g., normal distribution mean \\(\\mu\\) variance\n\\(\\sigma^2\\). formally, parametric model may defined\n\\[\\begin{equation*}\n  \\mathcal{M} = \\{P_{\\theta} : \\theta \\\\mathcal{R}^d \\}\n\\end{equation*}\\]Sadly, assumption data-generating distribution specific,\nparametric forms --common, even leap faith. \npractice oversimplification current culture data analysis typically\nderails attempt trying answer scientific question hand; alas,\nstatements ever-popular quip Box “models wrong \nuseful,” encourage data analyst make arbitrary choices even \noften force significant differences answers estimation\nproblem. Targeted Learning paradigm suffer bias since \ndefines statistical model representation true\ndata-generating distribution corresponding observed data.Now, Step 3: ``trying learn data?\"","code":""},{"path":"intro.html","id":"the-statistical-target-parameter-psi-and-estimand-psip_0","chapter":"2 The Roadmap for Targeted Learning","heading":"(3) The statistical target parameter \\(\\Psi\\) and estimand \\(\\Psi(P_0)\\)","text":"statistical target parameter, \\(\\Psi\\), defined mapping \nstatistical model, \\(\\mathcal{M}\\), parameter space (.e., real number)\n\\(\\mathcal{R}\\). , \\(\\Psi: \\mathcal{M}\\rightarrow\\mathbb{R}\\). target\nparameter may seen representation \nquantity wish learn data, answer well-specified\n(often causal) question interest. contrast purely statistical target\nparameters, causal target parameters require identification observed\ndata, based causal models include several untestable assumptions,\ndescribed detail section causal target parameters.simple example, consider data set contains observations \nsurvival time every subject, question interest “’s\nprobability someone lives longer five years?” ,\n\\[\\begin{equation*}\n  \\Psi(P_0) = \\mathbb{P}(O > 5)\n\\end{equation*}\\]answer question estimand, \\(\\Psi(P_0)\\), \nquantity ’re trying learn data. defined \\(O\\),\n\\(\\mathcal{M}\\) \\(\\Psi(P_0)\\) formally defined statistical\nestimation problem.","code":""},{"path":"intro.html","id":"the-estimator-hatpsi-and-estimate-hatpsip_n","chapter":"2 The Roadmap for Targeted Learning","heading":"(4) The estimator \\(\\hat{\\Psi}\\) and estimate \\(\\hat{\\Psi}(P_n)\\)","text":"obtain good approximation estimand, need estimator, \npriori-specified algorithm defined mapping set possible\nempirical distributions, \\(P_n\\), live non-parametric statistical\nmodel, \\(\\mathcal{M}_{NP}\\) (\\(P_n \\\\mathcal{M}_{NP}\\)), parameter space\nparameter interest. , \\(\\hat{\\Psi} : \\mathcal{M}_{NP} \\rightarrow \\mathbb{R}^d\\). estimator function takes input\nobserved data, realization \\(P_n\\), gives output value \nparameter space, estimate, \\(\\hat{\\Psi}(P_n)\\).estimator may seen operator maps observed data \ncorresponding empirical distribution value parameter space, \nnumerical output produced function estimate. Thus, \nelement parameter space based empirical probability distribution\nobserved data. plug realization \\(P_n\\) (based sample\nsize \\(n\\) random variable \\(O\\)), get back estimate \\(\\hat{\\Psi}(P_n)\\)\ntrue parameter value \\(\\Psi(P_0)\\).order quantify uncertainty estimate target parameter\n(.e., construct statistical inference), understanding sampling\ndistribution estimator necessary. brings us Step 5.","code":""},{"path":"intro.html","id":"a-measure-of-uncertainty-for-the-estimate-hatpsip_n","chapter":"2 The Roadmap for Targeted Learning","heading":"(5) A measure of uncertainty for the estimate \\(\\hat{\\Psi}(P_n)\\)","text":"Since estimator \\(\\hat{\\Psi}\\) function empirical\ndistribution \\(P_n\\), estimator random variable sampling\ndistribution. , repeat experiment drawing \\(n\\) observations \nevery time end different realization estimate \nestimator sampling distribution. sampling distribution estimator\ncan theoretically validated approximately normally distributed \nCentral Limit Theorem (CLT).class Central Limit Theorems (CLTs) statements regarding \nconvergence sampling distribution estimator normal\ndistribution. general, construct estimators whose limit sampling\ndistributions may shown approximately normal distributed sample size\nincreases. large enough \\(n\\) ,\n\\[\\begin{equation*}\n  \\hat{\\Psi}(P_n) \\sim N \\left(\\Psi(P_0), \\frac{\\sigma^2}{n}\\right),\n\\end{equation*}\\]\npermitting statistical inference. Now, can proceed quantify \nuncertainty chosen estimator construction hypothesis tests \nconfidence intervals. example, may construct confidence interval \nlevel \\((1 - \\alpha)\\) estimand, \\(\\Psi(P_0)\\):\n\\[\\begin{equation*}\n  \\hat{\\Psi}(P_n) \\pm z_{1 - \\frac{\\alpha}{2}}\n    \\left(\\frac{\\sigma}{\\sqrt{n}}\\right),\n\\end{equation*}\\]\n\\(z_{1 - \\frac{\\alpha}{2}}\\) \\((1 - \\frac{\\alpha}{2})^\\text{th}\\)\nquantile standard normal distribution. Often, interested \nconstructing 95% confidence intervals, corresponding mass \\(\\alpha = 0.05\\) \neither tail limit distribution; thus, typically take\n\\(z_{1 - \\frac{\\alpha}{2}} \\approx 1.96\\).Note: typically estimate standard error,\n\\(\\frac{\\sigma}{\\sqrt{n}}\\).95% confidence interval means take 100 different samples\nsize \\(n\\) compute 95% confidence interval sample \napproximately 95 100 confidence intervals contain estimand,\n\\(\\Psi(P_0)\\). practically, means 95% probability\n(95% confidence) confidence interval procedure contain \ntrue estimand. However, single estimated confidence interval either \ncontain true estimand .","code":""},{"path":"intro.html","id":"summary-of-the-roadmap","chapter":"2 The Roadmap for Targeted Learning","heading":"2.4 Summary of the Roadmap","text":"Data, \\(O\\), viewed random variable probability distribution.\noften \\(n\\) units independent identically distributed units \nprobability distribution \\(P_0\\) (\\(O_1, \\ldots, O_n \\sim P_0\\)). \nstatistical knowledge experiment generated data. \nwords, make statement true data distribution \\(P_0\\) falls \ncertain set called statistical model, \\(\\mathcal{M}\\). Often sets \nlarge statistical knowledge limited statistical models\noften infinite dimensional models. statistical query , “\ntrying learn data?” denoted statistical target parameter,\n\\(\\Psi\\), maps \\(P_0\\) estimand, \\(\\Psi(P_0)\\). point \nstatistical estimation problem formally defined now need\nstatistical theory guide us construction estimators. ’s lot\nstatistical theory review course , particular, relies\nCentral Limit Theorem, allowing us come estimators \napproximately normally distributed also allowing us come statistical\ninference (.e., confidence intervals hypothesis tests).","code":""},{"path":"intro.html","id":"causal","chapter":"2 The Roadmap for Targeted Learning","heading":"2.5 Causal Target Parameters","text":"","code":""},{"path":"intro.html","id":"the-causal-model","chapter":"2 The Roadmap for Targeted Learning","heading":"2.5.1 The Causal Model","text":"formalizing data statistical model, can define causal\nmodel express causal parameters interest. Directed acyclic graphs (DAGs)\none useful tool express know causal relations among\nvariables. Ignoring exogenous \\(U\\) terms (explained ), assume \nfollowing ordering variables observed data \\(O\\).directed acyclic graphs (DAGs) like provide convenient means \nvisualize causal relations variables, causal\nrelations among variables can represented via set structural equations,\ndefine non-parametric structural equation model (NPSEM):\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= f_A(W, U_A) \\\\\n  Y &= f_Y(W, , U_Y),\n\\end{align*}\\]\n\\(U_W\\), \\(U_A\\), \\(U_Y\\) represent unmeasured exogenous background\ncharacteristics influence value variable. NPSEM, \\(f_W\\),\n\\(f_A\\) \\(f_Y\\) denote variable (\\(W\\), \\(\\) \\(Y\\), respectively)\nfunction parents unmeasured background characteristics, note\nimposition particular functional constraints(e.g.,\nlinear, logit-linear, one interaction, etc.). reason, \ncalled non-parametric structural equation models (NPSEMs). \nDAG set nonparametric structural equations represent exactly \ninformation may used interchangeably.first hypothetical experiment consider assigning exposure \nwhole population observing outcome, assigning exposure \nwhole population observing outcome. nonparametric structural\nequations, corresponds comparison outcome distribution \npopulation two interventions:\\(\\) set \\(1\\) individuals, \\(\\) set \\(0\\) individuals.interventions imply two new nonparametric structural equation models. \ncase \\(= 1\\), \n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 1 \\\\\n  Y(1) &= f_Y(W, 1, U_Y),\n\\end{align*}\\]\ncase \\(=0\\),\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 0 \\\\\n  Y(0) &= f_Y(W, 0, U_Y).\n\\end{align*}\\]equations, \\(\\) longer function \\(W\\) \nintervened system, setting \\(\\) deterministically either values\n\\(1\\) \\(0\\). new symbols \\(Y(1)\\) \\(Y(0)\\) indicate outcome variable \npopulation generated respective NPSEMs ; \noften called counterfactuals (since run contrary--fact). difference\nmeans outcome two interventions defines \nparameter often called “average treatment effect” (ATE), denoted\n\\[\\begin{equation}\\label{eqn:ate}\n  ATE = \\mathbb{E}_X(Y(1)-Y(0)),\n\\end{equation}\\]\n\\(\\mathbb{E}_X\\) mean theoretical (unobserved) full data\n\\(X = (W, Y(1), Y(0))\\).Note, can define much complicated interventions NPSEM’s, \ninterventions based upon rules (based upon covariates), stochastic\nrules, etc. results different targeted parameter entails\ndifferent identifiability assumptions discussed .","code":""},{"path":"intro.html","id":"identifiability","chapter":"2 The Roadmap for Targeted Learning","heading":"2.5.2 Identifiability","text":"can never observe \\(Y(0)\\) (counterfactual outcome \\(=0\\))\n\\(Y(1)\\) (similarly, counterfactual outcome \\(=1\\)), \nestimate  directly. Instead, make assumptions \nquantity may estimated observed data \\(O \\sim P_0\\) \ndata-generating distribution \\(P_0\\). Fortunately, given causal model\nspecified NPSEM , can, handful untestable assumptions,\nestimate ATE, even observational data. assumptions may \nsummarized followsThe causal graph implies \\(Y() \\perp \\) \\(\\\\mathcal{}\\), \nrandomization assumption. case observational data, \nanalogous assumption strong ignorability unmeasured confounding\n\\(Y() \\perp \\mid W\\) \\(\\\\mathcal{}\\);Although represented causal graph, also required assumption\ninterference units, , outcome unit \\(\\) \\(Y_i\\) \naffected exposure unit \\(j\\) \\(A_j\\) unless \\(=j\\);Consistency treatment mechanism also required, .e., outcome\nunit \\(\\) \\(Y_i()\\) whenever \\(A_i = \\), assumption also known “\nversions treatment”;also necessary observed units, across strata defined \\(W\\),\nbounded (non-deterministic) probability receiving treatment –\n, \\(0 < \\mathbb{P}(= \\mid W) < 1\\) \\(\\) \\(W\\)). assumption\nreferred positivity overlap.Remark: Together, (2) (3), assumptions interference \nconsistency, respectively, jointly referred stable unit\ntreatment value assumption (SUTVA).Given assumptions, ATE may re-written function \\(P_0\\),\nspecifically\n\\[\\begin{equation}\\label{eqn:estimand}\n  ATE = \\mathbb{E}_0(Y(1) - Y(0)) = \\mathbb{E}_0\n    \\left(\\mathbb{E}_0[Y \\mid = 1, W] - \\mathbb{E}_0[Y \\mid = 0, W]\\right),\n\\end{equation}\\]\ndifference predicted outcome values subject, \ncontrast treatment conditions (\\(= 0\\) vs. \\(= 1\\)), population,\naveraged observations. Thus, parameter theoretical “full” data\ndistribution can represented estimand observed data\ndistribution. Significantly, nothing representation \n requires parameteric assumptions; thus, regressions\nright hand side may estimated freely machine learning. \ndifferent parameters, potentially different identifiability\nassumptions resulting estimands can functions different components\n\\(P_0\\).","code":""},{"path":"intro.html","id":"the-wash-benefits-example-dataset","chapter":"2 The Roadmap for Targeted Learning","heading":"2.6 The WASH Benefits Example Dataset","text":"data come study effect water quality, sanitation, hand\nwashing, nutritional interventions child development rural Bangladesh\n(WASH Benefits Bangladesh): cluster-randomised controlled trial\n(Luby et al. 2018). study enrolled pregnant women first second\ntrimester rural villages Gazipur, Kishoreganj, Mymensingh, \nTangail districts central Bangladesh, average eight women per\ncluster. Groups eight geographically adjacent clusters block-randomised,\nusing random number generator, six intervention groups (\nreceived weekly visits community health promoter first 6 months\nevery 2 weeks next 18 months) double-sized control group (\nintervention health promoter visit). six intervention groups :chlorinated drinking water;improved sanitation;hand-washing soap;combined water, sanitation, hand washing;improved nutrition counseling provision lipid-based nutrient\nsupplements; andcombined water, sanitation, handwashing, nutrition.workshop, concentrate child growth (size age) outcome \ninterest. reference, trial registered ClinicalTrials.gov \nNCT01590095.purposes workshop, start treating data independent\nidentically distributed (..d.) random draws large target\npopulation. , available options, account clustering \ndata (within sampled geographic units), , simplification, avoid \ndetails workshop presentations, although modifications \nmethodology biased samples, repeated measures, etc., available.28 variables measured, 1 variable set outcome \ninterest. outcome, \\(Y\\), weight--height Z-score (whz dat);\ntreatment interest, \\(\\), randomized treatment group (tr \ndat); adjustment set, \\(W\\), consists simply everything else. \nresults observed data structure \\(n\\) ..d. copies \\(O_i = (W_i, A_i, Y_i)\\), \\(= 1, \\ldots, n\\).Using skimr package, can\nquickly summarize variables measured WASH Benefits data set:(#tab:skim_washb_data)Data summaryVariable type: characterVariable type: numericA convenient summary relevant variables given just , complete\nsmall visualization describing marginal characteristics \ncovariate. Note asset variables reflect socioeconomic status \nstudy participants. Notice also uniform distribution treatment groups\n(twice many controls); , course, design.","code":"library(tidyverse)\n\n# read in data\ndat <- read_csv(\"https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv\")\ndat\n# A tibble: 4,695 × 28\n    whz tr      fracode month  aged sex    momage momedu momheight hfiacat Nlt18\n  <dbl> <chr>   <chr>   <dbl> <dbl> <chr>   <dbl> <chr>      <dbl> <chr>   <dbl>\n1  0    Control N05265      9   268 male       30 Prima…      146. Food S…     3\n2 -1.16 Control N05265      9   286 male       25 Prima…      149. Modera…     2\n3 -1.05 Control N08002      9   264 male       25 Prima…      152. Food S…     1\n4 -1.26 Control N08002      9   252 female     28 Prima…      140. Food S…     3\n5 -0.59 Control N06531      9   336 female     19 Secon…      151. Food S…     2\n# … with 4,690 more rows, and 17 more variables: Ncomp <dbl>, watmin <dbl>,\n#   elec <dbl>, floor <dbl>, walls <dbl>, roof <dbl>, asset_wardrobe <dbl>,\n#   asset_table <dbl>, asset_chair <dbl>, asset_khat <dbl>, asset_chouki <dbl>,\n#   asset_tv <dbl>, asset_refrig <dbl>, asset_bike <dbl>, asset_moto <dbl>,\n#   asset_sewmach <dbl>, asset_mobile <dbl>"},{"path":"sl3.html","id":"sl3","chapter":"3 Super Learning with sl3","heading":"3 Super Learning with sl3","text":"Rachael PhillipsBased sl3 R package Jeremy\nCoyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Oleg Sofrygin.Updated: 2022-05-23","code":""},{"path":"sl3.html","id":"learning-objectives-2","chapter":"3 Super Learning with sl3","heading":"Learning Objectives","text":"end chapter able :Select performance metric optimized true prediction\nfunction aligns intended use analysis real world.Select performance metric optimized true prediction\nfunction aligns intended use analysis real world.Assemble diverse set (“library”) learners considered super\nlearner. particular, able :\nCustomize learner modifying tuning parameters.\nCreate variations base learner different tuning\nparameter specifications.\nCouple screener(s) learner(s) create learners consider \ncovariates reduced, screener-selected subset .\nAssemble diverse set (“library”) learners considered super\nlearner. particular, able :Customize learner modifying tuning parameters.Create variations base learner different tuning\nparameter specifications.Couple screener(s) learner(s) create learners consider \ncovariates reduced, screener-selected subset .Specify meta-learner optimizes objective function interest.Specify meta-learner optimizes objective function interest.Justify library meta-learner terms prediction problem\nhand, intended use analysis real world, statistical model,\nsample size, number covariates, outcome prevalence discrete\noutcomes.Justify library meta-learner terms prediction problem\nhand, intended use analysis real world, statistical model,\nsample size, number covariates, outcome prevalence discrete\noutcomes.Interpret fit super learner table cross-validated risk\nestimates super learner coefficients.Interpret fit super learner table cross-validated risk\nestimates super learner coefficients.","code":""},{"path":"sl3.html","id":"super-learner-sl-introduction","chapter":"3 Super Learning with sl3","heading":"3.1 Super Learner (SL) Introduction","text":"common task data analysis prediction, using observed data \nlearn function takes input data covariates/predictors outputs \npredicted value. Occasionally, scientific question interest lends \ncausal effect estimation. Even scenarios, prediction \nforefront, prediction tasks embedded procedure. instance, \ntargeted minimum loss-based estimation (TMLE), predictive modeling necessary\nestimating outcome regressions propensity scores.various strategies can employed model relationships \ndata, refer interchangeably “estimators”, “algorithms”, \n“learners”. data algorithms can pick complex relationships\ndata necessary adequately model , data parametric\nregression learners might fit data reasonably well. generally\nimpossible know advance approach best given data\nset prediction problem.Super Learner (SL) solves issue selecting algorithm, can\nconsider many , simplest parametric regressions \ncomplex machine learning algorithms (e.g., neural nets, support vector machines,\netc). Additionally, proven perform well possible large\nsamples, given learners specified (van der Laan, Polley, Hubbard 2007). SL represents \nentirely pre-specified, data-adaptive, theoretically grounded approach \npredictive modeling. shown adaptive robust variety \napplications, even small samples. Detailed descriptions outlining\nSL procedure widely available (Polley van der Laan 2010; ???).\nPractical considerations specifying SL, including specify rich\ndiverse library learners, choose performance metric SL, \nspecify cross-validation (CV) scheme, described pre-print article\n(???). , focus introducing sl3, standard tlverse\nsoftware package SL.","code":""},{"path":"sl3.html","id":"how-to-fit-a-sl-with-sl3","chapter":"3 Super Learning with sl3","heading":"3.2 How to fit a SL with sl3","text":"section, core functionality fitting SL sl3 \nillustrated. sections follow, additional sl3 functionality \npresented.Fitting SL sl3 consists following three steps:Define prediction task make_sl3_Task.Instantiate SL Lrnr_sl.Fit SL task train.","code":""},{"path":"sl3.html","id":"wash-benefits-study-example","chapter":"3 Super Learning with sl3","heading":"WASH Benefits Study Example","text":"Using WASH Benefits Bangladesh data, interested predicting\nweight--height z-score whz set predictors/covariates (.e.,\nvariables measured weight--height z-score). \ninformation dataset described “Meet\nData” chapter.","code":""},{"path":"sl3.html","id":"preliminaries","chapter":"3 Super Learning with sl3","heading":"Preliminaries","text":"First, need load data relevant packages R session.","code":""},{"path":"sl3.html","id":"load-the-data","chapter":"3 Super Learning with sl3","heading":"3.2.0.1 Load the data","text":"use fread function data.table R package load \nWASH Benefits example dataset:Next, take peek first rows dataset:","code":"\nlibrary(data.table)\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)\nhead(washb_data)"},{"path":"sl3.html","id":"install-the-sl3-software-as-needed","chapter":"3 Super Learning with sl3","heading":"3.2.0.2 Install the sl3 software (as needed)","text":"install package, recommend first clearing R workspace \nrestarting R session. RStudio, can achieved clicking \ntab “Session” “Clear Workspace”, clicking “Session” \n“Restart R”.can install sl3 using function install_github provided \ndevtools R package:R package installed, recommend restarting R session .","code":"\nlibrary(devtools)\ninstall_github(\"tlverse/sl3\")"},{"path":"sl3.html","id":"load-the-sl3-software","chapter":"3 Super Learning with sl3","heading":"3.2.0.3 Load the sl3 software","text":"sl3 installed, can load like R package:","code":"\nlibrary(sl3)"},{"path":"sl3.html","id":"define-the-prediction-task-with-make_sl3_task","chapter":"3 Super Learning with sl3","heading":"1. Define the prediction task with make_sl3_Task","text":"sl3_Task object defines prediction task interest. Recall \ntask illustrative example use WASH Benefits Bangladesh\nexample dataset learn function covariates predicting\nweight--height Z-score whz.sl3_Task keeps track roles variables play prediction\nproblem. Additional information relevant prediction task (\nobservational-level weights, offset, id, CV folds) can also specified \nmake_sl3_Task. default CV fold structure sl3 V-fold CV (VFCV)\nV=10 folds; id specified task clustered V=10 VFCV\nscheme considered, outcome type binary categorical \nstratified V=10 VFCV scheme considered. Different CV schemes can \nspecified inputting origami folds object, generated \nmake_folds function origami R package. Refer documentation\norigami’s make_folds function information (e.g., RStudio, \nloading origami R package inputting “?make_folds” \nConsole). details sl3_Task, refer documentation (e.g., inputting “?sl3_Task” R).Tip: type task$ press tab key (press tab twice \nRStudio), can view active public fields, methods \ncan accessed task$ object. $ like key access\nmany internals object. next section, see can use $\ndig SL fit objects well, obtain predictions SL fit candidate learners, examine SL fit candidates, summarize \nSL fit.","code":"# create the task (i.e., use washb_data to predict outcome using covariates)\ntask <- make_sl3_Task(\n  data = washb_data,\n  outcome = \"whz\",\n  covariates = c(\"tr\", \"fracode\", \"month\", \"aged\", \"sex\", \"momage\", \"momedu\", \n                 \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\", \"elec\", \n                 \"floor\", \"walls\", \"roof\", \"asset_wardrobe\", \"asset_table\", \n                 \"asset_chair\", \"asset_khat\", \"asset_chouki\", \"asset_tv\", \n                 \"asset_refrig\", \"asset_bike\", \"asset_moto\", \"asset_sewmach\", \n                 \"asset_mobile\")\n)\n\n# let's examine the task\ntask\nAn sl3 Task with 4695 obs and these nodes:\n$covariates\n [1] \"tr\"              \"fracode\"         \"month\"           \"aged\"           \n [5] \"sex\"             \"momage\"          \"momedu\"          \"momheight\"      \n [9] \"hfiacat\"         \"Nlt18\"           \"Ncomp\"           \"watmin\"         \n[13] \"elec\"            \"floor\"           \"walls\"           \"roof\"           \n[17] \"asset_wardrobe\"  \"asset_table\"     \"asset_chair\"     \"asset_khat\"     \n[21] \"asset_chouki\"    \"asset_tv\"        \"asset_refrig\"    \"asset_bike\"     \n[25] \"asset_moto\"      \"asset_sewmach\"   \"asset_mobile\"    \"delta_momage\"   \n[29] \"delta_momheight\"\n\n$outcome\n[1] \"whz\"\n\n$id\nNULL\n\n$weights\nNULL\n\n$offset\nNULL\n\n$time\nNULL"},{"path":"sl3.html","id":"instantiate-the-sl-with-lrnr_sl","chapter":"3 Super Learning with sl3","heading":"3.2.1 2. Instantiate the SL with Lrnr_sl","text":"order create Lrnr_sl need specify, minimum, set \nlearners SL consider candidates. set algorithms \nalso commonly referred “library”. might also specify \nmeta-learner, algorithm ensembles learners, \noptional since already defaults set sl3. See “Practical\nconsiderations specifying super learner” step--step guidelines \ntailoring SL specification, including library meta-learner(s), \nperform well prediction task hand (???).Learners properties indicate features support. may use\nsl3_list_properties() function get list properties supported\nleast one learner:Since whz continuous outcome, can identify learners support\noutcome type sl3_list_learners():Now idea learners, let’s instantiate .\ninstantiate Lrnr_glm Lrnr_mean, main terms generalized\nlinear model (GLM) mean model, respectively.learners created , just used default tuning\nparameters. can also customize learner’s tuning parameters incorporate\ndiversity different settings, consider learner different\ntuning parameter specifications., consider base learner, Lrnr_glmnet (.e., GLMs\nelastic net regression), create two different candidates :\nL2-penalized/ridge regression L1-penalized/lasso regression.setting alpha Lrnr_glmnet , customized learner’s tuning\nparameter. instantiate Lrnr_hal9001 show multiple tuning\nparameters (specifically, max_degreeand num_knots) can modified \ntime.Let’s also instantiate learners enforce relationships \nlinear monotonic, diversify set candidates \ninclude nonparametric learners, since point learners ’ve\ninstantiated parametric.Let’s also include generalized additive model (GAM) Bayesian GLM \ndiversify pool consider candidates SL.Now ’ve instantiated set learners, need put together \nSL can consider candidates. sl3, creating \n-called Stack learners. Stack created way \ncreated learners. Stack learner ; \ninterface learners. makes stack special \nconsiders multiple learners : can train simultaneously, \npredictions can combined /compared.can see names learners stack long. \ndefault naming learner sl3 clunky: learner,\nevery tuning parameter sl3 contained name. next section,\n“Naming\nlearners”,\nshow different ways user name learners wish.Now instantiated set learners stacked together, \nready instantiate SL. use default meta-learner, \nnon-negative least squares (NNLS) regression (Lrnr_nnls) continuous\noutcomes, still go ahead specify illustrative purposes.","code":"sl3_list_properties()\n [1] \"binomial\"      \"categorical\"   \"continuous\"    \"cv\"           \n [5] \"density\"       \"h2o\"           \"ids\"           \"importance\"   \n [9] \"offset\"        \"preprocessing\" \"sampling\"      \"screener\"     \n[13] \"timeseries\"    \"weights\"       \"wrapper\"      sl3_list_learners(properties = \"continuous\")\n [1] \"Lrnr_arima\"                     \"Lrnr_bartMachine\"              \n [3] \"Lrnr_bayesglm\"                  \"Lrnr_bilstm\"                   \n [5] \"Lrnr_bound\"                     \"Lrnr_caret\"                    \n [7] \"Lrnr_cv_selector\"               \"Lrnr_dbarts\"                   \n [9] \"Lrnr_earth\"                     \"Lrnr_expSmooth\"                \n[11] \"Lrnr_ga\"                        \"Lrnr_gam\"                      \n[13] \"Lrnr_gbm\"                       \"Lrnr_glm\"                      \n[15] \"Lrnr_glm_fast\"                  \"Lrnr_glm_semiparametric\"       \n[17] \"Lrnr_glmnet\"                    \"Lrnr_glmtree\"                  \n[19] \"Lrnr_grf\"                       \"Lrnr_gru_keras\"                \n[21] \"Lrnr_gts\"                       \"Lrnr_h2o_glm\"                  \n[23] \"Lrnr_h2o_grid\"                  \"Lrnr_hal9001\"                  \n[25] \"Lrnr_HarmonicReg\"               \"Lrnr_hts\"                      \n[27] \"Lrnr_lightgbm\"                  \"Lrnr_lstm_keras\"               \n[29] \"Lrnr_mean\"                      \"Lrnr_multiple_ts\"              \n[31] \"Lrnr_nnet\"                      \"Lrnr_nnls\"                     \n[33] \"Lrnr_optim\"                     \"Lrnr_pkg_SuperLearner\"         \n[35] \"Lrnr_pkg_SuperLearner_method\"   \"Lrnr_pkg_SuperLearner_screener\"\n[37] \"Lrnr_polspline\"                 \"Lrnr_randomForest\"             \n[39] \"Lrnr_ranger\"                    \"Lrnr_rpart\"                    \n[41] \"Lrnr_rugarch\"                   \"Lrnr_screener_correlation\"     \n[43] \"Lrnr_solnp\"                     \"Lrnr_stratified\"               \n[45] \"Lrnr_svm\"                       \"Lrnr_tsDyn\"                    \n[47] \"Lrnr_xgboost\"                  \nlrn_glm <- Lrnr_glm$new()\nlrn_mean <- Lrnr_mean$new()\n# penalized regressions:\nlrn_ridge <- Lrnr_glmnet$new(alpha = 0)\nlrn_lasso <- Lrnr_glmnet$new(alpha = 1)\n# spline regressions:\nlrn_polspline <- Lrnr_polspline$new()\nlrn_earth <- Lrnr_earth$new()\n\n# fast highly adaptive lasso (HAL) implementation\nlrn_hal <- Lrnr_hal9001$new(max_degree = 2, num_knots = c(3,2), nfolds = 5)\n\n# tree-based methods\nlrn_ranger <- Lrnr_ranger$new()\nlrn_xgb <- Lrnr_xgboost$new()\nlrn_gam <- Lrnr_gam$new()\nlrn_bayesglm <- Lrnr_bayesglm$new()stack <- Stack$new(\n  lrn_glm, lrn_mean, lrn_ridge, lrn_lasso, lrn_polspline, lrn_earth, lrn_hal, \n  lrn_ranger, lrn_xgb, lrn_gam, lrn_bayesglm\n)\nstack\n [1] \"Lrnr_glm_TRUE\"                          \n [2] \"Lrnr_mean\"                              \n [3] \"Lrnr_glmnet_NULL_deviance_10_0_100_TRUE\"\n [4] \"Lrnr_glmnet_NULL_deviance_10_1_100_TRUE\"\n [5] \"Lrnr_polspline_5\"                       \n [6] \"Lrnr_earth_2_3_backward_0_1_0_0\"        \n [7] \"Lrnr_hal9001_2_1_c(3, 2)_5\"             \n [8] \"Lrnr_ranger_500_TRUE_none_1\"            \n [9] \"Lrnr_xgboost_20_1\"                      \n[10] \"Lrnr_gam_NULL_NULL_GCV.Cp\"              \n[11] \"Lrnr_bayesglm_TRUE\"                     \nsl <- Lrnr_sl$new(learners = stack, metalearner = Lrnr_nnls$new())"},{"path":"sl3.html","id":"fit-the-sl-to-the-task-with-train","chapter":"3 Super Learning with sl3","heading":"3.2.2 3. Fit the SL to the task with train","text":"last step fitting SL prediction task call train \nsupply task. also set random number generator calling\ntrain results reproducible.","code":"\nset.seed(4197)\nsl_fit <- sl$train(task)"},{"path":"sl3.html","id":"summary","chapter":"3 Super Learning with sl3","heading":"Summary","text":"section, core functionality fitting SL sl3 \nillustrated. consists following three steps:Define prediction task make_sl3_Task.Instantiate SL Lrnr_sl.Fit SL task train.example demonstrative purposes . See (???) \nstep--step guidelines constructing SL well-specified \nprediction task hand.","code":""},{"path":"sl3.html","id":"additional-sl3-topics","chapter":"3 Super Learning with sl3","heading":"3.3 Additional sl3 Topics","text":"","code":""},{"path":"sl3.html","id":"obtaining-predictions","chapter":"3 Super Learning with sl3","heading":"3.3.1 Obtaining Predictions","text":"draw fitted SL object , sl_fit, obtain \nSL’s predicted whz value subject.can also obtain predicted values candidate learner SL. \nobtain predictions GLM learner.visualize observed values whz predicted whz values \nSL, GLM mean.can also obtain cross-validated predictions candidate learners:wanted obtain predicted values new data need \ncreate new task new data. Also, covariates new task\nmust identical covariates task training. example,\nlet’s assume data washb_data_new want SL predictions.","code":"sl_preds <- sl_fit$predict(task)\nhead(sl_preds)\n[1] -0.55036 -0.87149 -0.71521 -0.75967 -0.63134 -0.67732glm_preds <- sl_fit$learner_fits$Lrnr_glm_TRUE$predict(task)\nhead(glm_preds)\n[1] -0.72617 -0.93615 -0.70850 -0.64918 -0.70132 -0.84618\n# table of observed and predicted outcome values and arrange by observed values\ndf_plot <- data.table(\n  Obs = washb_data[[\"whz\"]], SL_Pred = sl_preds, GLM_Pred = glm_preds,\n  Mean_Pred = sl_fit$learner_fits$Lrnr_mean$predict(task)\n)\ndf_plot <- df_plot[order(df_plot$Obs), ] \nhead(df_plot)\n# melt the table so we can plot observed and predicted values\ndf_plot$id <- seq(1:nrow(df_plot))\ndf_plot_melted <- melt(\n  df_plot, id.vars = \"id\",\n  measure.vars = c(\"Obs\", \"SL_Pred\", \"GLM_Pred\", \"Mean_Pred\")\n)\n\nlibrary(ggplot2)\nggplot(df_plot_melted, aes(id, value, color = variable)) + \n  geom_point(size = 0.1) + \n  labs(x = \"Subject id (ordered by increasing whz)\", \n       y = \"Weight-for-height z-score (whz)\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank(),\n        axis.text.x = element_blank(), axis.ticks.x = element_blank())\ncv_preds <- sl_fit$fit_object$cv_fit$predict_fold(task, \"validation\")\nhead(cv_preds)\nwashb_data_new$whz <- rep(NA, nrow(washb_data_new)) # create a fake outcome\npred_task <- make_sl3_Task(\n  data = washb_data_new,\n  outcome = \"whz\", \n  outcome_type = \"continuous\",\n  covariates = c(\"tr\", \"fracode\", \"month\", \"aged\", \"sex\", \"momage\", \"momedu\", \n                 \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\", \"elec\", \n                 \"floor\", \"walls\", \"roof\", \"asset_wardrobe\", \"asset_table\", \n                 \"asset_chair\", \"asset_khat\", \"asset_chouki\", \"asset_tv\", \n                 \"asset_refrig\", \"asset_bike\", \"asset_moto\", \"asset_sewmach\", \n                 \"asset_mobile\")\n)\nsl_preds_new_task <- sl_fit$predict(pred_task)"},{"path":"sl3.html","id":"summarizing-sl-fits","chapter":"3 Super Learning with sl3","heading":"3.3.2 Summarizing SL Fits","text":"","code":""},{"path":"sl3.html","id":"coefficients","chapter":"3 Super Learning with sl3","heading":"3.3.2.1 Coefficients","text":"can see meta-learner created function learners \nways. illustrative example, considered default, NNLS meta-learner\ncontinuous outcomes. meta-learners simply learn weighted\ncombination, can examine coefficients.can also examine coefficients directly accessing meta-learner’s\nfit object.Direct access meta-learner fit object also handy \ncomplex meta-learners (e.g., non-parametric meta-learners) defined\nsimple set main terms regression coefficients.","code":"round(sl_fit$coefficients, 3)\n                          Lrnr_glm_TRUE                               Lrnr_mean \n                                  0.000                                   0.000 \nLrnr_glmnet_NULL_deviance_10_0_100_TRUE Lrnr_glmnet_NULL_deviance_10_1_100_TRUE \n                                  0.076                                   0.000 \n                       Lrnr_polspline_5         Lrnr_earth_2_3_backward_0_1_0_0 \n                                  0.164                                   0.389 \n             Lrnr_hal9001_2_1_c(3, 2)_5             Lrnr_ranger_500_TRUE_none_1 \n                                  0.000                                   0.372 \n                      Lrnr_xgboost_20_1               Lrnr_gam_NULL_NULL_GCV.Cp \n                                  0.000                                   0.000 \n                     Lrnr_bayesglm_TRUE \n                                  0.000 metalrnr_fit <- sl_fit$fit_object$cv_meta_fit$fit_object\nround(metalrnr_fit$coefficients, 3)\n                          Lrnr_glm_TRUE                               Lrnr_mean \n                                  0.000                                   0.000 \nLrnr_glmnet_NULL_deviance_10_0_100_TRUE Lrnr_glmnet_NULL_deviance_10_1_100_TRUE \n                                  0.076                                   0.000 \n                       Lrnr_polspline_5         Lrnr_earth_2_3_backward_0_1_0_0 \n                                  0.164                                   0.389 \n             Lrnr_hal9001_2_1_c(3, 2)_5             Lrnr_ranger_500_TRUE_none_1 \n                                  0.000                                   0.372 \n                      Lrnr_xgboost_20_1               Lrnr_gam_NULL_NULL_GCV.Cp \n                                  0.000                                   0.000 \n                     Lrnr_bayesglm_TRUE \n                                  0.000 "},{"path":"sl3.html","id":"cross-validated-predictive-performance","chapter":"3 Super Learning with sl3","heading":"3.3.2.2 Cross-validated predictive performance","text":"can obtain table cross-validated (CV) predictive performance, .e.,\nCV risk, learner included SL. , use \nsquared error loss evaluation function, equates mean\nsquared error (MSE) metric summarize predictive performance. \nreason use MSE valid metric estimating \nconditional mean, ’re learning prediction function \nWASH Benefits example. information selecting appropriate\nperformance metric, see (???).","code":"\ncv_risk_table <- sl_fit$cv_risk(eval_fun = loss_squared_error)\ncv_risk_table[,c(1:3)]"},{"path":"sl3.html","id":"cross-validated-sl","chapter":"3 Super Learning with sl3","heading":"3.3.2.3 Cross-validated SL","text":"can see CV risk table SL contained. \nCV risk SL unless cross-validate \ninclude candidate another SL; latter shown next\nsubsection. show obtain cross-validated risk estimate \nSL using cv_sl. (Note: set eval = FALSE part code \nkilling build).","code":"\nset.seed(569)\ncv_sl_fit <- cv_sl(lrnr_sl = sl_fit, task = task, eval_fun = loss_squared_error)\ncv_sl_fit$cv_risk[,c(1:3)]"},{"path":"sl3.html","id":"discrete-sl","chapter":"3 Super Learning with sl3","heading":"3.3.3 Discrete SL","text":"discrete SL (dSL), SL uses winner-take-meta-learner called\ncross-validated selector. dSL therefore identical candidate\nbest cross-validated performance; predictions \ncandidate’s predictions. dSL meta-learner specification can invoked\nsl3 Lrnr_cv_selector.can examine CV risk ensemble SL (eSL), multiple eSLs, \nincluding candidate dSL. eSL SL uses parametric \nnon-parametric algorithm meta-learner. Therefore, eSL defined \ncombination multiple candidates; predictions defined combination\nmultiple candidates’ predictions. sl object defined previous\nsection, sl, eSL since used NNLS meta-learner.include eSL candidate dSL, allows eSL’s CV\nperformance compared learners \nconstructed. similar calling CV SL, cv_sl, . difference\nincluding eSL dSL calling cv_sl former\nautomates procedure final SL learner achieved best\nCV predictive performance, .e., lowest CV risk. eSL performs better \ncandidate, dSL end selecting . mentioned \n(???), \"another advantage approach multiple eSLs \nuse flexible meta-learner methods (e.g., non-parametric machine learning\nalgorithms like HAL) can evaluated simultaneously.","code":"\ncv_selector <- Lrnr_cv_selector$new(eval_function = loss_squared_error)\nstack_with_sl <- Stack$new(stack, sl)\ndSL <- Lrnr_sl$new(learners = stack_with_sl, metalearner = cv_selector)"},{"path":"sl3.html","id":"default-data-pre-processing","chapter":"3 Super Learning with sl3","heading":"3.3.4 Default Data Pre-processing","text":"sl3 required analytic dataset (.e., dataset\nconsisting observations outcome covariates) contain \nmissing values, contain character factor covariates.\nsubsection, review default functionality sl3 takes care\ninternally; specifically, data pre-processing occurs \nmake_sl3_Task called.Users can also perform pre-processing creating sl3_Task\n(needed) bypass default functionality discussed following.\nSee (???), section “Preliminaries: Analytic dataset pre-processing”\ninformation general guidelines follow pre-processing \nanalytic dataset, including considerations pre-processing high\ndimensional settings.Recall sl3_Task object defines prediction task interest. \ntask illustrative example use WASH Benefits\nBangladesh data learn function covariates predicting\nweight--height Z-score whz. details sl3_Task, refer \ndocumentation (e.g., inputting “?sl3_Task” R). instantiate \ntask order examine pre-processing washb_data.","code":"# create the task (i.e., use washb_data to predict outcome using covariates)\ntask <- make_sl3_Task(\n  data = washb_data,\n  outcome = \"whz\",\n  covariates = c(\"tr\", \"fracode\", \"month\", \"aged\", \"sex\", \"momage\", \"momedu\", \n                 \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\", \"elec\", \n                 \"floor\", \"walls\", \"roof\", \"asset_wardrobe\", \"asset_table\", \n                 \"asset_chair\", \"asset_khat\", \"asset_chouki\", \"asset_tv\", \n                 \"asset_refrig\", \"asset_bike\", \"asset_moto\", \"asset_sewmach\", \n                 \"asset_mobile\")\n)\nWarning in process_data(data, nodes, column_names = column_names, flag = flag, :\nMissing covariate data detected: imputing covariates."},{"path":"sl3.html","id":"imputation-and-missingness-indicators","chapter":"3 Super Learning with sl3","heading":"3.3.4.1 Imputation and missingness indicators","text":"Notice warning appeared created task . (muted \nwarning created task previous section). warning states\nmissing covariate data detected imputed. covariate column\nmissing values, sl3 uses median impute missing continuous\ncovariates, mode impute discrete (binary categorical) covariates.Also, covariate missing values, additional column indicating\nwhether value imputed incorporated. -called “missingness\nindicator” covariates can helpful, pattern covariate missingness\nmight informative predicting outcome.Users free handle missingness covariate data creating\nsl3 task. case, recommend inclusion \nmissingness indicator covariate. Let’s examine greater detail \ncompleteness. ’s also easier see ’s going examining \nexample.First, let’s examine missingness data:can see covariates momage momheight missing observations.\nLet’s check rows data missing values:called make_sl3_Task using washb_data missing covariate values,\nmomage momheight imputed respective medians (since \ncontinuous), missingness indicator (denoted prefix “delta_”) \nadded . See :Indeed, can see washb_task$data missing values. missingness\nindicators take value 0 observation original data\nvalue 1 observation original data.data supplied make_sl3_Task contains missing outcome values, \nerror thrown. Missing outcomes data can easily dropped \ntask created, setting drop_missing_outcome = TRUE. general, \nrecommend dropping missing outcomes data pre-processing, unless \nproblem interest purely prediction. complete case analyses\ngenerally biased; typically unrealistic assume missingness \ncompletely random therefore unsafe just drop observations \nmissing outcomes. instance, estimation estimands admit\nTargeted Minimum Loss-based Estimators (.e., pathwise differentiable estimands,\nincluding parameters arising causal inference violate\npositivity, reviewed following chapters), missingness \nreflected expression question interest (e.g., \naverage effect treatment Drug compared standard\ncare loss follow-) also incorporated estimation\nprocedure. , probability loss follow-prediction\nfunction approximated (e.g., SL) incorporated \nestimation target parameter inference / uncertainty\nquantification.","code":"# which columns have missing values, and how many observations are missing?\ncolSums(is.na(washb_data))\n           whz             tr        fracode          month           aged \n             0              0              0              0              0 \n           sex         momage         momedu      momheight        hfiacat \n             0             18              0             31              0 \n         Nlt18          Ncomp         watmin           elec          floor \n             0              0              0              0              0 \n         walls           roof asset_wardrobe    asset_table    asset_chair \n             0              0              0              0              0 \n    asset_khat   asset_chouki       asset_tv   asset_refrig     asset_bike \n             0              0              0              0              0 \n    asset_moto  asset_sewmach   asset_mobile \n             0              0              0 some_rows_with_missingness <- which(!complete.cases(washb_data))[31:33]\n# note: we chose 31:33 because missingness in momage & momheight is there\nwashb_data[some_rows_with_missingness, c(\"momage\", \"momheight\")]\n   momage momheight\n1:     NA     153.2\n2:     17        NA\n3:     23        NAtask$data[some_rows_with_missingness,\n          c(\"momage\", \"momheight\", \"delta_momage\", \"delta_momheight\")]\n   momage momheight delta_momage delta_momheight\n1:     23     153.2            0               1\n2:     17     150.6            1               0\n3:     23     150.6            1               0\ncolSums(is.na(task$data))\n             tr         fracode           month            aged             sex \n              0               0               0               0               0 \n         momage          momedu       momheight         hfiacat           Nlt18 \n              0               0               0               0               0 \n          Ncomp          watmin            elec           floor           walls \n              0               0               0               0               0 \n           roof  asset_wardrobe     asset_table     asset_chair      asset_khat \n              0               0               0               0               0 \n   asset_chouki        asset_tv    asset_refrig      asset_bike      asset_moto \n              0               0               0               0               0 \n  asset_sewmach    asset_mobile    delta_momage delta_momheight             whz \n              0               0               0               0               0 "},{"path":"sl3.html","id":"character-and-categorical-covariates","chapter":"3 Super Learning with sl3","heading":"3.3.4.2 Character and categorical covariates","text":"First character covariates converted factors. factor\ncovariates one-hot encoded, .e., levels factor become set \nbinary indicators. example, factor cats ’s one-hot encoding \nshown :second value cats “tabby” second row cats_onehot \nvalue 1 tabby. Every level cats except one represented \ncats_onehot table. first last cats “calico” first \nlast rows cats_onehot zero across columns, denote level\nappear explicitly table.learners sl3 trained object X task, sample \nX learners use cross-validation. Let’s check first six rows\ntask’s X object:can see character columns WASH Benefits dataset \nconverted factors factors (tr, momedu, hfiacat fracode)\none-hot encoded. can also see missingness indicators reviewed\nlast two columns task$X: delta_momage delta_momage.\nimputed momage momheight also task’s X object.","code":"cats <- c(\"calico\", \"tabby\", \"cow\", \"ragdoll\", \"mancoon\", \"dwarf\", \"calico\")\ncats <- factor(cats)\ncats_onehot <- factor_to_indicators(cats)\ncats_onehot\n     cow dwarf mancoon ragdoll tabby\n[1,]   0     0       0       0     0\n[2,]   0     0       0       0     1\n[3,]   1     0       0       0     0\n[4,]   0     0       0       1     0\n[5,]   0     0       1       0     0\n[6,]   0     1       0       0     0\n[7,]   0     0       0       0     0\nhead(task$X)"},{"path":"sl3.html","id":"learner-documentation","chapter":"3 Super Learning with sl3","heading":"3.3.5 Learner Documentation","text":"Documentation learners tuning parameters can found\nR session (e.g., see Lrnr_glmnet’s parameters, one type\n“?Lrnr_glmnet” RStudio’s R console) online sl3 Learners\nReference.\nlearners sl3 simply wrappers around existing functions \nsoftware packages R. example, sl3’s Lrnr_xgboost learner\nsl3 fitting XGBoost (eXtreme Gradient Boosting) algorithm. \ndescribed Lrnr_xgboost documentation, “learner provides fitting\nprocedures xgboost models, using xgboost package, via xgb.train”.\ngeneral, documentation sl3 learner refers reader \noriginal function package sl3 wrapped learner around. \nmind, sl3 learner documentation good first place look \nlearner, show us exactly package function learner\nbased . However, thorough investigation learner (\ndetailed explanation tuning parameters models data)\ntypically involves referencing original package. Continuing \nexample , means , information provided \nLrnr_xgboost documentation, learning Lrnr_xgboost uses \nxgboost package’s xgb.train function, deepest understanding \nXGBoost algorithm available sl3 come referencing xgboost\nR package xgb.train function.","code":""},{"path":"sl3.html","id":"naming-learners","chapter":"3 Super Learning with sl3","heading":"3.3.6 Naming learners","text":"Recall Stack example long names., show different ways user name learners. first way\nname learner upon instantiation, shown :can specify name learner upon instantiating . ,\nnamed GLM learner “GLM”.Also, can specify names learners upon creation Stack:","code":"stack\n [1] \"Lrnr_glm_TRUE\"                          \n [2] \"Lrnr_mean\"                              \n [3] \"Lrnr_glmnet_NULL_deviance_10_0_100_TRUE\"\n [4] \"Lrnr_glmnet_NULL_deviance_10_1_100_TRUE\"\n [5] \"Lrnr_polspline_5\"                       \n [6] \"Lrnr_earth_2_3_backward_0_1_0_0\"        \n [7] \"Lrnr_hal9001_2_1_c(3, 2)_5\"             \n [8] \"Lrnr_ranger_500_TRUE_none_1\"            \n [9] \"Lrnr_xgboost_20_1\"                      \n[10] \"Lrnr_gam_NULL_NULL_GCV.Cp\"              \n[11] \"Lrnr_bayesglm_TRUE\"                     \nlrn_glm <- Lrnr_glm$new(name = \"GLM\")learners_pretty_names <- c(\n  \"GLM\" = lrn_glm, \"Mean\" = lrn_mean, \"Ridge\" = lrn_ridge, \n  \"Lasso\" = lrn_lasso, \"Polspline\" = lrn_polspline, \"Earth\" = lrn_earth, \n  \"HAL\" = lrn_hal, \"RF\" = lrn_ranger, \"XGBoost\" = lrn_xgb, \"GAM\" = lrn_gam, \n  \"BayesGLM\" = lrn_bayesglm\n)\nstack_pretty_names <- Stack$new(learners_pretty_names)\nstack_pretty_names\n [1] \"GLM\"       \"Mean\"      \"Ridge\"     \"Lasso\"     \"Polspline\" \"Earth\"    \n [7] \"HAL\"       \"RF\"        \"XGBoost\"   \"GAM\"       \"BayesGLM\" "},{"path":"sl3.html","id":"defining-learners-over-grid-of-tuning-parameters","chapter":"3 Super Learning with sl3","heading":"3.3.7 Defining Learners over Grid of Tuning Parameters","text":"Customized learners can created grid tuning parameters. \nhighly data-adaptive learners require careful tuning, oftentimes\nhelpful consider different tuning parameter specifications. However,\ntime consuming, computational feasibility considered.\nAlso, effective sample size small, highly data-adaptive learners\nlikely perform well since typically require lot data fit\nmodels. See (???) information effective sample size,\nstep--step guidelines tailoring SL specification perform well\nprediction task hand.show two ways customize learners grid tuning parameters. \nfirst, “--” approach requires user collaborator \nknowledge algorithm tuning parameters, can adequately\nspecify set tuning parameters . second approach \nrequire user specialized knowledge algorithm (although \nunderstanding still helpful); uses caret software automatically\nselect “optimal” set tuning parameters grid .","code":""},{"path":"sl3.html","id":"do-it-yourself-grid","chapter":"3 Super Learning with sl3","heading":"3.3.7.1 Do-it-yourself grid","text":", show can create several variations XGBoost learner,\nLrnr_xgboost, hand. example just demonstrative purposes; users\nconsult documentation, consider computational feasibility \nprediction task specify appropriate grid tuning parameters \ntask.example , considered every possible combination grid \ncreate nine XGBoost learners. wanted create custom names \nlearners well:","code":"grid_params <- list(\n  max_depth = c(3, 5, 8),\n  eta = c(0.001, 0.1, 0.3),\n  nrounds = 100\n)\ngrid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)\n\nxgb_learners <- apply(grid, MARGIN = 1, function(tuning_params) {\n  do.call(Lrnr_xgboost$new, as.list(tuning_params))\n})\nxgb_learners\n[[1]]\n[1] \"Lrnr_xgboost_100_1_3_0.001\"\n\n[[2]]\n[1] \"Lrnr_xgboost_100_1_5_0.001\"\n\n[[3]]\n[1] \"Lrnr_xgboost_100_1_8_0.001\"\n\n[[4]]\n[1] \"Lrnr_xgboost_100_1_3_0.1\"\n\n[[5]]\n[1] \"Lrnr_xgboost_100_1_5_0.1\"\n\n[[6]]\n[1] \"Lrnr_xgboost_100_1_8_0.1\"\n\n[[7]]\n[1] \"Lrnr_xgboost_100_1_3_0.3\"\n\n[[8]]\n[1] \"Lrnr_xgboost_100_1_5_0.3\"\n\n[[9]]\n[1] \"Lrnr_xgboost_100_1_8_0.3\"\nnames(xgb_learners) <- c(\n  \"XGBoost_depth3_eta.001\", \"XGBoost_depth5_eta.001\", \"XGBoost_depth8_eta.001\", \n  \"XGBoost_depth3_eta.1\", \"XGBoost_depth5_eta.1\", \"XGBoost_depth8_eta.1\", \n  \"XGBoost_depth3_eta.3\", \"XGBoost_depth5_eta.3\", \"XGBoost_depth8_eta.3\"\n)"},{"path":"sl3.html","id":"automatic-grid-and-selection-with-caret","chapter":"3 Super Learning with sl3","heading":"3.3.7.2 Automatic grid and selection with caret","text":"can use Lrnr_caret use caret software. described \nLrnr_caret documentation, Lrnr_caret “uses caret package’s train\nfunction automatically tune predictive model”. , instantiate \nneural network automatically tuned caret name \nlearner “NNET_autotune”.","code":"\nlrnr_nnet_autotune <- Lrnr_caret$new(method = \"nnet\", name = \"NNET_autotune\")"},{"path":"sl3.html","id":"learners-with-interactions-and-formula-interface","chapter":"3 Super Learning with sl3","heading":"3.3.8 Learners with Interactions and formula Interface","text":"described (???), ’s known/possible \ninteractions among covariates can include learners pick \nexplicitly (e.g., including library parametric regression learner\ninteractions specified formula) implicitly (e.g., including \nlibrary tree-based algorithms learn interactions empirically).One way define interaction terms among covariates sl3 \nformula. argument exists Lrnr_base, inherited every\nlearner sl3; even though formula explicitly appear \nlearner argument, via inheritance. implementation allows\nformula supplied learners, even without native formula\nsupport. , show specify GLM learner considers two-way\ninteractions among covariates.can see , general behavior formulain R applies \nsl3. See Details formula stats R package details \nsyntax (e.g,. RStudio, type “?formula” Console information\nappear Help tab).","code":"\nlrnr_glm_interaction <- Lrnr_glm$new(formula = \"~.^2\")"},{"path":"sl3.html","id":"screeners","chapter":"3 Super Learning with sl3","heading":"3.3.9 Screeners","text":"One characteristic rich library learners effective \nhandling covariates high dimension. many covariates \ndata relative effective sample size (see Figure 1 Flowchart \n(???)), candidate learners coupled range -called\n“screeners”. screener simply function returns subset \ncovariates. screener intended coupled candidate learner, \ndefine new candidate learner considers reduced set \nscreener-returned covariates covariates.stated (???), “covariate screening essential \ndimensionality data large, can practically useful \nSL machine learning application. Screening covariates considers\nassociations outcome must cross validated avoid biasing \nestimate algorithm’s predictive performance”. including\nscreener-learner couplings additional candidates SL library, \ncross validating screening covariates. Covariates retained CV\nfold may vary.“range screeners” set screeners exhibits varying\ndegrees dimension reduction incorporates different fitting procedures\n(e.g., lasso-based screeners retain covariates non-zero\ncoefficients, importance-based screeners retain top \\(j\\) \nimportant covariates according importance metric. current set \nscreeners available sl3 described part .see , define screener learner coupling sl3,\nneed create Pipeline. Pipeline set learners\nfit sequentially, fit one learner used define \ntask next learner.","code":""},{"path":"sl3.html","id":"variable-importance-based-screeners","chapter":"3 Super Learning with sl3","heading":"3.3.9.1 Variable importance-based screeners","text":"Variable importance-based screeners retain top \\(j\\) important covariates\naccording importance metric. screener provided \nLrnr_screener_importance sl3 parameter \\(j\\) (default five) \nprovided user via num_screen argument. user also gets \nchoose importance metric considered via learner argument. \nlearner importance method can used Lrnr_screener_importance;\ncurrently includes following:Let’s consider screening covariates based Lrnr_ranger variable importance\nranking selects top ten important covariates, according \nranger’s “impurity_corrected” importance. couple screener \nLrnr_glm define new learner (1) selects top ten important\ncovariates, according ranger’s “impurity_corrected” importance, \n(2) passes screener-selected covariates Lrnr_glm, Lrnr_glm\nfits model according reduced set covariates. mentioned ,\ncoupling establishes new learner requires defining Pipeline.\nPipeline sl3’s way going (1) (2).even define Pipeline entire Stack, every\nlearner fit screener-selected, reduced set ten covariates.","code":"sl3_list_learners(properties = \"importance\")\n[1] \"Lrnr_lightgbm\"     \"Lrnr_randomForest\" \"Lrnr_ranger\"      \n[4] \"Lrnr_xgboost\"     \nranger_with_importance <- Lrnr_ranger$new(importance = \"impurity_corrected\")\nRFscreen_top10 <- Lrnr_screener_importance$new(\n  learner = ranger_with_importance, num_screen = 10\n)\nRFscreen_top10_glm <- Pipeline$new(RFscreen_top10, lrn_glm)\nRFscreen_top10_stack <- Pipeline$new(RFscreen_top10, stack)"},{"path":"sl3.html","id":"coefficient-threshold-based-screeners","chapter":"3 Super Learning with sl3","heading":"3.3.9.2 Coefficient threshold-based screeners","text":"Lrnr_screener_coefs provides screening covariates based magnitude\nestimated coefficients (possibly regularized) GLM. \nthreshold (default = 1e-3) defines minimum absolute size \ncoefficients, thus covariates, kept. Also, max_retain argument\ncan optionally provided restrict number selected covariates \nmax_retain.Let’s consider screening covariates Lrnr_screener_coefs select \nvariables non-zero lasso regression coefficients. couple \nscreener Lrnr_glm define new learner (1) selects covariates\nnon-zero lasso regression coefficients, (2) passes \nscreener-selected covariates Lrnr_glm, Lrnr_glm fits model\naccording reduced set covariates. structure similar \n.even define Pipeline entire Stack, every\nlearner fit lasso screener-selected, reduced set covariates.","code":"\nlasso_screen <- Lrnr_screener_coefs$new(learner = lrn_lasso, threshold = 0)\nlasso_screen_glm <- Pipeline$new(lasso_screen, lrn_glm)\nlasso_screen_stack <- Pipeline$new(lasso_screen, stack)"},{"path":"sl3.html","id":"correlation-based-screeners","chapter":"3 Super Learning with sl3","heading":"3.3.9.3 Correlation-based screeners","text":"Lrnr_screener_correlation provides covariate screening procedures \nrunning test correlation (Pearson default), selecting (1) top\nranked variables (default), (2) variables p-value lower \nuser-specified threshold.Let’s consider screening covariates Lrnr_screener_coefs. \nillustrate set pipeline Stack, looks \nprevious examples. Pipeline single learner also looks \nprevious examples.","code":"\n# select top 10 most correlated covariates\ncorRank_screen <- Lrnr_screener_correlation$new(\n  type = \"rank\", num_screen = 10\n)\ncorRank_screen_stack <- Pipeline$new(corRank_screen, stack)\n\n# select covariates with correlation p-value below 0.05, and a minimum of 3\ncorP_screen <- Lrnr_screener_correlation$new(\n  type = \"threshold\", pvalue_threshold = 0.05, min_screen = 3\n)\ncorP_screen_stack <- Pipeline$new(corP_screen, stack)"},{"path":"sl3.html","id":"augmented-screeners","chapter":"3 Super Learning with sl3","heading":"3.3.9.4 Augmented screeners","text":"Augmented screeners special enforce certain covariates \nalways included. , screener removes “mandatory” covariate \nLrnr_screener_augment reincorporate learner(s) \nPipeline fit. example use screener included .\nassume aged momage covariates must kept learner\nfitting.Lrnr_screener_augment useful subject-matter experts feel strongly\ncertain covariate sets must included, even screening procedures.","code":"\nkeepme <- c(\"aged\", \"momage\")\n# using corRank_screen as an example, but any instantiated screener can be \n# supplied as screener.\ncorRank_screen_augmented <- Lrnr_screener_augment$new(\n  screener = corRank_screen, default_covariates = keepme\n)\ncorRank_screen_augmented_glm <- Pipeline$new(corRank_screen_augmented, lrn_glm)"},{"path":"sl3.html","id":"stack-with-range-of-screeners","chapter":"3 Super Learning with sl3","heading":"3.3.9.5 Stack with range of screeners","text":", mentioned ’d like consider range screeners \ndiversify library. show can create new Stack \nlearners stacks includes learners screening, learners coupled\nvarious screeners.screeners_stack inputted learners Lrnr_sl \ndefine SL considers candidates learners screening, \nlearners coupled various screeners.","code":"\nscreeners_stack <- Stack$new(stack, corP_screen_stack, corRank_screen_stack, \n                             lasso_screen_stack, RFscreen_top10_stack)"},{"path":"sl3.html","id":"sl3-advanced","chapter":"3 Super Learning with sl3","heading":"3.4 Advanced sl3 Functionality","text":"","code":""},{"path":"sl3.html","id":"variable-importance-measures","chapter":"3 Super Learning with sl3","heading":"3.4.1 Variable Importance Measures","text":"Variable importance can interesting informative. can also \ncontradictory confusing. Nevertheless, collaborators tend like ,\ncreated variable importance function sl3. sl3\nimportance function returns table variables listed decreasing order\nimportance (.e., important first row).measure importance sl3 based ratio difference \npredictive performance SL fit removed permuted\ncovariate (covariate grouping), SL fit observed covariate (\ncovariate grouping), across . manner, larger \nratio/difference predictive performance, important covariate\n(covariate group) SL prediction.intuition measure calculates predictive risk (e.g.,\nMSE) losing one covariate (one group covariates), keeping\neverything else fixed, comparing predictive risk one \nanalytic dataset. ratio predictive risks one, difference \nzero, losing covariate (group) impact, thus \nimportant according measure. procedure repeated across \ncovariates/groups. stated , can remove covariate (\ncovariate group) refit SL without , just permute (faster) \nhope shuffling distort meaningful information present.\nidea permuting instead removing saves lot time, also\nincorporated randomForest variable importance measures. However, \npermutation approach risky. sl3 importance default remove\ncovariate refit. , use permute approach \nmuch faster.Let’s explore sl3 variable importance measurements sl_fit, \nSL fit WASH Benefits example dataset. define grouping\ncovariates based household assets, covariates considered\ntogether importance.","code":"\nassets <- c(\"asset_wardrobe\", \"asset_table\", \"asset_chair\", \"asset_khat\",\n            \"asset_chouki\", \"asset_tv\", \"asset_refrig\", \"asset_bike\", \n            \"asset_moto\", \"asset_sewmach\", \"asset_mobile\", \"Nlt18\", \"Ncomp\", \n            \"watmin\", \"elec\", \"floor\", \"walls\", \"roof\")\nset.seed(983)\nwashb_varimp <- importance(\n  fit = sl_fit, eval_fun = loss_squared_error, type = \"permute\", \n  covariate_groups = list(\"assets\" = assets)\n)\nwashb_varimp\n# plot variable importance\nimportance_plot(x = washb_varimp)"},{"path":"sl3.html","id":"conditional-density-estimation","chapter":"3 Super Learning with sl3","heading":"3.4.2 Conditional density estimation","text":"certain scenarios may useful estimate conditional density \ndependent variable, given predictors/covariates precede . \ncontext causal inference, arises readily working \ncontinuous-valued treatments. Specifically, conditional density estimation (CDE)\nnecessary estimating treatment mechanism continuous-valued\ntreatment, often called generalized propensity score. Compared \nclassical propensity score (PS) binary treatments (conditional\nprobability receiving treatment given covariates),\n\\(\\mathbb{P}(= 1 \\mid W)\\), generalized PS conditional density \ntreatment \\(\\), given covariates \\(W\\), \\(\\mathbb{P}(\\mid W)\\).CDE often requires specialized approaches tied specific algorithmic\nimplementations. knowledge, general flexible algorithms \nCDE proposed sparsely literature. implemented two\napproaches sl3: semiparametric CDE approach makes certain\nassumptions constancy (higher) moments underlying\ndistribution, second approach exploits relationship \nconditional hazard density functions allow CDE via pooled hazard\nregression. approaches flexible allow\nuse arbitrary regression functions machine learning algorithms \nestimation nuisance quantities (conditional mean conditional\nhazard, respectively). elaborate two frameworks . Importantly,\nper Dudoit van der Laan (2005) related works, loss function appropriate \ndensity estimation negative log-density loss \\(L(\\cdot) = -\\log(p_n(\\cdot))\\).","code":""},{"path":"sl3.html","id":"moment-restricted-location-scale","chapter":"3 Super Learning with sl3","heading":"3.4.2.1 Moment-restricted location-scale","text":"family semiparametric CDE approaches exploits general form \\(\\rho(Y - \\mu(X) / \\sigma(X))\\), \\(Y\\) dependent variable interest (e.g.,\ntreatment \\(\\) PS), \\(X\\) predictors (e.g., covariates \\(W\\) \nPS), $ specified marginal density function, \\(\\mu(X) = \\E(Y \\mid X)\\)\n\\(\\sigma(X) = \\E[(Y - \\mu(X))^2 \\mid X]\\) nuisance functions \ndependent variable may estimated flexibly. CDE procedures formulated\nwithin framework may characterized belonging \nconditional location-scale family, , \n\\(p_n(Y \\mid X) = \\rho((Y - \\mu_n(X)) / \\sigma_n(X))\\). CDE \nconditional location-scale families without potential disadvantages\n(e.g., restriction density’s functional form lead \nmisspecification bias), strategy flexible allows \narbitrary machine learning algorithms used estimating conditional\nmean \\(Y\\) given \\(X\\), (X) = (Y X)$, conditional variance\n\\(Y\\) given \\(X\\), \\(\\sigma(X) = \\E[(Y - \\mu(X))^2 \\mid X]\\).settings limited data, additional structure imposed \nassumption target density belongs location-scale family may prove\nadvantageous smoothing areas low support data. However, \npractice, impossible know whether assumption holds. \nprocedure novel contribution (unable \nlocate formal description literature); nevertheless, provide\ninformal algorithm sketch . algorithm considers access \\(n\\)\nindependendent identically distributed (..d.) copies observed data\nrandom variable \\(O = (Y, X)\\), priori-specified kernel function \\(\\rho\\), \ncandidate regression procedure \\(f_{\\mu}\\) estimate \\(\\mu(X)\\), candidate\nregression procedure \\(f_{\\sigma}\\) estimate \\(\\sigma(X)\\).Estimate \\(\\mu(X) = \\E[Y \\mid X]\\), conditional mean \\(Y\\) given \\(X\\), \napplying regression estimator \\(f_{\\mu}\\), yielding \\(\\hat{\\mu}(X)\\).Estimate \\(\\sigma(X) = \\mathbb{V}[Y \\mid X]\\), conditional variance \\(Y\\)\ngiven \\(X\\), applying regression estimator \\(f_{\\sigma}\\), yielding\n\\(\\hat{\\sigma}^2(X)\\). Note step involves estimation \nconditional mean \\(\\E[(Y - \\hat{\\mu}(X))^2 \\mid X]\\).Estimate one-dimensional density \\((Y - \\hat{\\mu}(X))^2 / \\hat{\\sigma}^2(X)\\), using kernel smoothing obtain \\(\\hat{\\rho}(Y)\\).Construct estimated conditional density \\(p_n(Y \\mid X) = \\hat{\\rho}((Y - \\hat{\\mu}(X)) / \\hat{\\sigma}(X))\\).algorithm sketch encompasses two forms CDE approach, diverge\nsecond step . simplify approach, one may elect estimate\nconditional mean \\(\\mu(X)\\), leaving conditional variance \nassumed constant (.e., estimated simply marginal mean \nresiduals \\(\\E[(Y - \\hat{\\mu}(X))^2]\\)). subclass CDE approaches \nhomoscedastic error based variance assumption made. conditional\nvariance can instead estimated conditional mean residuals\n\\((Y - \\hat{\\mu}(X))^2\\) given \\(X\\), \\(\\E[(Y - \\hat{\\mu}(X))^2 \\mid X]\\), \ncandidate algorithm \\(f_{\\sigma}\\) used evaluate expectation.\napproaches implemented sl3, learner\nLrnr_density_semiparametric. mean_learner argument specifies\n\\(f_{\\mu}\\) optional var_learner argument specifies \\(f_{\\sigma}\\). \ndemonstrate CDE approach .","code":"# semiparametric density estimator with homoscedastic errors (HOSE)\nhose_hal_lrnr <- Lrnr_density_semiparametric$new(\n  mean_learner = Lrnr_hal9001$new()\n)\n# semiparametric density estimator with heteroscedastic errors (HESE)\nhese_rf_glm_lrnr <- Lrnr_density_semiparametric$new(\n  mean_learner = Lrnr_ranger$new()\n  var_learner = Lrnr_glm$new()\n)\n\n# SL for the conditional treatment density\nsl_dens_lrnr <- Lrnr_sl$new(\n  learners = list(hose_hal_lrnr, hese_rf_glm_lrnr),\n  metalearner = Lrnr_solnp_density$new()\n)"},{"path":"sl3.html","id":"pooled-hazard-regression","chapter":"3 Super Learning with sl3","heading":"3.4.2.2 Pooled hazard regression","text":"Another approach CDE available sl3, originally proposed \nDı́az van der Laan (2011), leverages relationship (conditional) hazard \ndensity functions. develop CDE framework, Dı́az van der Laan (2011) proposed\ndiscretizing continuous dependent variable \\(Y\\) support \\(\\mathcal{Y}\\)\nbased number bins \\(T\\) binning procedure (e.g., cutting\n\\(\\mathcal{Y}\\) \\(T\\) bins exactly length). tuning parameter\n\\(T\\) conceptually corresponds choice bandwidth classical kernel\ndensity estimation. Following discretization, unit represented \ncollection records, number records representing given unit\ndepends rank bin (along discretized support) \nunit falls.take example, instantiation procedure might divide support\n\\(Y\\) , say, \\(T = 4\\), bins equal length (note requires \\(T+1\\) cut\npoints): \\([\\alpha_1, \\alpha_2), [\\alpha_2, \\alpha_3), [\\alpha_3, \\alpha_4), [\\alpha_4, \\alpha_5]\\) (n.b., rightmost interval fully closed \nothers partially closed). Next, artificial, repeated measures\ndataset created unit represented \\(T\\)\nrecords. better see structure, consider individual unit\n\\(O_i = (Y_i, X_i)\\) whose \\(Y_i\\) value within \\([\\alpha_3, \\alpha_4)\\), \nthird bin. unit represented three distinct records:\n\\(\\{Y_{ij}, X_{ij}\\}_{j=1}^3\\), \\(\\{\\{Y_{ij} = 0\\}_{j=1}^2\\), \\(Y_{i3} = 1\\}\\)\nthree exact copies \\(X_i\\), \\(\\{X_{ij}\\}_{j=1}^3\\). representation \nterms multiple records unit allows conditional hazard\nprobability \\(Y_i\\) falling given bin along discretized support \nevaluated via standard binary regression techniques.fact, proposal reformulates binary regression problem \ncorresponding set hazard regressions: \\(\\mathbb{P} (Y \\[\\alpha_{t-1}, \\alpha_t) \\mid X) = \\mathbb{P} (Y \\[\\alpha_{t-1}, \\alpha_t) \\mid Y \\geq \\alpha_{t-1}, X) \\times \\prod_{j = 1}^{t -1} \\{1 - \\mathbb{P} (Y \\[\\alpha_{j-1}, \\alpha_j) \\mid Y \\geq \\alpha_{j-1}, X) \\}\\). , probability\n\\(Y \\\\mathcal{Y}\\) falling bin \\([\\alpha_{t-1}, \\alpha_t)\\) may directly\nestimated via binary regression procedure, re-expressing corresponding\nlikelihood terms likelihood binary variable dataset \nrepeated measures structure. Finally, hazard estimates can mapped \ndensity estimates re-scaling hazard estimates bin sizes \\(\\lvert \\alpha_t - \\alpha_{t-1} \\rvert\\), , \\(p_{n, \\alpha}(Y \\mid X) = \\mathbb{P}(Y \\[\\alpha_{t-1}, \\alpha_t) \\mid X) / \\lvert \\alpha_t - \\alpha_{t-1} \\rvert\\), \\(\\alpha_{t-1} \\leq < \\alpha_t\\). provide \ninformal sketch algorithm .Apply procedure divide observed support \\(Y\\), \\(\\max(Y) - \\min(Y)\\),\n\\(T\\) bins: \\([\\alpha_1, \\alpha_2), \\ldots, [\\alpha_{t-1}, \\alpha_t), [\\alpha_t, \\alpha_{t+1}]\\).Expand observed data repeated measures data structure, expressing\nindividual observation set \\(T\\) records, recording \nobservation ID alongside record. single unit \\(\\), set \nrecords takes form \\(\\{Y_{ij}, X_{ij}\\}_{j=1}^{T_i}\\), \\(X_{ij}\\) \nconstant index set \\(\\mathcal{J}\\), \\(Y_{ij}\\) binary counting\nprocess jumps \\(0\\) \\(1\\) final index (bin \n\\(Y_i\\) falls), \\(T_i \\leq T\\) indicates bin along support \n\\(Y_i\\) falls.Estimate hazard probability, conditional \\(X\\), bin membership\n\\(\\mathbb{P}(Y_i \\[\\alpha_{t-1}, \\alpha_t) \\mid X)\\) using binary\nregression estimator appropriate machine learning algorithm.Rescale conditional hazard probability estimates conditional\ndensity scale dividing cumulative hazard width bin \n\\(X_i\\) falls, observation \\(= 1, \\ldots, n\\). support\nset partitioned bins equal size (approximately \\(n/T\\) samples \nbin), amounts rescaling constant. support\nset partitioned bins equal range, rescaling might vary\nacross bins.key element proposal flexibility use binary regression\nprocedure appropriate machine learning algorithm estimate \\(\\prob(Y \\[\\alpha_{t-1}, \\alpha_t) \\mid X)\\), facilitating incorporation flexible\ntechniques like ensemble learning (Breiman 1996; van der Laan, Polley, Hubbard 2007). \nextreme degree flexibility integrates perfectly underlying design\nprinciples sl3; however, yet implemented approach \nfull generality. version CDE approach, limits original\nproposal replacing use arbitrary binary regression highly\nadaptive lasso (HAL) algorithm (Benkeser van der Laan 2016) supported \nhaldensify package\n(Hejazi, Benkeser, van der Laan 2020) (HAL implementation haldensify provided \nhal9001 package\n(Coyle, Hejazi, van der Laan 2020; Hejazi, Coyle, van der Laan 2020)). CDE algorithm uses\nhaldensify incorporated learner Lrnr_haldensify sl3, \ndemonstrate .","code":"\n# learners used for conditional densities for (g_n)\nhaldensify_lrnr <- Lrnr_haldensify$new(\n  n_bins = c(5, 10)\n)"},{"path":"sl3.html","id":"sl3-exercises","chapter":"3 Super Learning with sl3","heading":"3.5 Exercises","text":"","code":""},{"path":"sl3.html","id":"sl3ex1","chapter":"3 Super Learning with sl3","heading":"3.5.1 Predicting Myocardial Infarction with sl3","text":"Follow steps predict myocardial infarction (mi) using \navailable covariate data. thank Prof. David Benkeser Emory University \nmaking Cardiovascular Health Study (CHS) data accessible.Let’s take quick peek data:Create sl3 task, setting myocardial infarction mi outcome \nusing available covariate data.Make library seven relatively fast base learning algorithms (.e., \nconsider BART HAL). Customize tuning parameters one \nlearners. Incorporate least one screener-learner coupling.Make SL train task.Print SL fit results adding $cv_risk(loss_squared_error) \nfit object.","code":"\n# load the data set\nlibrary(readr)\ndb_data <- url(\n  paste0(\n    \"https://raw.githubusercontent.com/benkeser/sllecture/master/\",\n    \"chspred.csv\"\n  )\n)\nchspred <- read_csv(file = db_data, col_names = TRUE)\nhead(chspred)"},{"path":"sl3.html","id":"concluding-remarks","chapter":"3 Super Learning with sl3","heading":"3.6 Concluding Remarks","text":"Super Learner (SL) general approach can applied diversity \nestimation prediction problems can defined loss function.Super Learner (SL) general approach can applied diversity \nestimation prediction problems can defined loss function.straightforward plug estimator returned SL \ntarget parameter mapping.\nexample, suppose average treatment effect (ATE) \nbinary treatment intervention:\n\\(\\Psi_0 = E_{0,W}[E_0(Y|=1,W) - E_0(Y|=0,W)]\\).\nuse SL trained original data (let’s call\nsl_fit) predict outcome subjects \nintervention. need take average difference\ncounterfactual outcomes intervention interest.\nConsidering \\(\\Psi_0\\) , first need two \\(n\\)-length vectors \npredicted outcomes intervention. One vector represent\npredicted outcomes intervention sets subjects \nreceive \\(=1\\), \\(Y_i|A_i=1,W_i\\) \\(=1,\\ldots,n\\). vector\nrepresent predicted outcomes intervention sets\nsubjects receive \\(=0\\), \\(Y_i|A_i=0,W_i\\) \\(=1,\\ldots,n\\).\nobtaining vectors counterfactual predicted outcomes, \nneed average take difference order \n“plug-” SL estimator target parameter mapping.\nsl3 current ATE example, achieved \nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task));\nA1_task$data contain 1’s (level pertains \nreceiving treatment) treatment column data (keeping\nelse ), A0_task$data contain 0’s (\nlevel pertains receiving treatment) treatment\ncolumn data.\nstraightforward plug estimator returned SL \ntarget parameter mapping.example, suppose average treatment effect (ATE) \nbinary treatment intervention:\n\\(\\Psi_0 = E_{0,W}[E_0(Y|=1,W) - E_0(Y|=0,W)]\\).use SL trained original data (let’s call\nsl_fit) predict outcome subjects \nintervention. need take average difference\ncounterfactual outcomes intervention interest.Considering \\(\\Psi_0\\) , first need two \\(n\\)-length vectors \npredicted outcomes intervention. One vector represent\npredicted outcomes intervention sets subjects \nreceive \\(=1\\), \\(Y_i|A_i=1,W_i\\) \\(=1,\\ldots,n\\). vector\nrepresent predicted outcomes intervention sets\nsubjects receive \\(=0\\), \\(Y_i|A_i=0,W_i\\) \\(=1,\\ldots,n\\).obtaining vectors counterfactual predicted outcomes, \nneed average take difference order \n“plug-” SL estimator target parameter mapping.sl3 current ATE example, achieved \nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task));\nA1_task$data contain 1’s (level pertains \nreceiving treatment) treatment column data (keeping\nelse ), A0_task$data contain 0’s (\nlevel pertains receiving treatment) treatment\ncolumn data.’s worthwhile exercise obtain predicted counterfactual outcomes\ncreate counterfactual sl3 tasks. ’s biased; however, \nplug SL fit target parameter mapping, (e.g., calling result\nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task)) \nestimated ATE. end estimator ATE \noptimized estimation prediction function, ATE!’s worthwhile exercise obtain predicted counterfactual outcomes\ncreate counterfactual sl3 tasks. ’s biased; however, \nplug SL fit target parameter mapping, (e.g., calling result\nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task)) \nestimated ATE. end estimator ATE \noptimized estimation prediction function, ATE!end “analysis day”, want estimator optimized \ntarget estimand interest. ultimately care good job\nestimating \\(\\psi_0\\). SL essential step help us get . \nfact, use counterfactual predicted outcomes explained\nlength . However, SL end estimation procedure.\nSpecifically, Super Learner asymptotically linear\nestimator target estimand; efficient substitution\nestimator. begs question, important estimator \npossess properties?\nasymptotically linear estimator converges estimand \n\\(\\frac{1}{\\sqrt{n}}\\) rate, thereby permitting formal statistical inference\n(.e., confidence intervals \\(p\\)-values) [ADD REF].\nSubstitution, plug-, estimators estimand desirable \nrespect local global constraints statistical model\n(e.g., bounds), better finite-sample properties[ADD REF].\nefficient estimator optimal sense lowest\npossible variance, thus precise. estimator efficient\nasymptotically linear influence curve equal \ncanonical gradient [ADD REF].\ncanonical gradient mathematical object specific \ntarget estimand, provides information level \ndifficulty estimation problem [ADD REF]. Various canonical\ngradient shown chapters follow.\nPractitioner’s need know calculate canonical\ngradient order understand efficiency use Targeted Maximum\nLikelihood Estimation (TMLE). Metaphorically, need \nYoda order Jedi.\n\nend “analysis day”, want estimator optimized \ntarget estimand interest. ultimately care good job\nestimating \\(\\psi_0\\). SL essential step help us get . \nfact, use counterfactual predicted outcomes explained\nlength . However, SL end estimation procedure.\nSpecifically, Super Learner asymptotically linear\nestimator target estimand; efficient substitution\nestimator. begs question, important estimator \npossess properties?asymptotically linear estimator converges estimand \n\\(\\frac{1}{\\sqrt{n}}\\) rate, thereby permitting formal statistical inference\n(.e., confidence intervals \\(p\\)-values) [ADD REF].asymptotically linear estimator converges estimand \n\\(\\frac{1}{\\sqrt{n}}\\) rate, thereby permitting formal statistical inference\n(.e., confidence intervals \\(p\\)-values) [ADD REF].Substitution, plug-, estimators estimand desirable \nrespect local global constraints statistical model\n(e.g., bounds), better finite-sample properties[ADD REF].Substitution, plug-, estimators estimand desirable \nrespect local global constraints statistical model\n(e.g., bounds), better finite-sample properties[ADD REF].efficient estimator optimal sense lowest\npossible variance, thus precise. estimator efficient\nasymptotically linear influence curve equal \ncanonical gradient [ADD REF].\ncanonical gradient mathematical object specific \ntarget estimand, provides information level \ndifficulty estimation problem [ADD REF]. Various canonical\ngradient shown chapters follow.\nPractitioner’s need know calculate canonical\ngradient order understand efficiency use Targeted Maximum\nLikelihood Estimation (TMLE). Metaphorically, need \nYoda order Jedi.\nefficient estimator optimal sense lowest\npossible variance, thus precise. estimator efficient\nasymptotically linear influence curve equal \ncanonical gradient [ADD REF].canonical gradient mathematical object specific \ntarget estimand, provides information level \ndifficulty estimation problem [ADD REF]. Various canonical\ngradient shown chapters follow.Practitioner’s need know calculate canonical\ngradient order understand efficiency use Targeted Maximum\nLikelihood Estimation (TMLE). Metaphorically, need \nYoda order Jedi.TMLE general strategy succeeds constructing efficient \nasymptotically linear plug-estimators.TMLE general strategy succeeds constructing efficient \nasymptotically linear plug-estimators.SL fantastic pure prediction, obtaining initial\nestimate first step TMLE, need second step TMLE \ndesirable statistical properties mentioned .SL fantastic pure prediction, obtaining initial\nestimate first step TMLE, need second step TMLE \ndesirable statistical properties mentioned .chapters follow, focus targeted maximum likelihood\nestimator targeted minimum loss-based estimator, referred \nTMLE.chapters follow, focus targeted maximum likelihood\nestimator targeted minimum loss-based estimator, referred \nTMLE.","code":""},{"path":"sl3.html","id":"appendix","chapter":"3 Super Learning with sl3","heading":"3.7 Appendix","text":"","code":""},{"path":"sl3.html","id":"sl3ex1-sol","chapter":"3 Super Learning with sl3","heading":"3.7.1 Exercise 1 Solution","text":"potential solution sl3 Exercise 1 – Predicting Myocardial\nInfarction sl3.","code":"\ndb_data <- url(\n  \"https://raw.githubusercontent.com/benkeser/sllecture/master/chspred.csv\"\n)\nchspred <- read_csv(file = db_data, col_names = TRUE)\ndata.table::setDT(chspred)\n\n# make task\nchspred_task <- make_sl3_Task(\n  data = chspred,\n  covariates = colnames(chspred)[-1],\n  outcome = \"mi\"\n)\n\n# make learners\nglm_learner <- Lrnr_glm$new()\nlasso_learner <- Lrnr_glmnet$new(alpha = 1)\nridge_learner <- Lrnr_glmnet$new(alpha = 0)\nenet_learner <- Lrnr_glmnet$new(alpha = 0.5)\n# curated_glm_learner uses formula = \"mi ~ smoke + beta\"\ncurated_glm_learner <- Lrnr_glm_fast$new(covariates = c(\"smoke\", \"beta\"))\nmean_learner <- Lrnr_mean$new() # That is one mean learner!\nglm_fast_learner <- Lrnr_glm_fast$new()\nranger_learner <- Lrnr_ranger$new()\nsvm_learner <- Lrnr_svm$new()\nxgb_learner <- Lrnr_xgboost$new()\n\n# screening\nscreen_cor <- make_learner(Lrnr_screener_correlation)\nglm_pipeline <- make_learner(Pipeline, screen_cor, glm_learner)\n\n# stack learners together\nstack <- make_learner(\n  Stack,\n  glm_pipeline, glm_learner,\n  lasso_learner, ridge_learner, enet_learner,\n  curated_glm_learner, mean_learner, glm_fast_learner,\n  ranger_learner, svm_learner, xgb_learner\n)\n\n# make and train SL\nsl <- Lrnr_sl$new(\n  learners = stack\n)\nsl_fit <- sl$train(chspred_task)\nsl_fit$cv_risk(loss_squared_error)"},{"path":"tmle3.html","id":"tmle3","chapter":"4 The TMLE Framework (Brief Review)","heading":"4 The TMLE Framework (Brief Review)","text":"Jeremy Coyle Nima HejaziBased tmle3 R package.","code":""},{"path":"tmle3.html","id":"learn-tmle","chapter":"4 The TMLE Framework (Brief Review)","heading":"4.1 Learning Objectives","text":"end chapter, able toUse tmle3 estimate Average Treatment Effect (ATE).Understand use tmle3 “Specs” objects.","code":""},{"path":"tmle3.html","id":"tmle-intro","chapter":"4 The TMLE Framework (Brief Review)","heading":"4.2 Introduction","text":"Mark Alan introduced core concepts associated TMLE intro talk. Today, ’ll focused advanced applications tmle3, ’d like review basics use package. , conceptual clarifications TMLE?following sections describe simple way \nspecifying estimating TMLE tlverse. designing tmle3, \nsought replicate closely possible general estimation framework\nTMLE, theoretical object relevant TMLE encoded \ncorresponding software object/method. information design can found handbook.","code":""},{"path":"tmle3.html","id":"easy-bake-example-tmle3-for-ate","chapter":"4 The TMLE Framework (Brief Review)","heading":"4.3 Easy-Bake Example: tmle3 for ATE","text":"’ll illustrate basic use TMLE using WASH Benefits data\nintroduced earlier estimating average treatment effect. Similar specifications relevant later sections advanced tmle3 usage.","code":""},{"path":"tmle3.html","id":"load-the-data-1","chapter":"4 The TMLE Framework (Brief Review)","heading":"4.3.1 Load the Data","text":"’ll use WASH Benefits data earlier chapters:","code":"\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(tmle3)\nlibrary(sl3)\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)"},{"path":"tmle3.html","id":"define-the-variable-roles","chapter":"4 The TMLE Framework (Brief Review)","heading":"4.3.2 Define the variable roles","text":"’ll use common \\(W\\) (covariates), \\(\\) (treatment/intervention), \\(Y\\)\n(outcome) data structure. tmle3 needs know variables dataset\ncorrespond roles. use list character vectors tell\n. call “Node List” corresponds nodes Directed\nAcyclic Graph (DAG), way displaying causal relationships variables.","code":"\nnode_list <- list(\n  W = c(\n    \"month\", \"aged\", \"sex\", \"momage\", \"momedu\",\n    \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\",\n    \"elec\", \"floor\", \"walls\", \"roof\", \"asset_wardrobe\",\n    \"asset_table\", \"asset_chair\", \"asset_khat\",\n    \"asset_chouki\", \"asset_tv\", \"asset_refrig\",\n    \"asset_bike\", \"asset_moto\", \"asset_sewmach\",\n    \"asset_mobile\"\n  ),\n  A = \"tr\",\n  Y = \"whz\"\n)"},{"path":"tmle3.html","id":"handle-missingness","chapter":"4 The TMLE Framework (Brief Review)","heading":"4.3.3 Handle Missingness","text":"Currently, missingness tmle3 handled fairly simple way:Missing covariates median- (continuous) mode- (discrete)\nimputed, additional covariates indicating imputation generated, just\ndescribed sl3 chapter.Missing treatment variables excluded – observations dropped.Missing outcomes efficiently handled automatic calculation (\nincorporation estimators) inverse probability censoring weights\n(IPCW); also known IPCW-TMLE may thought joint\nintervention remove missingness analogous procedure used \nclassical inverse probability weighted estimators.steps implemented process_missing function tmle3:","code":"\nprocessed <- process_missing(washb_data, node_list)\nwashb_data <- processed$data\nnode_list <- processed$node_list"},{"path":"tmle3.html","id":"create-a-spec-object","chapter":"4 The TMLE Framework (Brief Review)","heading":"4.3.4 Create a “Spec” Object","text":"tmle3 general, allows components TMLE procedure \nspecified modular way. However, end-users interested \nmanually specifying components. Therefore, tmle3 implements \ntmle3_Spec object bundles set components specification\n(“Spec”) , minimal additional detail, can run end-user.’ll start using one specs, work way \ninternals tmle3.","code":"\nate_spec <- tmle_ATE(\n  treatment_level = \"Nutrition + WSH\",\n  control_level = \"Control\"\n)"},{"path":"tmle3.html","id":"define-the-learners","chapter":"4 The TMLE Framework (Brief Review)","heading":"4.3.5 Define the learners","text":"Currently, thing user must define sl3 learners used\nestimate relevant factors likelihood: Q g.takes form list sl3 learners, one likelihood factor\nestimated sl3:, use Super Learner defined previous chapter. future,\nplan include reasonable defaults learners.","code":"\n# choose base learners\nlrnr_mean <- make_learner(Lrnr_mean)\nlrnr_rf <- make_learner(Lrnr_ranger)\n\n# define metalearners appropriate to data types\nls_metalearner <- make_learner(Lrnr_nnls)\nmn_metalearner <- make_learner(\n  Lrnr_solnp, metalearner_linear_multinomial,\n  loss_loglik_multinomial\n)\nsl_Y <- Lrnr_sl$new(\n  learners = list(lrnr_mean, lrnr_rf),\n  metalearner = ls_metalearner\n)\nsl_A <- Lrnr_sl$new(\n  learners = list(lrnr_mean, lrnr_rf),\n  metalearner = mn_metalearner\n)\nlearner_list <- list(A = sl_A, Y = sl_Y)"},{"path":"tmle3.html","id":"fit-the-tmle","chapter":"4 The TMLE Framework (Brief Review)","heading":"4.3.6 Fit the TMLE","text":"now everything need fit tmle using tmle3:","code":"tmle_fit <- tmle3(ate_spec, washb_data, node_list, learner_list)\nprint(tmle_fit)\nA tmle3_Fit that took 1 step(s)\n   type                                    param   init_est  tmle_est       se\n1:  ATE ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}] -0.0031624 0.0077013 0.050351\n       lower   upper psi_transformed lower_transformed upper_transformed\n1: -0.090985 0.10639       0.0077013         -0.090985           0.10639"},{"path":"tmle3.html","id":"evaluate-the-estimates","chapter":"4 The TMLE Framework (Brief Review)","heading":"4.3.7 Evaluate the Estimates","text":"can see summary results printing fit object. Alternatively, \ncan extra results summary indexing :","code":"estimates <- tmle_fit$summary$psi_transformed\nprint(estimates)\n[1] 0.0077013"},{"path":"tmle3.html","id":"summary-1","chapter":"4 The TMLE Framework (Brief Review)","heading":"4.4 Summary","text":"tmle3 general purpose framework generating TML estimates. easiest\nway use use predefined spec, allowing just fill \nblanks data, variable roles, sl3 learners. next sections,\n’ll see framework can used estimate advanced parameters \noptimal treatments stochastic shift interventions.exercises brief chapter, may find exercises corresponding handbook chapter helpful.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"optimal-individualized-treatment-regimes","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5 Optimal Individualized Treatment Regimes","text":"Ivana MalenicaBased tmle3mopttx R package\nIvana Malenica, Jeremy Coyle, Mark van der Laan.Updated: 2022-05-23","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"learning-objectives-3","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.1 Learning Objectives","text":"end lesson able :Differentiate dynamic optimal dynamic treatment interventions static\ninterventions.Explain benefits, challenges, associated using optimal\nindividualized treatment regimes practice.Contrast impact implementing optimal individualized treatment\nregime population impact implementing static dynamic\ntreatment regimes population.Estimate causal effects optimal individualized treatment regimes \ntmle3mopttx R package.Assess mean optimal individualized treatment resource\nconstraints.Implement optimal individualized treatment rules based sub-optimal\nrules, “simple” rules, recognize practical benefit rules.Construct “realistic” optimal individualized treatment regimes respect\nreal data subject-matter knowledge limitations interventions \nconsidering interventions supported data.Measure variable importance defined terms optimal individualized\ntreatment interventions.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"introduction-1","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.2 Introduction","text":"Identifying intervention effective patient\nbased lifestyle, genetic environmental factors common goal \nprecision medicine. One opts administer intervention individuals\nbenefit , instead assigning treatment population level.aim motivates different type intervention, opposed static\nexposures might used .aim motivates different type intervention, opposed static\nexposures might used .chapter, learn dynamic (individualized) interventions \ntailor treatment decision based collected covariates.chapter, learn dynamic (individualized) interventions \ntailor treatment decision based collected covariates.motivate types interventions, turn actual randomized trial.goal improve retention HIV care.goal improve retention HIV care.Several interventions show efficacy – appointment reminders text\nmessages, small cash incentives -time clinic visits, peer health\nworkers.Several interventions show efficacy – appointment reminders text\nmessages, small cash incentives -time clinic visits, peer health\nworkers.want improve effectiveness assigning patient intervention\nlikely benefit .want improve effectiveness assigning patient intervention\nlikely benefit .also want allocate resources individuals benefit\nintervention.also want allocate resources individuals benefit\nintervention.\nFIGURE 5.1: Illustration Dynamic Treatment Regime Clinical Setting\nSuppose one wishes maximize population mean outcome, \nindividual access set measured covariates.Suppose one wishes maximize population mean outcome, \nindividual access set measured covariates.ITR maximal value referred optimal ITR \noptimal individualized treatment.ITR maximal value referred optimal ITR \noptimal individualized treatment.value optimal ITR termed optimal value, mean\noptimal individualized treatment.value optimal ITR termed optimal value, mean\noptimal individualized treatment.One opts administer intervention individuals profit \n, instead assigning treatment population level.One opts administer intervention individuals profit \n, instead assigning treatment population level.chapter, examine optimal individualized treatment regimes, \nestimate mean outcome ITR.chapter, examine optimal individualized treatment regimes, \nestimate mean outcome ITR.candidate rules restricted depend user-supplied subset \nbaseline covariates.candidate rules restricted depend user-supplied subset \nbaseline covariates.use tmle3mopttx estimate optimal ITR corresponding value.use tmle3mopttx estimate optimal ITR corresponding value.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"data-structure-and-notation","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.3 Data Structure and Notation","text":"Suppose observe \\(n\\) independent identically distributed observations \nform \\(O=(W,,Y) \\sim P_0\\).Suppose observe \\(n\\) independent identically distributed observations \nform \\(O=(W,,Y) \\sim P_0\\).\\(P_0 \\\\mathcal{M}\\), \\(\\mathcal{M}\\) \nfully nonparametric model.\\(P_0 \\\\mathcal{M}\\), \\(\\mathcal{M}\\) \nfully nonparametric model.Denote \\(\\\\mathcal{}\\) categorical treatment.Denote \\(\\\\mathcal{}\\) categorical treatment.Let\n\\(\\mathcal{} \\equiv \\{a_1, \\ldots, a_{n_A} \\}\\) \\(n_A = |\\mathcal{}|\\), \n\\(n_A\\) denoting number categories.Let\n\\(\\mathcal{} \\equiv \\{a_1, \\ldots, a_{n_A} \\}\\) \\(n_A = |\\mathcal{}|\\), \n\\(n_A\\) denoting number categories.Denote \\(Y\\) final outcome.Denote \\(Y\\) final outcome.\\(W\\) vector-valued collection baseline covariates.\\(W\\) vector-valued collection baseline covariates.Finally, let \\(V\\) subset baseline covariates \\(W\\) \nrule might depend .Finally, let \\(V\\) subset baseline covariates \\(W\\) \nrule might depend .likelihood data admits factorization, implied time ordering \\(O\\).\n\\[\\begin{align*}\\label{eqn:likelihood_factorization}\np_0(O) &= p_{Y,0}(Y|,W) p_{,0}(|W) p_{W,0}(W) \\\\\n       &= q_{Y,0}(Y|,W) q_{,0}(|W) q_{W,0}(W),\n\\end{align*}\\]likelihood data admits factorization, implied time ordering \\(O\\).\n\\[\\begin{align*}\\label{eqn:likelihood_factorization}\np_0(O) &= p_{Y,0}(Y|,W) p_{,0}(|W) p_{W,0}(W) \\\\\n       &= q_{Y,0}(Y|,W) q_{,0}(|W) q_{W,0}(W),\n\\end{align*}\\]Consequently, let\n\\[P_{Y,0}(Y|,W)=Q_{Y,0}(Y|,W),\\] \\[P_{,0}(|W)=g_0(|W)\\] \\[P_{W,0}(W)=Q_{W,0}(W).\\]Consequently, let\n\\[P_{Y,0}(Y|,W)=Q_{Y,0}(Y|,W),\\] \\[P_{,0}(|W)=g_0(|W)\\] \\[P_{W,0}(W)=Q_{W,0}(W).\\]also define \\(\\bar{Q}_{Y,0}(,W) \\equiv E_0[Y|,W]\\).","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"causal-effect-of-an-oit","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.4 Causal Effect of an OIT","text":"define relationships variables structural equations:\\[\\begin{align*}\n  W &= f_W(U_W) \\\\ &= f_A(W, U_A) \\\\ Y &= f_Y(, W, U_Y).\n\\end{align*}\\]\\(U=(U_W,U_A,U_Y)\\) denotes exogenous random variables, drawn \\(U \\sim P_U\\).\\(U=(U_W,U_A,U_Y)\\) denotes exogenous random variables, drawn \\(U \\sim P_U\\).endogenous variables, written \\(O=(W,,Y)\\), correspond observed data.endogenous variables, written \\(O=(W,,Y)\\), correspond observed data.Consider dynamic treatment rules, denoted \\(d\\), set possible rules\n\\(\\mathcal{D}\\).Consider dynamic treatment rules, denoted \\(d\\), set possible rules\n\\(\\mathcal{D}\\).point treatment setting, \\(d\\) deterministic function\ntakes input \\(V\\) outputs treatment decision whereIn point treatment setting, \\(d\\) deterministic function\ntakes input \\(V\\) outputs treatment decision \\[V \\rightarrow d(V) \\\\{a_1, \\cdots, a_{n_A} \\}.\\]given rule \\(d\\), modified system takes following form:\\[\\begin{align*}\n  W &= f_W(U_W) \\\\ &= d(V) \\\\ Y_{d(V)} &= f_Y(d(V), W, U_Y).\n\\end{align*}\\]counterfactual outcome \\(Y_{d(V)}\\) denotes outcome patient \ntreatment assigned using dynamic rule \\(d(V)\\) (possibly contrary\nfact).counterfactual outcome \\(Y_{d(V)}\\) denotes outcome patient \ntreatment assigned using dynamic rule \\(d(V)\\) (possibly contrary\nfact).Distribution counterfactual outcomes \\(P_{U,X}\\), implied \ndistribution exogenous variables \\(U\\) structural equations \\(f\\).Distribution counterfactual outcomes \\(P_{U,X}\\), implied \ndistribution exogenous variables \\(U\\) structural equations \\(f\\).set possible counterfactual distributions encompassed \ncausal model \\(\\mathcal{M}^F\\), \\(P_{U,X} \\\\mathcal{M}^F\\).set possible counterfactual distributions encompassed \ncausal model \\(\\mathcal{M}^F\\), \\(P_{U,X} \\\\mathcal{M}^F\\).can consider different treatment rules, set \\(\\mathcal{D}\\):true rule, \\(d_0\\), corresponding causal parameter\n\\(E_{U,X}[Y_{d_0(V)}]\\) denoting expected outcome \ntrue treatment rule \\(d_0(V)\\).true rule, \\(d_0\\), corresponding causal parameter\n\\(E_{U,X}[Y_{d_0(V)}]\\) denoting expected outcome \ntrue treatment rule \\(d_0(V)\\).estimated rule, \\(d_n\\), corresponding causal parameter\n\\(E_{U,X}[Y_{d_n(V)}]\\) denoting expected outcome \nestimated treatment rule \\(d_n(V)\\).estimated rule, \\(d_n\\), corresponding causal parameter\n\\(E_{U,X}[Y_{d_n(V)}]\\) denoting expected outcome \nestimated treatment rule \\(d_n(V)\\).optimal individualized rule rule maximal value:\n\\[d_{opt}(V) \\equiv \\text{argmax}_{d(V) \\\\mathcal{D}}\nE_{P_{U,X}}[Y_{d(V)}].\\]causal target parameter interest expected outcome \nestimated optimal individualized rule:\\[\\Psi_{d_{n, \\text{opt}}(V)}(P_{U,X}) := E_{P_{U,X}}[Y_{d_{n,\n\\text{opt}}(V)}].\\]","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"identification-and-statistical-estimand","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.4.1 Identification and Statistical Estimand","text":"step roadmap requires make assumptions:Strong ignorability: \\(\\) independent \\(Y^{d_n(v)} \\mid W\\), \\(\\\\mathcal{}\\).Strong ignorability: \\(\\) independent \\(Y^{d_n(v)} \\mid W\\), \\(\\\\mathcal{}\\).Positivity (overlap): \\(P_0(\\min_{\\\\mathcal{}} g_0(\\mid W) > 0) = 1\\)Positivity (overlap): \\(P_0(\\min_{\\\\mathcal{}} g_0(\\mid W) > 0) = 1\\)causal assumptions, can identify causal target parameter\nobserved data using G-computation formula.value individualized\nrule can now expressed \\[E_0[Y_{d_n(V)}] = E_{0,W}[\\bar{Q}_{Y,0}(=d_n(V),W)].\\]\nFinally, statistical counterpart causal parameter interest \ndefined :\\[\\psi_0 = E_{0,W}[\\bar{Q}_{Y,0}(=d_{n,\\text{opt}}(V),W)].\\]\n","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"high-level-idea","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.4.2 High-level idea","text":"Learn optimal ITR using Super Learner.Estimate value cross-validated Targeted Minimum Loss-based\nEstimator (CV-TMLE).","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"why-cv-tmle","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.4.3 Why CV-TMLE?","text":"CV-TMLE necessary non-cross-validated TMLE\nbiased upward mean outcome rule, therefore overly optimistic.CV-TMLE necessary non-cross-validated TMLE\nbiased upward mean outcome rule, therefore overly optimistic.generally however, using CV-TMLE allows us freedom estimation therefore greater\ndata adaptivity, without sacrificing inference.generally however, using CV-TMLE allows us freedom estimation therefore greater\ndata adaptivity, without sacrificing inference.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"binary-treatment","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.5 Binary Treatment","text":"case binary treatment, key quantity optimal ITR blip function.case binary treatment, key quantity optimal ITR blip function.Optimal ITR assigns treatment individuals falling strata \nstratum specific average treatment effect, blip function, positive.Optimal ITR assigns treatment individuals falling strata \nstratum specific average treatment effect, blip function, positive.Consequently, assign treatment individuals \nquantity negative.Consequently, assign treatment individuals \nquantity negative.define blip function :\\[\\bar{Q}_0(V) \\equiv E_0[Y_1-Y_0|V] \\equiv E_0[\\bar{Q}_{Y,0}(1,W) - \\bar{Q}_{Y,0}(0,W) | V], \\]\naverage treatment effect within stratum \\(V\\).Optimal individualized rule can now derived :\n\\[d_{opt}(V) = (\\bar{Q}_{0}(V) > 0).\\]follow steps order obtain value ITR:Estimate \\(\\bar{Q}_{Y,0}(,W)\\) \\(g_0(|W)\\) using sl3. denote estimates\n\\(\\bar{Q}_{Y,n}(,W)\\) \\(g_n(|W)\\).Apply doubly robust Augmented-Inverse Probability Weighted (-IPW) transform \noutcome, define:\\[D_{\\bar{Q}_Y,g,}(O) \\equiv \\frac{(=)}{g(|W)} (Y-\\bar{Q}_Y(,W)) + \\bar{Q}_Y(=,W)\\]\nnotes -IPW transform:randomization positivity assumptions, \n\\(E[D_{\\bar{Q}_Y,g,}(O) | V] = E[Y_a |V].\\)randomization positivity assumptions, \n\\(E[D_{\\bar{Q}_Y,g,}(O) | V] = E[Y_a |V].\\)Due double robust nature\n-IPW transform, consistency \\(E[Y_a |V]\\) depend correct estimation\neither \\(\\bar{Q}_{Y,0}(,W)\\) \\(g_0(|W)\\).Due double robust nature\n-IPW transform, consistency \\(E[Y_a |V]\\) depend correct estimation\neither \\(\\bar{Q}_{Y,0}(,W)\\) \\(g_0(|W)\\).randomized trial, \nguaranteed consistent estimate \\(E[Y_a |V]\\) even get \\(\\bar{Q}_{Y,0}(,W)\\) wrong!randomized trial, \nguaranteed consistent estimate \\(E[Y_a |V]\\) even get \\(\\bar{Q}_{Y,0}(,W)\\) wrong!Using transform, can define following contrast:\\[D_{\\bar{Q}_Y,g}(O) = D_{\\bar{Q}_Y,g,=1}(O) - D_{\\bar{Q}_Y,g,=0}(O).\\]\nestimate blip function, \\(\\bar{Q}_{0,}(V)\\), regressing \\(D_{\\bar{Q}_Y,g}(O)\\) \\(V\\).estimated rule \\(d(V) = \\text{argmax}_{\\\\mathcal{}} \\bar{Q}_{0,}(V)\\).Finally, get mean OIT:obtain inference mean outcome estimated optimal rule using CV-TMLE.combined:Estimate \\(\\bar{Q}_{Y,0}(,W)\\) \\(g_0(|W)\\) using sl3. denote estimates\n\\(\\bar{Q}_{Y,n}(,W)\\) \\(g_n(|W)\\).Apply doubly robust Augmented-Inverse Probability Weighted (-IPW) transform \noutcome. estimated rule \\(d(V) = \\text{argmax}_{\\\\mathcal{}} \\bar{Q}_{0,}(V)\\).obtain inference mean outcome estimated optimal rule using CV-TMLE.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"causal-effect-of-oit-with-binary-a","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.5.1 Causal Effect of OIT with Binary A","text":"start, let us load packages use set seed simulation:","code":"\nlibrary(data.table)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(tmle3mopttx)\nlibrary(devtools)\nlibrary(here)\nset.seed(111)"},{"path":"optimal-individualized-treatment-regimes.html","id":"simulate-data","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.5.1.1 Simulate Data","text":"data generating distribution following form:\\[W \\sim \\mathcal{N}(\\bf{0},I_{3 \\times 3})\\]\n\\[P(=1|W) = \\frac{1}{1+\\exp^{(-0.8*W_1)}}\\]\\[\\begin{align}\n  P(Y=1|,W) &= 0.5\\text{logit}^{-1}[-5I(=1)(W_1-0.5) \\\\\n  &+ 5I(=0)(W_1-0.5)] +0.5\\text{logit}^{-1}(W_2W_3)\n\\end{align}\\]composes observed data structure \\(O = (W, , Y)\\).composes observed data structure \\(O = (W, , Y)\\).Note mean OIT \\(\\psi_0=0.578\\) data generating\ndistribution.Note mean OIT \\(\\psi_0=0.578\\) data generating\ndistribution.Next, specify role variable data set plays nodes DAG.now observed data structure (data), specification role\nvariable data set plays nodes DAG.","code":"head(data)\n          W1       W2       W3 A Y\n1: -0.591031 -0.40168  0.15008 1 0\n2:  0.026594 -0.37093  0.79472 0 0\n3: -1.516553 -0.42515  0.43203 1 0\n4: -1.362653  0.44115  0.34370 1 1\n5:  1.178489 -0.67275  0.38710 1 0\n6: -0.934151  0.41669 -0.78808 1 1# organize data and nodes for tmle3\nnode_list <- list(\n  W = c(\"W1\", \"W2\", \"W3\"),\n  A = \"A\",\n  Y = \"Y\"\n)\nnode_list\n$W\n[1] \"W1\" \"W2\" \"W3\"\n\n$A\n[1] \"A\"\n\n$Y\n[1] \"Y\""},{"path":"optimal-individualized-treatment-regimes.html","id":"constructing-stacked-regressions-with-sl3","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.5.1.2 Constructing Stacked Regressions with sl3","text":"generate three different ensemble learners must fit.learners outcome regression,learners outcome regression,propensity score, andpropensity score, andblip function.blip function.make explicit respect standard\nnotation bundling ensemble learners list object :","code":"\n# Define sl3 library and metalearners:\nlrn_mean  <- Lrnr_mean$new()\nlrn_glm   <- Lrnr_glm_fast$new()\nlrn_lasso <- Lrnr_glmnet$new()\nlrnr_hal  <- Lrnr_hal9001$new(reduce_basis=1/sqrt(nrow(data)) )\n\n## Define the Q learner:\nQ_learner <- Lrnr_sl$new(\n  learners = list(lrn_lasso, lrn_mean, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n## Define the g learner:\ng_learner <- Lrnr_sl$new(\n  learners = list(lrn_lasso, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n## Define the B learner:\nb_learner <- Lrnr_sl$new(\n  learners = list(lrn_mean, lrn_glm, lrn_lasso),\n  metalearner = Lrnr_nnls$new()\n)# specify outcome and treatment regressions and create learner list\nlearner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)\nlearner_list\n$Y\n[1] \"Super learner:\"\nList of 3\n $ : chr \"Lrnr_glmnet_NULL_deviance_10_1_100_TRUE\"\n $ : chr \"Lrnr_mean\"\n $ : chr \"Lrnr_glm_fast_TRUE_Cholesky\"\n\n$A\n[1] \"Super learner:\"\nList of 2\n $ : chr \"Lrnr_glmnet_NULL_deviance_10_1_100_TRUE\"\n $ : chr \"Lrnr_glm_fast_TRUE_Cholesky\"\n\n$B\n[1] \"Super learner:\"\nList of 3\n $ : chr \"Lrnr_mean\"\n $ : chr \"Lrnr_glm_fast_TRUE_Cholesky\"\n $ : chr \"Lrnr_glmnet_NULL_deviance_10_1_100_TRUE\""},{"path":"optimal-individualized-treatment-regimes.html","id":"targeted-estimation","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.5.1.3 Targeted Estimation","text":"start, initialize specification TMLE parameter \ninterest simply calling tmle3_mopttx_blip_revere.specify argument V = c(\"W1\", \"W2\", \"W3\") order communicate \n’re interested learning rule dependent V covariates.specify argument V = c(\"W1\", \"W2\", \"W3\") order communicate \n’re interested learning rule dependent V covariates.also need specify type blip use estimation problem, \nlist learners used.also need specify type blip use estimation problem, \nlist learners used.addition, need specify whether want maximize minimize \nmean outcome rule (maximize=TRUE).addition, need specify whether want maximize minimize \nmean outcome rule (maximize=TRUE).can also get interpretable surrogate rule terms HAL:","code":"\n# initialize a tmle specification\ntmle_spec <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\"), type = \"blip1\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE,\n  realistic = FALSE, resource = 1, \n  interpret=TRUE\n)\n# fit the TML estimator\nfit <- tmle3(tmle_spec, data, node_list, learner_list)# see the result\nfit\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est       se   lower   upper\n1:  TSM E[Y_{A=NULL}]  0.35585  0.55755 0.025566 0.50744 0.60765\n   psi_transformed lower_transformed upper_transformed\n1:         0.55755           0.50744           0.60765# Interpretable rule\nhead(tmle_spec$blip_fit_interpret$coef)\n[1]  0.368043  0.156435 -0.064386 -0.054062  0.042386  0.037879# Interpretable rule\nhead(tmle_spec$blip_fit_interpret$term)\n[1] \"(Intercept)\"                                                                                                  \n[2] \"[ I(W1 >= 0.899)*(W1 - 0.899)^1 ] * [ I(W2 >= -3.861)*(W2 - -3.861)^1 ]\"                                      \n[3] \"[ I(W1 >= 0.074)*(W1 - 0.074)^1 ] * [ I(W2 >= -3.861)*(W2 - -3.861)^1 ] * [ I(W3 >= -2.945)*(W3 - -2.945)^1 ]\"\n[4] \"[ I(W1 >= -1.418)*(W1 - -1.418)^1 ] * [ I(W2 >= -3.861)*(W2 - -3.861)^1 ]\"                                    \n[5] \"[ I(W1 >= 0.856)*(W1 - 0.856)^1 ] * [ I(W2 >= -3.861)*(W2 - -3.861)^1 ] * [ I(W3 >= -1.292)*(W3 - -1.292)^1 ]\"\n[6] \"[ I(W1 >= -1.368)*(W1 - -1.368)^1 ] * [ I(W2 >= -3.861)*(W2 - -3.861)^1 ] * [ I(W3 >= 0.843)*(W3 - 0.843)^1 ]\""},{"path":"optimal-individualized-treatment-regimes.html","id":"resource-constraint","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.5.1.4 Resource constraint","text":"can also restrict number individuals get treatment, even giving\ntreatment beneficial (according estimated blip).order impose resource constraint, specify percent\nindividuals \nbenefit getting treatment.order impose resource constraint, specify percent\nindividuals \nbenefit getting treatment.example, resource=1, \nindividuals blip higher zero get treatment.example, resource=1, \nindividuals blip higher zero get treatment.resource=0, none get treatment.resource=0, none get treatment.can compare number individuals got treatment without \nresource constraint:","code":"\n# initialize a tmle specification\ntmle_spec_resource <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\"), type = \"blip1\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE,\n  realistic = FALSE, resource = 0.80\n)\n# fit the TML estimator\nfit_resource <- tmle3(tmle_spec_resource, data, node_list, learner_list)# see the result\nfit_resource\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est       se   lower   upper\n1:  TSM E[Y_{A=NULL}]  0.32947  0.53611 0.025833 0.48548 0.58674\n   psi_transformed lower_transformed upper_transformed\n1:         0.53611           0.48548           0.58674# Number of individuals with A=1 (no resource constraint):\ntable(tmle_spec$return_rule)\n\n  0   1 \n280 720 \n\n# Number of individuals with A=1 (resource constraint):\ntable(tmle_spec_resource$return_rule)\n\n  0   1 \n422 578 "},{"path":"optimal-individualized-treatment-regimes.html","id":"empty-v","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.5.1.5 Empty V","text":"","code":"\n# initialize a tmle specification\ntmle_spec_V_empty <- tmle3_mopttx_blip_revere(\n  type = \"blip1\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE,\n  realistic = FALSE, resource = 1\n)\n# fit the TML estimator\nfit_V_empty <- tmle3(tmle_spec_V_empty, data, node_list, learner_list)# see the result:\nfit_V_empty\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est     se   lower   upper psi_transformed\n1:  TSM E[Y_{A=NULL}]  0.32699  0.53301 0.0104 0.51263 0.55339         0.53301\n   lower_transformed upper_transformed\n1:           0.51263           0.55339"},{"path":"optimal-individualized-treatment-regimes.html","id":"categorical-treatment","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.6 Categorical Treatment","text":"define pseudo-blips: vector valued entities output given\n\\(V\\) vector length equal number treatment categories, \\(n_A\\).define pseudo-blips: vector valued entities output given\n\\(V\\) vector length equal number treatment categories, \\(n_A\\)., define :\n\\[\\bar{Q}_0^{pblip}(V) = \\{\\bar{Q}_{0,}^{pblip}(V): \\\\mathcal{} \\}.\\], define :\n\\[\\bar{Q}_0^{pblip}(V) = \\{\\bar{Q}_{0,}^{pblip}(V): \\\\mathcal{} \\}.\\]implement three different pseudo-blips tmle3mopttx.Blip1 corresponds choosing reference category treatment, \ndefining blip categories relative specified reference:\n\\[\\bar{Q}_{0,}^{pblip-ref}(V) \\equiv E_0(Y_a-Y_0|V)\\]\nBlip1 corresponds choosing reference category treatment, \ndefining blip categories relative specified reference:\n\\[\\bar{Q}_{0,}^{pblip-ref}(V) \\equiv E_0(Y_a-Y_0|V)\\]\nBlip2 corresponds defining blip relative average \ncategories:\n\\[\\bar{Q}_{0,}^{pblip-avg}(V) \\equiv E_0(Y_a- \\frac{1}{n_A} \\sum_{\\\\mathcal{}} Y_a|V)\\]\nBlip2 corresponds defining blip relative average \ncategories:\n\\[\\bar{Q}_{0,}^{pblip-avg}(V) \\equiv E_0(Y_a- \\frac{1}{n_A} \\sum_{\\\\mathcal{}} Y_a|V)\\]\nBlip3 reflects extension Blip2, average now weighted average:\n\\[\\bar{Q}_{0,}^{pblip-wavg}(V) \\equiv E_0(Y_a- \\frac{1}{n_A} \\sum_{\\\\mathcal{}} Y_{} P(=|V)\n|V)\\]Blip3 reflects extension Blip2, average now weighted average:\n\\[\\bar{Q}_{0,}^{pblip-wavg}(V) \\equiv E_0(Y_a- \\frac{1}{n_A} \\sum_{\\\\mathcal{}} Y_{} P(=|V)\n|V)\\]","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"causal-effect-of-oit-with-categorical-a","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.6.1 Causal Effect of OIT with Categorical A","text":"now need pay attention type blip define estimation stage,\nwell construct learners.","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"simulated-data","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.6.1.1 Simulated Data","text":"First, load simulated data. , data generating distribution \nfollowing form:\\[W \\sim \\mathcal{N}(\\bf{0},I_{4 \\times 4})\\]\n\\[P(|W) = \\frac{1}{1+\\exp^{(-(0.05*(=1)*W_1+0.8*(=2)*W_1+0.8*(=3)*W_1))}}\\]\\[P(Y|,W)=0.5\\text{logit}^{-1}[15I(=1)(W_1-0.5) - 3I(=2)(2W_1+0.5) \\\\\n+ 3I(=3)(3W_1-0.5)] +\\text{logit}^{-1}(W_2W_1)\\]can just load data available part package follows:constructs observed data structure \\(O = (W, , Y)\\).constructs observed data structure \\(O = (W, , Y)\\).true mean OIT \\(\\psi=0.658\\), quantity aim\nestimate.true mean OIT \\(\\psi=0.658\\), quantity aim\nestimate.can see number observed categories treatment :","code":"head(data)\n   W1       W2       W3       W4 A Y\n1:  2 -0.40168  0.15008  1.44274 2 1\n2:  3 -0.37093  0.79472  1.08879 3 0\n3:  1 -0.42515  0.43203  0.22471 2 1\n4:  1  0.44115  0.34370  1.55538 3 0\n5:  3 -0.67275  0.38710 -0.31411 2 0\n6:  1  0.41669 -0.78808 -0.28718 2 1# organize data and nodes for tmle3\ndata <- data_cat_realistic\nnode_list <- list(\n  W = c(\"W1\", \"W2\", \"W3\", \"W4\"),\n  A = \"A\",\n  Y = \"Y\"\n)\nnode_list\n$W\n[1] \"W1\" \"W2\" \"W3\" \"W4\"\n\n$A\n[1] \"A\"\n\n$Y\n[1] \"Y\"# organize data and nodes for tmle3\ntable(data$A)\n\n  1   2   3 \n 24 528 448 "},{"path":"optimal-individualized-treatment-regimes.html","id":"constructing-stacked-regressions-with-sl3-1","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.6.1.2 Constructing Stacked Regressions with sl3","text":"QUESTION: categorical treatment, dimension blip now?\ndimension current example? go estimating ?need estimate \\(g_0(|W)\\) categorical \\(\\):\nuse multinomial Super Learner option available within sl3 package.need estimate \\(g_0(|W)\\) categorical \\(\\):\nuse multinomial Super Learner option available within sl3 package.need estimate blip using multivariate Super Learner\navailable within sl3 package.need estimate blip using multivariate Super Learner\navailable within sl3 package.order see learners can\nused estimate \\(g_0(|W)\\) sl3, run following:","code":"\n# Initialize few of the learners:\nlrn_xgboost_50 <- Lrnr_xgboost$new(nrounds = 50)\nlrn_mean <- Lrnr_mean$new()\nlrn_glm <- Lrnr_glm_fast$new()\n\n## Define the Q learner, which is just a regular learner:\nQ_learner <- Lrnr_sl$new(\n  learners = list(lrn_xgboost_50, lrn_mean, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n## Define the g learner, which is a multinomial learner:\n# specify the appropriate loss of the multinomial learner:\nmn_metalearner <- make_learner(Lrnr_solnp,\n  eval_function = loss_loglik_multinomial,\n  learner_function = metalearner_linear_multinomial\n)\ng_learner <- make_learner(Lrnr_sl, list(lrn_xgboost_50, lrn_mean), mn_metalearner)\n\n## Define the Blip learner, which is a multivariate learner:\nlearners <- list(lrn_xgboost_50, lrn_mean, lrn_glm)\nb_learner <- create_mv_learners(learners = learners)# See which learners support multi-class classification:\nsl3_list_learners(c(\"categorical\"))\n [1] \"Lrnr_bound\"                \"Lrnr_caret\"               \n [3] \"Lrnr_cv_selector\"          \"Lrnr_ga\"                  \n [5] \"Lrnr_glmnet\"               \"Lrnr_grf\"                 \n [7] \"Lrnr_gru_keras\"            \"Lrnr_h2o_glm\"             \n [9] \"Lrnr_h2o_grid\"             \"Lrnr_independent_binomial\"\n[11] \"Lrnr_lightgbm\"             \"Lrnr_lstm_keras\"          \n[13] \"Lrnr_mean\"                 \"Lrnr_multivariate\"        \n[15] \"Lrnr_nnet\"                 \"Lrnr_optim\"               \n[17] \"Lrnr_polspline\"            \"Lrnr_pooled_hazards\"      \n[19] \"Lrnr_randomForest\"         \"Lrnr_ranger\"              \n[21] \"Lrnr_rpart\"                \"Lrnr_screener_correlation\"\n[23] \"Lrnr_solnp\"                \"Lrnr_svm\"                 \n[25] \"Lrnr_xgboost\"             # specify outcome and treatment regressions and create learner list\nlearner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)\nlearner_list\n$Y\n[1] \"Super learner:\"\nList of 3\n $ : chr \"Lrnr_xgboost_50_1\"\n $ : chr \"Lrnr_mean\"\n $ : chr \"Lrnr_glm_fast_TRUE_Cholesky\"\n\n$A\n[1] \"Super learner:\"\nList of 2\n $ : chr \"Lrnr_xgboost_50_1\"\n $ : chr \"Lrnr_mean\"\n\n$B\n[1] \"Super learner:\"\nList of 3\n $ : chr \"Lrnr_xgboost_50_1_multivariate\"\n $ : chr \"Lrnr_mean_multivariate\"\n $ : chr \"Lrnr_glm_fast_TRUE_Cholesky_multivariate\""},{"path":"optimal-individualized-treatment-regimes.html","id":"targeted-estimation-1","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.6.1.3 Targeted Estimation","text":"can see confidence interval covers truth.","code":"\n# initialize a tmle specification\ntmle_spec_cat <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\", \"W4\"), type = \"blip2\",\n  learners = learner_list, maximize = TRUE, complex = TRUE,\n  realistic = FALSE\n)\n# fit the TML estimator\nfit_cat <- tmle3(tmle_spec_cat, data, node_list, learner_list)# see the result:\nfit_cat\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est       se   lower   upper\n1:  TSM E[Y_{A=NULL}]  0.54334  0.61973 0.063746 0.49479 0.74467\n   psi_transformed lower_transformed upper_transformed\n1:         0.61973           0.49479           0.74467\n\n# How many individuals got assigned each treatment?\ntable(tmle_spec_cat$return_rule)\n\n  1   2   3 \n438 380 182 "},{"path":"optimal-individualized-treatment-regimes.html","id":"extensions","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.7 Extensions","text":"consider multiple extensions procedure described \nestimating value ITR.One might interested grid possible suboptimal rules, corresponding \npotentially limited knowledge potential effect modifiers (Simpler Rules).Certain regimes might preferred, due positivity restraints realistic implement (Realistic Interventions).","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"simpler-rules","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.7.1 Simpler Rules","text":"define \\(S\\)-optimal rules optimal rule considers possible subsets\n\\(V\\) covariates, card(\\(S\\)) \\(\\leq\\) card(\\(V\\)) \\(\\emptyset \\S\\).allows us consider sub-optimal rules easier estimate:\nallow statistical inference counterfactual mean outcome sub-optimal rule.Even though user specified baseline covariates basis\nrule estimation, simpler rule sufficient \nmaximize mean optimal individualized treatment.QUESTION: set covariates picked tmle3mopttx\ncompare baseline covariates true rule depends ?","code":"\n# initialize a tmle specification\ntmle_spec_cat_simple <- tmle3_mopttx_blip_revere(\n  V = c(\"W4\", \"W3\", \"W2\", \"W1\"), type = \"blip2\",\n  learners = learner_list,\n  maximize = TRUE, complex = FALSE, realistic = FALSE\n)\n# fit the TML estimator\nfit_cat_simple <- tmle3(tmle_spec_cat_simple, data, node_list, learner_list)# see the result:\nfit_cat_simple\nA tmle3_Fit that took 1 step(s)\n   type                param init_est tmle_est      se   lower   upper\n1:  TSM E[Y_{d(V=W3,W2,W1)}]  0.55423  0.62196 0.05204 0.51997 0.72396\n   psi_transformed lower_transformed upper_transformed\n1:         0.62196           0.51997           0.72396"},{"path":"optimal-individualized-treatment-regimes.html","id":"realistic-optimal-individual-regimes","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.7.2 Realistic Optimal Individual Regimes","text":"tmle3mopttx also provides option estimate mean \nrealistic, implementable, optimal individualized treatment.often case assigning particular regime might optimize\ndesired outcome, due \nglobal practical positivity constrains, treatment\ncan never implemented (highly unlikely).often case assigning particular regime might optimize\ndesired outcome, due \nglobal practical positivity constrains, treatment\ncan never implemented (highly unlikely).Specifying realistic=\"TRUE\", consider possibly suboptimal\ntreatments optimize outcome question \nsupported data.Specifying realistic=\"TRUE\", consider possibly suboptimal\ntreatments optimize outcome question \nsupported data.","code":"\n# initialize a tmle specification\ntmle_spec_cat_realistic <- tmle3_mopttx_blip_revere(\n  V = c(\"W4\", \"W3\", \"W2\", \"W1\"), type = \"blip2\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE, realistic = TRUE\n)\n# fit the TML estimator\nfit_cat_realistic <- tmle3(tmle_spec_cat_realistic, data, node_list, learner_list)# see the result:\nfit_cat_realistic\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est       se  lower   upper psi_transformed\n1:  TSM E[Y_{A=NULL}]   0.5546  0.57351 0.059035 0.4578 0.68921         0.57351\n   lower_transformed upper_transformed\n1:            0.4578           0.68921\n\n# How many individuals got assigned each treatment?\ntable(tmle_spec_cat_realistic$return_rule)\n\n  1   2   3 \n  5 511 484 "},{"path":"optimal-individualized-treatment-regimes.html","id":"missingness-and-tmle3mopttx","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.7.3 Missingness and tmle3mopttx","text":"section, present use tmle3mopttx package data subject\nmissingness. Currently support missing process following nodes:outcome node, \\(Y\\);types missingness handled sl3 package. Let’s add missingness \noutcome.start, must first add library.Now delta_Y fits missing outcome process.","code":"\ndata_missing <- data_cat_realistic\n\n#Add some random missingless:\nrr <- sample(nrow(data_missing), 100, replace = FALSE)\ndata_missing[rr,\"Y\"]<-NA# look at the data again:\nsummary(data_missing$Y)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    0.00    0.00    0.47    1.00    1.00     100 delta_learner <- Lrnr_sl$new(\n  learners = list(lrn_mean, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n# specify outcome and treatment regressions and create learner list\nlearner_list <- list(Y = Q_learner, A = g_learner, B = b_learner, delta_Y=delta_learner)\nlearner_list\n$Y\n[1] \"Super learner:\"\nList of 3\n $ : chr \"Lrnr_xgboost_50_1\"\n $ : chr \"Lrnr_mean\"\n $ : chr \"Lrnr_glm_fast_TRUE_Cholesky\"\n\n$A\n[1] \"Super learner:\"\nList of 2\n $ : chr \"Lrnr_xgboost_50_1\"\n $ : chr \"Lrnr_mean\"\n\n$B\n[1] \"Super learner:\"\nList of 3\n $ : chr \"Lrnr_xgboost_50_1_multivariate\"\n $ : chr \"Lrnr_mean_multivariate\"\n $ : chr \"Lrnr_glm_fast_TRUE_Cholesky_multivariate\"\n\n$delta_Y\n[1] \"Super learner:\"\nList of 2\n $ : chr \"Lrnr_mean\"\n $ : chr \"Lrnr_glm_fast_TRUE_Cholesky\"\n# initialize a tmle specification\ntmle_spec_cat_miss <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\", \"W4\"), type = \"blip2\",\n  learners = learner_list, maximize = TRUE, complex = TRUE,\n  realistic = FALSE\n)\n# fit the TML estimator\nfit_cat_miss <- tmle3(tmle_spec_cat_miss, data_missing, node_list, learner_list)fit_cat_miss\nA tmle3_Fit that took 1 step(s)\n   type                    param init_est tmle_est       se   lower   upper\n1:  TSM E[Y_{A=NULL, delta_Y=1}]  0.55533  0.81095 0.017859 0.77595 0.84595\n   psi_transformed lower_transformed upper_transformed\n1:         0.81095           0.77595           0.84595"},{"path":"optimal-individualized-treatment-regimes.html","id":"variable-importance-analysis","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.7.4 Variable Importance Analysis","text":"order run tmle3mopttx variable importance measure, \nneed considered covariates categorical variables.illustration purpose, bin baseline covariates corresponding\ndata-generating distribution described previous section:Note node list now includes \\(W_1\\) treatments well!Note node list now includes \\(W_1\\) treatments well!Don’t worry, still properly adjust baseline covariates \nconsidering \\(\\) treatment.Don’t worry, still properly adjust baseline covariates \nconsidering \\(\\) treatment.","code":"# bin baseline covariates to 3 categories:\ndata$W1<-ifelse(data$W1<quantile(data$W1)[2],1,ifelse(data$W1<quantile(data$W1)[3],2,3))\n\nnode_list <- list(\n  W = c(\"W3\", \"W4\", \"W2\"),\n  A = c(\"W1\", \"A\"),\n  Y = \"Y\"\n)\nnode_list\n$W\n[1] \"W3\" \"W4\" \"W2\"\n\n$A\n[1] \"W1\" \"A\" \n\n$Y\n[1] \"Y\""},{"path":"optimal-individualized-treatment-regimes.html","id":"targeted-estimation-2","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.7.4.1 Targeted Estimation","text":"initialize specification TMLE parameter \ninterest (called tmle3_Spec tlverse nomenclature) simply calling\ntmle3_mopttx_vim.final result tmle3_vim tmle3mopttx spec ordered list\nmean outcomes optimal individualized treatment categorical\ncovariates dataset.","code":"\n# initialize a tmle specification\ntmle_spec_vim <- tmle3_mopttx_vim(\n  V=c(\"W2\"),\n  type = \"blip2\",\n  learners = learner_list,\n  maximize = FALSE,\n  method = \"SL\",\n  complex = TRUE,\n  realistic = FALSE\n)\n\n# fit the TML estimator\nvim_results <- tmle3_vim(tmle_spec_vim, data, node_list, learner_list,\n  adjust_for_other_A = TRUE\n)# see results:\nprint(vim_results)\n   type                param   init_est  tmle_est       se      lower\n1:  ATE E[Y_{A=NULL}] - E[Y] -0.0128837 -0.051833 0.021934 -0.0948231\n2:  ATE E[Y_{A=NULL}] - E[Y] -0.0022747  0.036924 0.017144  0.0033228\n        upper psi_transformed lower_transformed upper_transformed  A\n1: -0.0088424       -0.051833        -0.0948231        -0.0088424 W1\n2:  0.0705248        0.036924         0.0033228         0.0705248  A\n             W  Z_stat      p_nz p_nz_corrected\n1:  W3,W4,W2,A -2.3631 0.0090615       0.015629\n2: W3,W4,W2,W1  2.1538 0.0156285       0.015629"},{"path":"optimal-individualized-treatment-regimes.html","id":"exercise","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.8 Exercise","text":"","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"real-world-data-and-tmle3mopttx","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.8.1 Real World Data and tmle3mopttx","text":"Finally, cement everything learned far real data application.previous sections, using WASH Benefits data,\ncorresponding effect water quality, sanitation, hand washing, \nnutritional interventions child development rural Bangladesh trial.main aim cluster-randomized controlled trial assess \nimpact six intervention groups, including:ControlControlHandwashing soapHandwashing soapImproved nutrition counselling provision lipid-based nutrient supplementsImproved nutrition counselling provision lipid-based nutrient supplementsCombined water, sanitation, handwashing, nutrition.Combined water, sanitation, handwashing, nutrition.Improved sanitationImproved sanitationCombined water, sanitation, handwashingCombined water, sanitation, handwashingChlorinated drinking waterChlorinated drinking waterWe aim estimate optimal ITR corresponding value optimal ITR\nmain intervention WASH Benefits data.outcome interest weight--height Z-score, whereas treatment \nsix intervention groups aimed improving living conditions.Questions:Define \\(V\\) mother’s education (momedu), current living conditions (floor),\npossession material items including refrigerator (asset_refrig).\nwant minimize maximize outcome? blip type use?\nConstruct appropriate sl3 library \\(\\), \\(Y\\) \\(B\\).Define \\(V\\) mother’s education (momedu), current living conditions (floor),\npossession material items including refrigerator (asset_refrig).\nwant minimize maximize outcome? blip type use?\nConstruct appropriate sl3 library \\(\\), \\(Y\\) \\(B\\).Based \\(V\\) defined previous question, estimate mean ITR \nmain randomized intervention used WASH Benefits trial\nweight--height Z-score outcome. ’s TMLE value optimal ITR?\nchange initial estimate? intervention dominant?\nthink ?Based \\(V\\) defined previous question, estimate mean ITR \nmain randomized intervention used WASH Benefits trial\nweight--height Z-score outcome. ’s TMLE value optimal ITR?\nchange initial estimate? intervention dominant?\nthink ?Using formulation questions 1 2, estimate realistic optimal ITR\ncorresponding value realistic ITR. results change? intervention\ndominant realistic rules? think ?Using formulation questions 1 2, estimate realistic optimal ITR\ncorresponding value realistic ITR. results change? intervention\ndominant realistic rules? think ?","code":""},{"path":"optimal-individualized-treatment-regimes.html","id":"solutions","chapter":"5 Optimal Individualized Treatment Regimes","heading":"5.8.2 Solutions","text":"start, let’s load data, convert columns class numeric,\ntake quick look :, specify NPSEM via node_list object.pick potential effect modifiers, including mother’s education, current\nliving conditions (floor), possession material items including refrigerator.\nconcentrate covariates might indicative socio-economic status\nindividuals involved trial. can explore distribution \\(V\\), \\(\\) \\(Y\\):specify simple library outcome regression, propensity score\nblip function. Since treatment categorical, need define \nmultinomial learner \\(\\) multivariate learner \\(B\\). \ndefine xgboost grid parameters, initialize mean learner.seen , initialize tmle3_mopttx_blip_revere Spec order \nanswer Question 1. want maximize outcome, higher weight--height Z-score\nbetter. also use blip2 blip type, note used blip1\nwell since “Control” good reference category.Using formulation , estimate realistic optimal ITR\ncorresponding value realistic ITR:","code":"\nwashb_data <- fread(\"https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv\", stringsAsFactors = TRUE)\nwashb_data <- washb_data[!is.na(momage), lapply(.SD, as.numeric)]\nhead(washb_data, 3)\nnode_list <- list(W = names(washb_data)[!(names(washb_data) %in% c(\"whz\", \"tr\", \"momheight\"))],\n                  A = \"tr\", Y = \"whz\")\n#V1, V2 and V3:\ntable(washb_data$momedu)\ntable(washb_data$floor)\ntable(washb_data$asset_refrig)\n\n#A:\ntable(washb_data$tr)\n\n#Y:\nsummary(washb_data$whz)\n# Initialize few of the learners:\ngrid_params = list(nrounds = c(100, 500),\n                     eta = c(0.01, 0.1))\ngrid = expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)\nxgb_learners = apply(grid, MARGIN = 1, function(params_tune) {\n    do.call(Lrnr_xgboost$new, c(as.list(params_tune)))\n  })\nlrn_mean <- Lrnr_mean$new()\n\n## Define the Q learner, which is just a regular learner:\nQ_learner <- Lrnr_sl$new(\n  learners = list(xgb_learners[[1]], xgb_learners[[2]], xgb_learners[[3]],\n                  xgb_learners[[4]], lrn_mean),\n  metalearner = Lrnr_nnls$new()\n)\n\n## Define the g learner, which is a multinomial learner:\n#specify the appropriate loss of the multinomial learner:\nmn_metalearner <- make_learner(Lrnr_solnp, loss_function = loss_loglik_multinomial,\n                               learner_function = metalearner_linear_multinomial)\ng_learner <- make_learner(Lrnr_sl, list(xgb_learners[[4]], lrn_mean), mn_metalearner)\n\n## Define the Blip learner, which is a multivariate learner:\nlearners <- list(xgb_learners[[1]], xgb_learners[[2]], xgb_learners[[3]],\n                  xgb_learners[[4]], lrn_mean)\nb_learner <- create_mv_learners(learners = learners)\n\nlearner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)\n## Question 2:\n\n#Initialize a tmle specification\ntmle_spec_Q <- tmle3_mopttx_blip_revere(\n  V = c(\"momedu\", \"floor\", \"asset_refrig\"), type = \"blip2\",\n  learners = learner_list, maximize = TRUE, complex = TRUE,\n  realistic = FALSE\n)\n\n#Fit the TML estimator.\nfit_Q <- tmle3(tmle_spec_Q, data=washb_data, node_list, learner_list)\nfit_Q\n\n#Which intervention is the most dominant? \ntable(tmle_spec_Q$return_rule)\n## Question 3:\n\n#Initialize a tmle specification with \"realistic=TRUE\":\ntmle_spec_Q_realistic <- tmle3_mopttx_blip_revere(\n  V = c(\"momedu\", \"floor\", \"asset_refrig\"), type = \"blip2\",\n  learners = learner_list, maximize = TRUE, complex = TRUE,\n  realistic = TRUE\n)\n\n#Fit the TML estimator.\nfit_Q_realistic <- tmle3(tmle_spec_Q_realistic, data=washb_data, node_list, learner_list)\nfit_Q_realistic\n\ntable(tmle_spec_Q_realistic$return_rule)"},{"path":"stochastic-treatment-regimes-optional.html","id":"stochastic-treatment-regimes-optional","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6 Stochastic Treatment Regimes (optional)","text":"Nima HejaziBased tmle3shift R package.Updated: 2022-05-23","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"learning-objectives-4","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.1 Learning Objectives","text":"Differentiate stochastic treatment regimes static, dynamic, optimal\ntreatment regimes.Describe estimating causal effects stochastic interventions informs \nreal-world data analysis.Contrast population level stochastic intervention policy modified\ntreatment policy.Estimate causal effects stochastic treatment regimes \ntmle3shift R package.Specify grid counterfactual shift interventions used defining\nset stochastic intervention policies.Interpret set effect estimates grid counterfactual shift\ninterventions.Construct marginal structural models measure variable importance terms\nstochastic interventions, using grid shift interventions.Implement shift intervention individual level, facilitate\nshifting individual value ’s supported data.Define novel shift intervention functions extend tmle3shift R\npackage.","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"introduction-2","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.2 Introduction","text":"section, examine simple example stochastic treatment regimes \ncontext continuous treatment variable interest, defining \nintuitive causal effect examine stochastic interventions \ngenerally. first step using stochastic\ntreatment regimes practice, present tmle3shift R\npackage, features \nimplementation recently developed algorithm computing targeted minimum\nloss-based estimates causal effect based stochastic treatment regime\nshifts natural value treatment based shifting function\n\\(d(,W)\\). also use tmle3shift construct marginal structural models\nvariable importance measures, implement shift interventions \nindividual level, define novel shift intervention functions.","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"stochastic-interventions","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.3 Stochastic Interventions","text":"Present relatively simple yet extremely flexible manner realistic\ncausal effects (contrasts thereof) may defined.Present relatively simple yet extremely flexible manner realistic\ncausal effects (contrasts thereof) may defined.May applied nearly manner treatment variable – continuous,\nordinal, categorical, binary – allowing rich set causal effects \ndefined formalism.May applied nearly manner treatment variable – continuous,\nordinal, categorical, binary – allowing rich set causal effects \ndefined formalism.Arguably general classes interventions causal\neffects may defined, conceptually simple.Arguably general classes interventions causal\neffects may defined, conceptually simple.may consider stochastic interventions two ways:\nequation \\(f_A\\), produces \\(\\), replaced probabilistic\nmechanism \\(g_{,\\delta}(\\mid W)\\) differs original \\(g_A(\\mid W)\\). stochastically modified value treatment \\(A_{\\delta}\\)\ndrawn user-specified distribution \\(g_{,\\delta}(\\mid W)\\), \nmay depend original distribution \\(g_A(\\mid W)\\) indexed \nuser-specified parameter \\(\\delta\\). case, stochastically\nmodified value treatment \\(A_{\\delta} \\sim g_{,\\delta}(\\cdot \\mid W)\\).\nobserved value \\(\\) replaced new value \\(A_{d(,W)}\\) based \napplying user-defined function \\(d(,W)\\) \\(\\). case, \nstochastic treatment regime may viewed intervention \\(\\)\nset equal value based hypothetical regime \\(d(, W)\\), \nregime \\(d\\) depends treatment level \\(\\) assigned \nabsence regime well covariates \\(W\\). Stochastic\ninterventions variety may referred depending \nnatural value treatment modified treatment policies\n(Haneuse Rotnitzky 2013; Young, Hernán, Robins 2014).\nmay consider stochastic interventions two ways:equation \\(f_A\\), produces \\(\\), replaced probabilistic\nmechanism \\(g_{,\\delta}(\\mid W)\\) differs original \\(g_A(\\mid W)\\). stochastically modified value treatment \\(A_{\\delta}\\)\ndrawn user-specified distribution \\(g_{,\\delta}(\\mid W)\\), \nmay depend original distribution \\(g_A(\\mid W)\\) indexed \nuser-specified parameter \\(\\delta\\). case, stochastically\nmodified value treatment \\(A_{\\delta} \\sim g_{,\\delta}(\\cdot \\mid W)\\).equation \\(f_A\\), produces \\(\\), replaced probabilistic\nmechanism \\(g_{,\\delta}(\\mid W)\\) differs original \\(g_A(\\mid W)\\). stochastically modified value treatment \\(A_{\\delta}\\)\ndrawn user-specified distribution \\(g_{,\\delta}(\\mid W)\\), \nmay depend original distribution \\(g_A(\\mid W)\\) indexed \nuser-specified parameter \\(\\delta\\). case, stochastically\nmodified value treatment \\(A_{\\delta} \\sim g_{,\\delta}(\\cdot \\mid W)\\).observed value \\(\\) replaced new value \\(A_{d(,W)}\\) based \napplying user-defined function \\(d(,W)\\) \\(\\). case, \nstochastic treatment regime may viewed intervention \\(\\)\nset equal value based hypothetical regime \\(d(, W)\\), \nregime \\(d\\) depends treatment level \\(\\) assigned \nabsence regime well covariates \\(W\\). Stochastic\ninterventions variety may referred depending \nnatural value treatment modified treatment policies\n(Haneuse Rotnitzky 2013; Young, Hernán, Robins 2014).observed value \\(\\) replaced new value \\(A_{d(,W)}\\) based \napplying user-defined function \\(d(,W)\\) \\(\\). case, \nstochastic treatment regime may viewed intervention \\(\\)\nset equal value based hypothetical regime \\(d(, W)\\), \nregime \\(d\\) depends treatment level \\(\\) assigned \nabsence regime well covariates \\(W\\). Stochastic\ninterventions variety may referred depending \nnatural value treatment modified treatment policies\n(Haneuse Rotnitzky 2013; Young, Hernán, Robins 2014).","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"identifying-the-causal-effect-of-a-stochastic-intervention","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.3.1 Identifying the Causal Effect of a Stochastic Intervention","text":"stochastic intervention generates counterfactual random variable\n\\(Y_{d(,W)} := f_Y(d(,W), W, U_Y) \\equiv Y_{g_{,\\delta}} := f_Y(A_{\\delta}, W, U_Y)\\), \\(Y_{d(,W)} \\sim P_0^{\\delta}\\), \\(P_0^{\\delta}\\) \ncounterfactual distribution implied intervention \\(\\).stochastic intervention generates counterfactual random variable\n\\(Y_{d(,W)} := f_Y(d(,W), W, U_Y) \\equiv Y_{g_{,\\delta}} := f_Y(A_{\\delta}, W, U_Y)\\), \\(Y_{d(,W)} \\sim P_0^{\\delta}\\), \\(P_0^{\\delta}\\) \ncounterfactual distribution implied intervention \\(\\).target causal estimand analysis \\(\\psi_{0, \\delta} := \\mathbb{E}_{P_0^{\\delta}}\\{Y_{d(,W)}\\}\\), mean counterfactual\noutcome variable \\(Y_{d(, W)}\\). statistical target parameter may also \ndenoted \\(\\Psi(P_0) = \\mathbb{E}_{P_0}{\\overline{Q}_{Y,0}(d(, W), W)}\\), \n\\(\\overline{Q}_{Y,0}(d(, W), W)\\) counterfactual outcome value \ngiven individual stochastic intervention distribution\n(Dı́az van der Laan 2018).target causal estimand analysis \\(\\psi_{0, \\delta} := \\mathbb{E}_{P_0^{\\delta}}\\{Y_{d(,W)}\\}\\), mean counterfactual\noutcome variable \\(Y_{d(, W)}\\). statistical target parameter may also \ndenoted \\(\\Psi(P_0) = \\mathbb{E}_{P_0}{\\overline{Q}_{Y,0}(d(, W), W)}\\), \n\\(\\overline{Q}_{Y,0}(d(, W), W)\\) counterfactual outcome value \ngiven individual stochastic intervention distribution\n(Dı́az van der Laan 2018).prior work, Dı́az van der Laan (2012) showed causal quantity interest\n\\(\\mathbb{E}_{P_0} \\{Y_{d(, W)}\\}\\) identified functional \ndistribution observed data \\(O\\):\n\\[\\begin{align*}\\label{eqn:identification2012}\n  \\psi_{0,\\delta} = \\int_{\\mathcal{W}} \\int_{\\mathcal{}} & \\mathbb{E}_{P_0}\n   \\{Y \\mid = d(, w), W = w\\} \\\\ &g_{,0}(\\mid W = w) q_{W,0}(w)\n   d\\mu()d\\nu(w).\n\\end{align*}\\]prior work, Dı́az van der Laan (2012) showed causal quantity interest\n\\(\\mathbb{E}_{P_0} \\{Y_{d(, W)}\\}\\) identified functional \ndistribution observed data \\(O\\):\n\\[\\begin{align*}\\label{eqn:identification2012}\n  \\psi_{0,\\delta} = \\int_{\\mathcal{W}} \\int_{\\mathcal{}} & \\mathbb{E}_{P_0}\n   \\{Y \\mid = d(, w), W = w\\} \\\\ &g_{,0}(\\mid W = w) q_{W,0}(w)\n   d\\mu()d\\nu(w).\n\\end{align*}\\]Two standard assumptions necessary order establish identifiability\ncausal parameter observed data via statistical functional\n. \n\nDefinition 6.1  (Strong Ignorability) \\(A_i \\perp \\!\\!\\! \\perp Y^{d(a_i, w_i)}_i \\mid W_i\\), \\(= 1, \\ldots, n\\).\n\n\nDefinition 6.2  (Treatment Positivity (Overlap)) \\(a_i \\\\mathcal{} \\implies d(a_i, w_i) \\\\mathcal{}\\) \n\\(w \\\\mathcal{W}\\), \\(\\mathcal{}\\) denotes support \n\\(\\mid W = w_i \\quad \\forall = 1, \\ldots n\\).\nTwo standard assumptions necessary order establish identifiability\ncausal parameter observed data via statistical functional\n. wereDefinition 6.1  (Strong Ignorability) \\(A_i \\perp \\!\\!\\! \\perp Y^{d(a_i, w_i)}_i \\mid W_i\\), \\(= 1, \\ldots, n\\).Definition 6.2  (Treatment Positivity (Overlap)) \\(a_i \\\\mathcal{} \\implies d(a_i, w_i) \\\\mathcal{}\\) \n\\(w \\\\mathcal{W}\\), \\(\\mathcal{}\\) denotes support \n\\(\\mid W = w_i \\quad \\forall = 1, \\ldots n\\).identification assumptions satisfied, Dı́az van der Laan (2018) provide \nefficient influence function respect nonparametric model\n\\(\\mathcal{M}\\) \n\\[\\begin{equation*}\\label{eqn:eif}\n  D(P_0)(o) = H(, w)({y - \\overline{Q}_{Y,0}(, w)}) +\n  \\overline{Q}_{Y,0}(d(, w), w) - \\Psi(P_0),\n\\end{equation*}\\]\nauxiliary covariate \\(H(,w)\\) may expressed\n\\[\\begin{equation*}\\label{eqn:aux_covar_full}\n  H(,w) = \\mathbb{}(+ \\delta < u(w)) \\frac{g_{,0}(- \\delta \\mid w)}\n    {g_{,0}(\\mid w)} + \\mathbb{}(+ \\delta \\geq u(w)),\n\\end{equation*}\\]\nmay reduced \n\\[\\begin{equation*}\\label{eqn:aux_covar_simple}\n  H(,w) = \\frac{g_{,0}(- \\delta \\mid w)}{g_{,0}(\\mid w)} + 1\n\\end{equation*}\\]\ncase treatment limits arise conditioning\n\\(W\\), .e., \\(A_i \\(u(w) - \\delta, u(w))\\).identification assumptions satisfied, Dı́az van der Laan (2018) provide \nefficient influence function respect nonparametric model\n\\(\\mathcal{M}\\) \n\\[\\begin{equation*}\\label{eqn:eif}\n  D(P_0)(o) = H(, w)({y - \\overline{Q}_{Y,0}(, w)}) +\n  \\overline{Q}_{Y,0}(d(, w), w) - \\Psi(P_0),\n\\end{equation*}\\]\nauxiliary covariate \\(H(,w)\\) may expressed\n\\[\\begin{equation*}\\label{eqn:aux_covar_full}\n  H(,w) = \\mathbb{}(+ \\delta < u(w)) \\frac{g_{,0}(- \\delta \\mid w)}\n    {g_{,0}(\\mid w)} + \\mathbb{}(+ \\delta \\geq u(w)),\n\\end{equation*}\\]\nmay reduced \n\\[\\begin{equation*}\\label{eqn:aux_covar_simple}\n  H(,w) = \\frac{g_{,0}(- \\delta \\mid w)}{g_{,0}(\\mid w)} + 1\n\\end{equation*}\\]\ncase treatment limits arise conditioning\n\\(W\\), .e., \\(A_i \\(u(w) - \\delta, u(w))\\).","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"interpreting-the-causal-effect-of-a-stochastic-intervention","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.3.2 Interpreting the Causal Effect of a Stochastic Intervention","text":"\nFIGURE 5.1: Animation counterfactual outcome changes natural treatment distribution subjected simple stochastic intervention\n","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"estimating-the-causal-effect-of-a-stochastic-intervention-with-tmle3shift","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.4 Estimating the Causal Effect of a Stochastic Intervention with tmle3shift","text":"use tmle3shift construct targeted maximum likelihood (TML) estimator \ncausal effect stochastic treatment regime shifts natural\nvalue treatment based shifting function \\(d(,W)\\). follow\nrecipe provided Dı́az van der Laan (2018), tailored tmle3 framework:Construct initial estimators \\(g_{,n}\\) \\(g_{,0}(, W)\\) \n\\(\\overline{Q}_{Y,n}\\) \\(\\overline{Q}_{Y,0}(, W)\\), perhaps using\ndata-adaptive regression techniques.observation \\(\\), compute estimate \\(H_n(a_i, w_i)\\) \nauxiliary covariate \\(H(a_i,w_i)\\).Estimate parameter \\(\\epsilon\\) logistic regression model\n\\[ \\text{logit}\\overline{Q}_{Y, n, \\epsilon}(, w) =\n\\text{logit}\\overline{Q}_{Y,n}(, w) + \\epsilon H_n(, w),\\]\nalternative regression model incorporating weights.Compute TML estimator \\(\\Psi_n\\) target parameter, defining update\n\\(\\overline{Q}_{Y,n}^{\\star}\\) initial estimate\n\\(\\overline{Q}_{Y, n, \\epsilon_n}\\):\n\\[\\begin{equation*}\\label{eqn:tmle}\n  \\psi_n^{\\star} = \\Psi(P_n^{\\star}) = \\frac{1}{n} \\sum_{= 1}^n\n  \\overline{Q}_{Y,n}^{\\star}(d(A_i, W_i), W_i).\n\\end{equation*}\\]start, let’s load packages ’ll use set seed simulation:1. Construct initial estimators \\(g_{,n}\\) \\(g_{,0}(, W)\\) \n\\(\\overline{Q}_{Y,n}\\) \\(\\overline{Q}_{Y,0}(, W)\\).need estimate two components likelihood order construct \nTML estimator.outcome regression, \\(\\overline{Q}_{Y,n}\\), simple regression\nform \\(\\mathbb{E}[Y \\mid ,W]\\).second estimate treatment mechanism, \\(g_{,n}\\),\n.e., generalized propensity score. case continuous\nintervention node \\(\\), quantity takes form \\(p(\\mid W)\\), \nconditional density treatment, given covariates. Generally speaking,\nconditional density estimation challenging problem received\nmuch attention literature. estimate treatment mechanism, \nmust make use learning algorithms specifically suited conditional\ndensity estimation; list learners may extracted sl3 \nusing sl3_list_learners():proceed, ’ll select two learners, Lrnr_haldensify using\nhighly adaptive lasso conditional density estimation, based \nalgorithm given Dı́az van der Laan (2011) implemented Hejazi, Benkeser, van der Laan (2020), \nsemiparametric location-scale conditional density estimators implemented \nsl3 package. Super Learner may \nconstructed pooling estimates modified conditional\ndensity estimation techniques.Finally, construct learner_list object use constructing TML\nestimator target parameter interest:","code":"\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(tmle3shift)\nset.seed(429153)\n# learners used for conditional expectation regression\nmean_learner <- Lrnr_mean$new()\nfglm_learner <- Lrnr_glm_fast$new()\nxgb_learner <- Lrnr_xgboost$new(nrounds = 200)\nsl_regression_learner <- Lrnr_sl$new(\n  learners = list(mean_learner, fglm_learner, xgb_learner)\n)sl3_list_learners(\"density\")\n[1] \"Lrnr_density_discretize\"     \"Lrnr_density_hse\"           \n[3] \"Lrnr_density_semiparametric\" \"Lrnr_haldensify\"            \n[5] \"Lrnr_solnp_density\"         \n# learners used for conditional densities (i.e., generalized propensity score)\nhaldensify_learner <- Lrnr_haldensify$new(\n  n_bins = c(3, 5),\n  lambda_seq = exp(seq(-1, -10, length = 200))\n)\n# semiparametric density estimator based on homoscedastic errors (HOSE)\nhose_learner_xgb <- make_learner(Lrnr_density_semiparametric,\n  mean_learner = xgb_learner\n)\n# semiparametric density estimator based on heteroscedastic errors (HESE)\nhese_learner_xgb_fglm <- make_learner(Lrnr_density_semiparametric,\n  mean_learner = xgb_learner,\n  var_learner = fglm_learner\n)\n# SL for the conditional treatment density\nsl_density_learner <- Lrnr_sl$new(\n  learners = list(haldensify_learner, hose_learner_xgb,\n                  hese_learner_xgb_fglm),\n  metalearner = Lrnr_solnp_density$new()\n)\nQ_learner <- sl_regression_learner\ng_learner <- sl_density_learner\nlearner_list <- list(Y = Q_learner, A = g_learner)"},{"path":"stochastic-treatment-regimes-optional.html","id":"simulate-data-1","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.4.1 Simulate Data","text":"now observed data structure (data) specification role\nvariable data set plays nodes directed acyclic\ngraph (DAG) via nonparametric structural equation models (NPSEMs).start, initialize specification TMLE parameter \ninterest (tmle3_Spec tlverse nomenclature) simply calling\ntmle_shift. specify argument shift_val = 0.5 initializing \ntmle3_Spec object communicate ’re interested shift \\(0.5\\) \nscale treatment \\(\\) – , specify \\(\\delta = 0.5\\).seen , tmle_shift specification object (like tmle3_Spec\nobjects) store data specific analysis interest. Later,\n’ll see passing data object directly tmle3 wrapper function,\nalongside instantiated tmle_spec, serve construct tmle3_Task\nobject internally (see tmle3 documentation details).","code":"# simulate simple data for tmle-shift sketch\nn_obs <- 1000 # number of observations\ntx_mult <- 2 # multiplier for the effect of W = 1 on the treatment\n\n## baseline covariates -- simple, binary\nW <- replicate(2, rbinom(n_obs, 1, 0.5))\n\n## create treatment based on baseline W\nA <- rnorm(n_obs, mean = tx_mult * W, sd = 1)\n\n## create outcome as a linear function of A, W + white noise\nY <- rbinom(n_obs, 1, prob = plogis(A + W))\n\n# organize data and nodes for tmle3\ndata <- data.table(W, A, Y)\nsetnames(data, c(\"W1\", \"W2\", \"A\", \"Y\"))\nnode_list <- list(W = c(\"W1\", \"W2\"), A = \"A\", Y = \"Y\")\nhead(data)\n   W1 W2        A Y\n1:  1  1  3.58065 1\n2:  1  0  3.20718 1\n3:  1  1  1.03584 1\n4:  0  0 -0.65785 1\n5:  1  1  3.01990 1\n6:  1  1  2.78031 1\n# initialize a tmle specification\ntmle_spec <- tmle_shift(\n  shift_val = 0.5,\n  shift_fxn = shift_additive,\n  shift_fxn_inv = shift_additive_inv\n)"},{"path":"stochastic-treatment-regimes-optional.html","id":"targeted-estimation-of-stochastic-interventions-effects","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.4.2 Targeted Estimation of Stochastic Interventions Effects","text":"print method resultant tmle_fit object conveniently displays \nresults computing TML estimator.","code":"tmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)\n\nIter: 1 fn: 1342.0479    Pars:  0.56750948 0.00000282 0.43248770\nIter: 2 fn: 1342.0479    Pars:  0.5675095127 0.0000005883 0.4324898990\nsolnp--> Completed in 2 iterations\ntmle_fit\nA tmle3_Fit that took 1 step(s)\n   type         param init_est tmle_est       se   lower   upper\n1:  TSM E[Y_{A=NULL}]  0.79815  0.79755 0.012914 0.77224 0.82286\n   psi_transformed lower_transformed upper_transformed\n1:         0.79755           0.77224           0.82286"},{"path":"stochastic-treatment-regimes-optional.html","id":"stochastic-interventions-over-a-grid-of-counterfactual-shifts","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.5 Stochastic Interventions over a Grid of Counterfactual Shifts","text":"Consider arbitrary scalar \\(\\delta\\) defines counterfactual outcome\n\\(\\psi_n = \\mathbb{E}_P \\overline{Q}_{Y,n}(d(, W), W)\\), , simplicity,\nlet \\(d(, W) = + \\delta\\). simplified expression auxiliary\ncovariate TML estimator \\(\\psi\\) \\(H_n := \\frac{g^{\\star}_{,0}(\\mid w)}{g_{,0}(\\mid w)}\\), \\(g^{\\star}_{,0}(\\mid w)\\) defines \ntreatment mechanism stochastic intervention implemented. \nmanner, can specify grid shifts \\(\\delta\\) define set \nstochastic intervention policies priori manner.Consider arbitrary scalar \\(\\delta\\) defines counterfactual outcome\n\\(\\psi_n = \\mathbb{E}_P \\overline{Q}_{Y,n}(d(, W), W)\\), , simplicity,\nlet \\(d(, W) = + \\delta\\). simplified expression auxiliary\ncovariate TML estimator \\(\\psi\\) \\(H_n := \\frac{g^{\\star}_{,0}(\\mid w)}{g_{,0}(\\mid w)}\\), \\(g^{\\star}_{,0}(\\mid w)\\) defines \ntreatment mechanism stochastic intervention implemented. \nmanner, can specify grid shifts \\(\\delta\\) define set \nstochastic intervention policies priori manner.ascertain whether given choice shift \\(\\delta\\) acceptable, let\nbound \\(C(\\delta) = \\frac{g^{\\star}_{,0}(\\mid w)}{g_{,0}(\\mid w)} \\leq M\\), \\(g^{\\star}_{,0}(\\mid w)\\) function \\(\\delta\\) \npart, \\(M\\) user-specified upper bound \\(C(\\delta)\\). ,\n\\(C(\\delta)\\) measure influence given observation (\nbound ratio conditional densities), provides way \nlimit maximum influence given observation choice \nshift \\(\\delta\\).ascertain whether given choice shift \\(\\delta\\) acceptable, let\nbound \\(C(\\delta) = \\frac{g^{\\star}_{,0}(\\mid w)}{g_{,0}(\\mid w)} \\leq M\\), \\(g^{\\star}_{,0}(\\mid w)\\) function \\(\\delta\\) \npart, \\(M\\) user-specified upper bound \\(C(\\delta)\\). ,\n\\(C(\\delta)\\) measure influence given observation (\nbound ratio conditional densities), provides way \nlimit maximum influence given observation choice \nshift \\(\\delta\\).purpose using shift practice, present software\nprovides functions shift_additive_bounded \nshift_additive_bounded_inv, define variation shift:\n\\[\\begin{equation}\n  \\delta(, w) =\n    \\begin{cases}\n      \\delta, & C(\\delta) \\leq M \\\\\n      0, \\text{otherwise} \\\\\n    \\end{cases},\n\\end{equation}\\]\ncorresponds intervention natural value treatment\ngiven observational unit shifted value \\(\\delta\\) case \nratio intervened density \\(g^{\\star}_{,0}(\\mid w)\\) natural\ndensity \\(g_{,0}(\\mid w)\\) (, \\(C(\\delta)\\)) exceed bound\n\\(M\\). case ratio \\(C(\\delta)\\) exceeds bound \\(M\\), \nstochastic intervention policy apply given unit \nremain natural value treatment \\(\\).purpose using shift practice, present software\nprovides functions shift_additive_bounded \nshift_additive_bounded_inv, define variation shift:\n\\[\\begin{equation}\n  \\delta(, w) =\n    \\begin{cases}\n      \\delta, & C(\\delta) \\leq M \\\\\n      0, \\text{otherwise} \\\\\n    \\end{cases},\n\\end{equation}\\]\ncorresponds intervention natural value treatment\ngiven observational unit shifted value \\(\\delta\\) case \nratio intervened density \\(g^{\\star}_{,0}(\\mid w)\\) natural\ndensity \\(g_{,0}(\\mid w)\\) (, \\(C(\\delta)\\)) exceed bound\n\\(M\\). case ratio \\(C(\\delta)\\) exceeds bound \\(M\\), \nstochastic intervention policy apply given unit \nremain natural value treatment \\(\\).","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"initializing-vimshift-through-its-tmle3_spec","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.5.1 Initializing vimshift through its tmle3_Spec","text":"start, initialize specification TMLE parameter \ninterest (called tmle3_Spec tlverse nomenclature) simply calling\ntmle_shift. specify argument shift_grid = seq(-1, 1, = 1)\ninitializing tmle3_Spec object communicate ’re interested\nassessing mean counterfactual outcome grid shifts -1, 0, 1 scale treatment \\(\\).","code":"\n# what's the grid of shifts we wish to consider?\ndelta_grid <- seq(from = -1, to = 1, by = 1)\n\n# initialize a tmle specification\ntmle_spec <- tmle_vimshift_delta(\n  shift_grid = delta_grid,\n  max_shifted_ratio = 2\n)"},{"path":"stochastic-treatment-regimes-optional.html","id":"targeted-estimation-of-stochastic-intervention-effects","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.5.2 Targeted Estimation of Stochastic Intervention Effects","text":"One may walk step--step procedure fitting TML estimator\nmean counterfactual outcome shift grid, using \nmachinery exposed tmle3 R package, \nsimply invoke tmle3 wrapper function fit series TML estimators\n(one parameter defined grid delta) single function call.\nconvenience, choose latter:Remark: print method resultant tmle_fit object conveniently\ndisplays results computing TML estimator.","code":"tmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)\n\nIter: 1 fn: 1340.1739    Pars:  0.59196 0.12955 0.27849\nIter: 2 fn: 1340.1739    Pars:  0.59196 0.12956 0.27848\nsolnp--> Completed in 2 iterations\ntmle_fit\nA tmle3_Fit that took 1 step(s)\n         type          param init_est tmle_est        se   lower   upper\n1:        TSM  E[Y_{A=NULL}]  0.61155  0.61513 0.0140867 0.58752 0.64274\n2:        TSM  E[Y_{A=NULL}]  0.73900  0.73900 0.0138950 0.71177 0.76623\n3:        TSM  E[Y_{A=NULL}]  0.84892  0.84841 0.0111864 0.82649 0.87034\n4: MSM_linear MSM(intercept)  0.73316  0.73418 0.0125521 0.70958 0.75878\n5: MSM_linear     MSM(slope)  0.11869  0.11664 0.0044021 0.10802 0.12527\n   psi_transformed lower_transformed upper_transformed\n1:         0.61513           0.58752           0.64274\n2:         0.73900           0.71177           0.76623\n3:         0.84841           0.82649           0.87034\n4:         0.73418           0.70958           0.75878\n5:         0.11664           0.10802           0.12527"},{"path":"stochastic-treatment-regimes-optional.html","id":"inference-with-marginal-structural-models","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.5.3 Inference with Marginal Structural Models","text":"Since consider estimating mean counterfactual outcome \\(\\psi_n\\) \nseveral values intervention \\(\\delta\\), taken aforementioned\n\\(\\delta\\)-grid, one approach obtaining inference single summary measure\nestimated quantities involves leveraging working marginal structural\nmodels (MSMs). Summarizing estimates \\(\\psi_n\\) working MSM allows\ninference trend imposed \\(\\delta\\)-grid evaluated via \nsimple hypothesis test parameter working MSM. Letting\n\\(\\psi_{\\delta}(P_0)\\) mean outcome shift \\(\\delta\\) \ntreatment, \\(\\vec{\\psi}_{\\delta} = (\\psi_{\\delta}: \\delta)\\) \ncorresponding estimators \\(\\vec{\\psi}_{n, \\delta} = (\\psi_{n, \\delta}: \\delta)\\).\n, let \\(\\beta(\\vec{\\psi}_{\\delta}) = \\phi((\\psi_{\\delta}: \\delta))\\). \nstraightforward application delta method (discussed previously), may\nwrite efficient influence function MSM parameter \\(\\beta\\) terms \nEIFs corresponding point estimates. Based , inference\nworking MSM rather straightforward. wit, limiting distribution\n\\(m_{\\beta}(\\delta)\\) may expressed \\[\\sqrt{n}(\\beta_n - \\beta_0) \\N(0,\n\\Sigma),\\] \\(\\Sigma\\) empirical covariance matrix \n\\(\\text{EIF}_{\\beta}(O)\\).","code":"tmle_fit$summary[4:5, ]\n         type          param init_est tmle_est        se   lower   upper\n1: MSM_linear MSM(intercept)  0.73316  0.73418 0.0125521 0.70958 0.75878\n2: MSM_linear     MSM(slope)  0.11869  0.11664 0.0044021 0.10802 0.12527\n   psi_transformed lower_transformed upper_transformed\n1:         0.73418           0.70958           0.75878\n2:         0.11664           0.10802           0.12527"},{"path":"stochastic-treatment-regimes-optional.html","id":"directly-targeting-the-msm-parameter-beta","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.5.4 Directly Targeting the MSM Parameter \\(\\beta\\)","text":"Note , working MSM fit individual TML estimates \nmean counterfactual outcome given value shift \\(\\delta\\) \nsupplied grid. parameter interest \\(\\beta\\) MSM \nasymptotically linear (, fact, TML estimator) consequence \nconstruction individual TML estimators. smaller samples, may \nprudent perform TML estimation procedure targets parameter\n\\(\\beta\\) directly, opposed constructing several independently\ntargeted TML estimates. approach constructing estimator \nproposed sequel.Suppose simple working MSM \\(\\mathbb{E}Y_{g^0_{\\delta}} = \\beta_0 + \\beta_1 \\delta\\), TML estimator targeting \\(\\beta_0\\) \\(\\beta_1\\) may \nconstructed \n\\[\\overline{Q}_{n, \\epsilon}(,W) = \\overline{Q}_n(,W) + \\epsilon (H_1(g),\nH_2(g),\\] \\(\\delta\\), \\(H_1(g)\\) auxiliary covariate \n\\(\\beta_0\\) \\(H_2(g)\\) auxiliary covariate \\(\\beta_1\\).construct targeted maximum likelihood estimator directly targets \nparameters working marginal structural model, may use \ntmle_vimshift_msm Spec (instead tmle_vimshift_delta Spec \nappears ):","code":"# initialize a tmle specification\ntmle_msm_spec <- tmle_vimshift_msm(\n  shift_grid = delta_grid,\n  max_shifted_ratio = 2\n)\n\n# fit the TML estimator and examine the results\ntmle_msm_fit <- tmle3(tmle_msm_spec, data, node_list, learner_list)\n\nIter: 1 fn: 1338.2186    Pars:  0.621115419 0.000009161 0.378875424\nIter: 2 fn: 1338.2186    Pars:  0.6211155417 0.0000003725 0.3788840858\nsolnp--> Completed in 2 iterations\ntmle_msm_fit\nA tmle3_Fit that took 100 step(s)\n         type          param init_est tmle_est        se   lower   upper\n1: MSM_linear MSM(intercept)  0.73317  0.73345 0.0125938 0.70877 0.75813\n2: MSM_linear     MSM(slope)  0.11842  0.11823 0.0043158 0.10978 0.12669\n   psi_transformed lower_transformed upper_transformed\n1:         0.73345           0.70877           0.75813\n2:         0.11823           0.10978           0.12669"},{"path":"stochastic-treatment-regimes-optional.html","id":"example-with-the-wash-benefits-data","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.5.5 Example with the WASH Benefits Data","text":"complete walk , let’s turn using stochastic interventions \ninvestigate data WASH Benefits trial. start, let’s load \ndata, convert columns class numeric, take quick look itNext, specify NPSEM via node_list object. example analysis,\n’ll consider outcome weight--height Z-score (previous\nsections), intervention interest mother’s age time \nchild’s birth, take covariates potential confounders.consider counterfactual weight--height Z-score shifts \nage mother child’s birth, interpret estimates \nparameter?simplify interpretation, consider shift () two years \nmother’s age (.e., \\(\\delta = \\{-2, 0, 2\\}\\)); setting, stochastic\nintervention correspond policy advocating potential mothers\ndefer accelerate plans child two calendar years, possibly\nimplemented deployment encouragement design.First, let’s try simple upward shift just two years:examine effect modification approach looked previous chapters,\n’ll estimate effect shift \\(\\delta = 2\\) stratifying \nmother’s education level (momedu, categorical variable three levels).\n, augment initialized tmle3_Spec object like soPrior running analysis, ’ll modify learner_list object \ncreated include just one semiparametric location-scale conditional\ndensity estimators, fitting estimators much faster \ncomputationally intensive approach implemented \nhaldensify package\n(Hejazi, Benkeser, van der Laan 2020).Now ’re ready construct TML estimate shift parameter \n\\(\\delta = 2\\), stratified across levels variable interest:next example, ’ll use variable importance strategy considering\ngrid stochastic interventions evaluate weight--height Z-score\nshift mother’s age two years (\\(\\delta = -2\\)) \ntwo years (\\(\\delta = 2\\)), incrementing single year two. \n, simply initialize Spec tmle_vimshift_delta similar \nprevious example:made preparations, ’re now ready estimate \ncounterfactual mean weight--height Z-score small grid \nshifts mother’s age child’s birth. Just , \nsimple call tmle3 wrapper function:","code":"washb_data <- fread(\"https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data_subset.csv\", stringsAsFactors = TRUE)\nwashb_data <- washb_data[!is.na(momage) & !is.na(momheight), ]\nhead(washb_data, 3)\n     whz          tr fracode month aged    sex momage         momedu momheight\n1: -0.94 Handwashing  N06505     7  237   male     21 Primary (1-5y)    146.00\n2: -1.13     Control  N06505     8  310 female     26   No education    148.90\n3: -1.61     Control  N06524     3  162   male     25 Primary (1-5y)    153.75\n       hfiacat Nlt18 Ncomp watmin elec floor walls roof asset_wardrobe\n1: Food Secure     1    25      2    1     0     1    1              0\n2: Food Secure     1     7      4    1     0     0    1              0\n3: Food Secure     0    15      2    0     0     1    1              0\n   asset_table asset_chair asset_khat asset_chouki asset_tv asset_refrig\n1:           1           0          0            1        0            0\n2:           1           1          0            1        0            0\n3:           1           0          1            1        0            0\n   asset_bike asset_moto asset_sewmach asset_mobile\n1:          0          0             0            0\n2:          0          0             0            1\n3:          0          0             0            0\nnode_list <- list(\n  W = names(washb_data)[!(names(washb_data) %in%\n    c(\"whz\", \"momage\"))],\n  A = \"momage\", Y = \"whz\"\n)\n# initialize a tmle specification for just a single delta shift\nwashb_shift_spec <- tmle_shift(\n  shift_val = 2,\n  shift_fxn = shift_additive,\n  shift_fxn_inv = shift_additive_inv\n)\n# initialize effect modification specification around previous specification\nwashb_shift_strat_spec <-  tmle_stratified(washb_shift_spec)\n# learners used for conditional density regression (i.e., propensity score),\n# but we need to turn on cross-validation for this conditional density learner\nhose_learner_xgb_cv <- Lrnr_cv$new(\n  learner = hose_learner_xgb,\n  full_fit = TRUE\n)\n\n# modify learner list, using existing SL for Q fit\nlearner_list <- list(Y = Q_learner, A = hose_learner_xgb_cv)# fit stratified TMLE\nstrat_node_list <- copy(node_list)\nstrat_node_list$W <- setdiff(strat_node_list$W,\"momedu\")\nstrat_node_list$V <- \"momedu\"\nwashb_shift_strat_fit <- tmle3(washb_shift_strat_spec, washb_data, strat_node_list,\n                               learner_list)\nwashb_shift_strat_fit\nA tmle3_Fit that took 1 step(s)\n             type                             param init_est tmle_est       se\n1:            TSM                     E[Y_{A=NULL}] -0.57002 -0.59407 0.058813\n2: stratified TSM  E[Y_{A=NULL}] | V=Primary (1-5y) -0.60764 -0.70818 0.075504\n3: stratified TSM    E[Y_{A=NULL}] | V=No education -0.65186 -0.85500 0.125652\n4: stratified TSM E[Y_{A=NULL}] | V=Secondary (>5y) -0.52289 -0.44745 0.094402\n      lower    upper psi_transformed lower_transformed upper_transformed\n1: -0.70934 -0.47880        -0.59407          -0.70934          -0.47880\n2: -0.85616 -0.56020        -0.70818          -0.85616          -0.56020\n3: -1.10128 -0.60873        -0.85500          -1.10128          -0.60873\n4: -0.63248 -0.26243        -0.44745          -0.63248          -0.26243\n# initialize a tmle specification for the variable importance parameter\nwashb_vim_spec <- tmle_vimshift_delta(\n  shift_grid = seq(from = -2, to = 2, by = 1),\n  max_shifted_ratio = 2\n)washb_tmle_fit <- tmle3(washb_vim_spec, washb_data, node_list, learner_list)\nwashb_tmle_fit\nA tmle3_Fit that took 1 step(s)\n         type          param   init_est   tmle_est        se      lower\n1:        TSM  E[Y_{A=NULL}] -0.5614487 -0.5606145 0.0459489 -0.6506728\n2:        TSM  E[Y_{A=NULL}] -0.5633714 -0.5603987 0.0464541 -0.6514470\n3:        TSM  E[Y_{A=NULL}] -0.5652941 -0.5652941 0.0466314 -0.6566901\n4:        TSM  E[Y_{A=NULL}] -0.5672168 -0.5688442 0.0468844 -0.6607360\n5:        TSM  E[Y_{A=NULL}] -0.5691396 -0.5653575 0.0479076 -0.6592546\n6: MSM_linear MSM(intercept) -0.5652941 -0.5641018 0.0466522 -0.6555384\n7: MSM_linear     MSM(slope) -0.0019227 -0.0017931 0.0015528 -0.0048366\n        upper psi_transformed lower_transformed upper_transformed\n1: -0.4705563      -0.5606145        -0.6506728        -0.4705563\n2: -0.4693503      -0.5603987        -0.6514470        -0.4693503\n3: -0.4738982      -0.5652941        -0.6566901        -0.4738982\n4: -0.4769525      -0.5688442        -0.6607360        -0.4769525\n5: -0.4714603      -0.5653575        -0.6592546        -0.4714603\n6: -0.4726652      -0.5641018        -0.6555384        -0.4726652\n7:  0.0012503      -0.0017931        -0.0048366         0.0012503"},{"path":"stochastic-treatment-regimes-optional.html","id":"exercises","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.6 Exercises","text":"Exercise 6.1  Set sl3 library algorithms Super Learner simple,\ninterpretable library use new library estimate counterfactual\nmean mother’s age child’s birth (momage) shift \\(\\delta = 0\\).\ncounterfactual mean equate terms observed data?Exercise 6.2  Describe two (equivalent) ways causal effects stochastic\ninterventions may interpreted.Exercise 6.3  Using grid values shift parameter \\(\\delta\\) (e.g., \\(\\{-1, 0, +1\\}\\)),\nrepeat analysis variable interest (momage), summarizing \ntrend sequence shifts using marginal structural model.Exercise 6.4  either grid shifts example preceding exercises \nestimated (3) , plot resultant estimates respective\ncounterfactual shifts. Graphically add scatterplot line slope \nintercept equivalent MSM fit individual TML estimates.Exercise 6.5  marginal structural model used summarize trend along \nsequence shifts previously help contextualize estimated effect \nsingle shift? , access estimates across several shifts \nmarginal structural model parameters allow us richly interpret \nfindings?","code":""},{"path":"causal-mediation-analysis.html","id":"causal-mediation-analysis","chapter":"7 Causal Mediation Analysis","heading":"7 Causal Mediation Analysis","text":"Nima HejaziBased tmle3mediate R\npackage.","code":""},{"path":"causal-mediation-analysis.html","id":"learning-objectives-5","chapter":"7 Causal Mediation Analysis","heading":"7.1 Learning Objectives","text":"Understand can estimate natural direct indirect effects binary treatment using tmle3mediate R package.Know learn tmle3mediate package.","code":""},{"path":"causal-mediation-analysis.html","id":"causal-mediation-analysis-1","chapter":"7 Causal Mediation Analysis","heading":"7.2 Causal Mediation Analysis","text":"time briefly introduce parameters. can find information relevant handbook chapterIn presence post-treatment intermediate variables affected \nexposure (, mediators), path-specific effects allow complex,\nmechanistic relationships teased apart. causal effects \nwide interest definition identification object \nstudy statistics nearly century – indeed, earliest examples \nmodern causal mediation analysis can traced back work path analysis\n(???).recent decades, renewed interest resulted \nformulation novel direct indirect effects within potential\noutcomes nonparametric structural equation modeling frameworks\n(???; ???; ???; ???; Pearl 2009). Generally, indirect effect (IE) portion \ntotal effect found work mediating variables, direct\neffect (DE) encompasses components total effect, including\neffect treatment directly outcome effect\npaths explicitly involving mediators. mechanistic\nknowledge conveyed direct indirect effects can used improve\nunderstanding treatments may efficacious.","code":""},{"path":"causal-mediation-analysis.html","id":"data-structure-and-notation-1","chapter":"7 Causal Mediation Analysis","heading":"7.3 Data Structure and Notation","text":"Let us return familiar sample \\(n\\) units \\(O_1, \\ldots, O_n\\), \nnow consider slightly complex data structure \\(O = (W, , Z, Y)\\) \ngiven observational unit. , \\(W\\) represents vector observed\ncovariates, \\(\\) binary continuous treatment, \\(Y\\) binary continuous\noutcome; new post-treatment variable \\(Z\\) represents (possibly\nmultivariate) set mediators. Avoiding assumptions unsupported background\nscientific knowledge, assume \\(O \\sim P_0 \\\\M\\), \\(\\M\\) \nnonparametric statistical model places assumptions form \ndata-generating distribution \\(P_0\\).preceding chapters, structural causal model (SCM) (Pearl 2009)\nhelps formalize definition counterfactual variables:\n\\[\\begin{align}\n  W &= f_W(U_W) \\\\ \\nonumber\n  &= f_A(W, U_A) \\\\ \\nonumber\n  Z &= f_Z(W, , U_Z) \\\\ \\nonumber\n  Y &= f_Y(W, , Z, U_Y).\n  \\tag{7.1}\n\\end{align}\\]\nset equations\nconstitutes mechanistic model generating observed data \\(O\\); furthermore,\nSCM encodes several fundamental assumptions. Firstly, implicit\ntemporal ordering: \\(W\\) occurs first, depending exogenous factors \\(U_W\\);\n\\(\\) happens next, based \\(W\\) exogenous factors \\(U_A\\); come \nmediators \\(Z\\), depend \\(\\), \\(W\\), another set exogenous factors\n\\(U_Z\\); finally appears outcome \\(Y\\). assume neither access set\nexogenous factors \\(\\{U_W, U_A, U_Z, U_Y\\}\\) knowledge forms \ndeterministic generating functions \\(\\{f_W, f_A, f_Z, f_Y\\}\\). practice, \navailable knowledge data-generating experiment incorporated\nmodel – example, data randomized controlled trial\n(RCT), form \\(f_A\\) may known. SCM corresponds following DAG:factorizing likelihood data \\(O\\), can express \\(p_0\\), \ndensity \\(O\\) respect product measure, evaluated \nparticular observation \\(o\\), terms several orthogonal components:\n\\[\\begin{align}\n  p_0(o) = &q_{0,Y}(y \\mid Z = z, = , W = w) \\\\ \\nonumber\n    &q_{0,Z}(z \\mid = , W = w) \\\\ \\nonumber\n    &g_{0,}(\\mid W = w) \\\\ \\nonumber\n    &q_{0,W}(w).\\\\ \\nonumber\n  \\tag{7.2}\n\\end{align}\\]\nEquation (7.2), \\(q_{0, Y}\\) \nconditional density \\(Y\\) given \\(\\{Z, , W\\}\\), \\(q_{0, Z}\\) conditional\ndensity \\(Z\\) given \\(\\{, W\\}\\), \\(g_{0, }\\) conditional density \\(\\)\ngiven \\(W\\), \\(q_{0, W}\\) marginal density \\(W\\). convenience \nconsistency notation, define \\(\\overline{Q}_Y(Z, , W) := \\E[Y \\mid Z, , W]\\) \\(g(\\mid W) := \\P(\\mid W)\\) (.e., propensity score).","code":""},{"path":"causal-mediation-analysis.html","id":"defining-the-natural-direct-and-indirect-effects","chapter":"7 Causal Mediation Analysis","heading":"7.4 Defining the Natural Direct and Indirect Effects","text":"","code":""},{"path":"causal-mediation-analysis.html","id":"decomposing-the-average-treatment-effect","chapter":"7 Causal Mediation Analysis","heading":"7.4.1 Decomposing the Average Treatment Effect","text":"natural direct indirect effects arise decomposition ATE:\n\\[\\begin{align*}\n  \\E[Y(1) - Y(0)] =\n    &\\underbrace{\\E[Y(1, Z(0)) - Y(0, Z(0))]}_{\\text{NDE}} \\\\ &+\n    \\underbrace{\\E[Y(1, Z(1)) - Y(1, Z(0))]}_{\\text{NIE}}.\n\\end{align*}\\]\nparticular, natural indirect effect (NIE) measures effect \ntreatment \\(\\\\{0, 1\\}\\) outcome \\(Y\\) mediators \\(Z\\), \nnatural direct effect (NDE) measures effect treatment \noutcome pathways.Identifiability results necessary assumptions parameters thoroughly covered handbook.","code":""},{"path":"causal-mediation-analysis.html","id":"estimating-the-natural-direct-effect","chapter":"7 Causal Mediation Analysis","heading":"7.4.2 Estimating the Natural Direct Effect","text":"NDE defined \n\\[\\begin{align*}\n  \\psi_{\\text{NDE}} =&~\\E[Y(1, Z(0)) - Y(0, Z(0))] \\\\\n  =& \\sum_w \\sum_z\n  [\\underbrace{\\E(Y \\mid = 1, z, w)}_{\\overline{Q}_Y(= 1, z, w)} -\n  \\underbrace{\\E(Y \\mid = 0, z, w)}_{\\overline{Q}_Y(= 0, z, w)}] \\\\\n  &\\times \\underbrace{p(z \\mid = 0, w)}_{q_Z(Z \\mid 0, w))}\n  \\underbrace{p(w)}_{q_W},\n\\end{align*}\\]\nlikelihood factors arise factorization joint\nlikelihood:\n\\[\\begin{equation*}\n  p(w, , z, y) = \\underbrace{p(y \\mid w, , z)}_{q_Y(, W, Z)}\n  \\underbrace{p(z \\mid w, )}_{q_Z(Z \\mid , W)}\n  \\underbrace{p(\\mid w)}_{g(\\mid W)}\n  \\underbrace{p(w)}_{q_W}.\n\\end{equation*}\\]process estimating NDE begins constructing \\(\\overline{Q}_{Y, n}\\),\nestimate conditional mean outcome, given \\(Z\\), \\(\\), \\(W\\).\nestimate conditional mean hand, predictions \nquantities \\(\\overline{Q}_Y(Z, 1, W)\\) (setting \\(= 1\\)) ,\nlikewise, \\(\\overline{Q}_Y(Z, 0, W)\\) (setting \\(= 0\\)) readily obtained. \ndenote difference conditional means \\(\\overline{Q}_{\\text{diff}} = \\overline{Q}_Y(Z, 1, W) - \\overline{Q}_Y(Z, 0, W)\\), \nfunctional parameter data distribution. \\(\\overline{Q}_{\\text{diff}}\\)\ncaptures differences conditional mean \\(Y\\) across contrasts \\(\\).procedure constructing targeted maximum likelihood (TML) estimator \nNDE treats \\(\\overline{Q}_{\\text{diff}}\\) nuisance parameter,\nregressing estimate \\(\\overline{Q}_{\\text{diff}, n}\\) baseline covariates\n\\(W\\), among observations control condition (.e., \n\\(= 0\\) observed); goal step remove part marginal\nimpact \\(Z\\) \\(\\overline{Q}_{\\text{diff}}\\), since covariates \\(W\\) precede\nmediators \\(Z\\) time. Regressing difference \\(W\\) among controls\nrecovers expected \\(\\overline{Q}_{\\text{diff}}\\), setting \nindividuals treated falling control condition \\(= 0\\). \nresidual additive effect \\(Z\\) \\(\\overline{Q}_{\\text{diff}}\\) removed\nTML estimation step using auxiliary (“clever”) covariate,\naccounts mediators \\(Z\\). auxiliary covariate takes form\\[\\begin{equation*}\n  C_Y(q_Z, g)(O) = \\Bigg\\{\\frac{\\mathbb{}(= 1)}{g(1 \\mid W)}\n  \\frac{q_Z(Z \\mid 0, W)}{q_Z(Z \\mid 1, W)} -\n  \\frac{\\mathbb{}(= 0)}{g(0 \\mid W)} \\Bigg\\} \\ .\n\\end{equation*}\\]\nBreaking , \\(\\mathbb{}(= 1) / g(1 \\mid W)\\) inverse propensity\nscore weight \\(= 1\\) , likewise, \\(\\mathbb{}(= 0) / g(0 \\mid W)\\) \ninverse propensity score weight \\(= 0\\). middle term ratio \nconditional densities mediator control (\\(= 0\\)) treatment\n(\\(= 1\\)) conditions (n.b., recall mediator positivity condition ).subtle appearance ratio conditional densities concerning –\ntools estimate quantities sparse statistics literature\n(Dı́az van der Laan 2011; Hejazi, Benkeser, van der Laan 2020), unfortunately, problem still\ncomplicated (computationally taxing) \\(Z\\) high-dimensional. \nratio conditional densities required, convenient\nre-parametrization may achieved, ,\n\\[\\begin{equation*}\n  \\frac{p(= 0 \\mid Z, W)}{g(0 \\mid W)}\n  \\frac{g(1 \\mid W)}{p(= 1 \\mid Z, W)} \\ .\n\\end{equation*}\\]\nGoing forward, denote re-parameterized conditional probability\nfunctional \\(e(\\mid Z, W) := p(\\mid Z, W)\\). re-parameterization\ntechnique used (???), (???),\n(???), (???), (???) similar\ncontexts. reformulation particularly useful fact reduces\nestimation problem one requiring estimation conditional\nmeans, opening door use wide range machine learning\nalgorithms, discussed previously.Underneath hood, mean outcome difference \\(\\overline{Q}_{\\text{diff}}\\)\n\\(e(\\mid Z, W)\\), conditional probability \\(\\) given \\(Z\\) \\(W\\), \nused constructing auxiliary covariate TML estimation. nuisance\nparameters play important role bias-correcting update step TML\nestimation procedure.","code":""},{"path":"causal-mediation-analysis.html","id":"estimating-the-natural-indirect-effect","chapter":"7 Causal Mediation Analysis","heading":"7.4.3 Estimating the Natural Indirect Effect","text":"Derivation estimation NIE analogous NDE. Recall\nNIE effect \\(\\) \\(Y\\) mediator \\(Z\\).\ncounterfactual quantity, may expressed \\(\\E(Y(Z(1), 1) - \\E(Y(Z(0), 1)\\), corresponds difference conditional mean \\(Y\\)\ngiven \\(= 1\\) \\(Z(1)\\) (values mediator take \\(= 1\\)) \nconditional expectation \\(Y\\) given \\(= 1\\) \\(Z(0)\\) (values \nmediator take \\(= 0\\)).NDE, re-parameterization can used replace \\(q_Z(Z \\mid , W)\\)\n\\(e(\\mid Z, W)\\) estimation process, avoiding estimation \npossibly multivariate conditional density. However, case, mediated\nmean outcome difference, previously computed regressing\n\\(\\overline{Q}_{\\text{diff}}\\) \\(W\\) among control units (\\(= 0\\) \nobserved) instead replaced two-step process. First, \\(\\overline{Q}_Y(Z, 1, W)\\), conditional mean \\(Y\\) given \\(Z\\) \\(W\\) \\(= 1\\), regressed\n\\(W\\), among treated units (.e., \\(= 1\\) observed). ,\nquantity, \\(\\overline{Q}_Y(Z, 1, W)\\) regressed \\(W\\), \ntime among control units (.e., \\(= 0\\) observed). mean\ndifference two functionals data distribution valid\nestimator NIE. can thought additive marginal effect \ntreatment conditional mean \\(Y\\) given \\((W, = 1, Z)\\) \neffect \\(Z\\). , case NIE, estimand\n\\(\\psi_{\\text{NIE}}\\) different, estimation techniques useful \nconstructing efficient estimators NDE come play.","code":""},{"path":"causal-mediation-analysis.html","id":"the-population-intervention-direct-and-indirect-effects","chapter":"7 Causal Mediation Analysis","heading":"7.5 The Population Intervention Direct and Indirect Effects","text":"times, natural direct indirect effects may prove limiting, \neffect definitions based static interventions (.e., setting\n\\(= 0\\) \\(= 1\\)), may unrealistic real-world interventions. \ncases, one may turn instead population intervention direct effect\n(PIDE) population intervention indirect effect (PIIE), based\ndecomposing effect population intervention effect (PIE) \nflexible stochastic interventions (???).previously discussed stochastic interventions considering \nintervene continuous-valued treatments; however, intervention\nschemes may applied manner treatment variables.\nparticular type stochastic intervention well-suited working binary\ntreatments incremental propensity score intervention (IPSI), first\nproposed (???). interventions \ndeterministically set treatment level observed unit fixed\nquantity (.e., setting \\(= 1\\)), instead alter odds receiving \ntreatment fixed amount (\\(0 \\leq \\delta \\leq \\infty\\)) individual.\nparticular, intervention takes form\n\\[\\begin{equation*}\n  g_{\\delta}(1 \\mid w) = \\frac{\\delta g(1 \\mid w)}{\\delta g(1 \\mid w) + 1\n  - g(1\\mid w)},\n\\end{equation*}\\]\nscalar \\(0 < \\delta < \\infty\\) specifies change odds \nreceiving treatment. described (???) context causal\nmediation analysis, identification assumptions required PIDE \nPIIE significantly lax required NDE NIE. \nidentification assumptions include following. Importantly, assumption \ncross-world counterfactual independence required.Definition 7.1  (Conditional Exchangeability Treatment Mediators) Assume \\(\\E\\{Y(, z) \\mid Z, , W\\} = \\E\\{Y(, z) \\mid Z, W\\}~\\forall~(, z) \\\\mathcal{} \\times \\mathcal{Z}W\\). assumption \nstronger implied assumption \\(Y(, z) \\indep (,Z) \\mid W\\),\noriginally proposed (???) identification mediated\neffects among treated. introducing assumption (???) state\n“assumption satisfied pre-exposure \\(W\\) \nrandomized experiment exposure mediators randomized. Thus, \ndirect effect population intervention corresponds contrasts \ntreatment regimes randomized experiment via interventions \\(\\) \\(Z\\),\nunlike natural direct effect average treatment effect\n(???).”Definition 7.2  (Common Support Treatment Mediators) Assume \\(\\text{supp}\\{g_{\\delta}(\\cdot \\mid w)\\} \\subseteq \\text{supp}\\{g(\\cdot \\mid w)\\}~\\forall~w \\\\mathcal{W}\\). assumption \nstandard requires post-intervention value \\(\\) supported\ndata. Note significantly weaker treatment \nmediator positivity conditions required natural direct indirect\neffects, direct consequence using stochastic (rather static)\ninterventions.","code":""},{"path":"causal-mediation-analysis.html","id":"evaluating-the-direct-and-indirect-effects","chapter":"7 Causal Mediation Analysis","heading":"7.6 Evaluating the Direct and Indirect Effects","text":"now turn estimating natural direct indirect effects, well \npopulation intervention direct effect, using WASH Benefits data,\nintroduced earlier chapters. Let’s first load data:’ll next define baseline covariates \\(W\\), treatment \\(\\), mediators \\(Z\\),\noutcome \\(Y\\) nodes NPSEM via “Node List” object:, node_list encodes parents node – example, \\(Z\\) (\nmediators) parents \\(\\) (treatment) \\(W\\) (baseline confounders),\n\\(Y\\) (outcome) parents \\(Z\\), \\(\\), \\(W\\). ’ll also handle \nmissingness data invoking process_missing:’ll now construct ensemble learner using handful popular machine\nlearning algorithms:","code":"\nlibrary(data.table)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(tmle3mediate)\n\n# download data\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)\n\n# make intervention node binary and subsample\nwashb_data <- washb_data[sample(.N, 600), ]\nwashb_data[, tr := as.numeric(tr != \"Control\")]\nnode_list <- list(\n  W = c(\n    \"momage\", \"momedu\", \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\",\n    \"elec\", \"floor\", \"walls\", \"roof\"\n  ),\n  A = \"tr\",\n  Z = c(\"sex\", \"month\", \"aged\"),\n  Y = \"whz\"\n)\nprocessed <- process_missing(washb_data, node_list)\nwashb_data <- processed$data\nnode_list <- processed$node_list\n# SL learners used for continuous data (the nuisance parameter Z)\nenet_contin_learner <- Lrnr_glmnet$new(\n  alpha = 0.5, family = \"gaussian\", nfolds = 3\n)\nlasso_contin_learner <- Lrnr_glmnet$new(\n  alpha = 1, family = \"gaussian\", nfolds = 3\n)\nfglm_contin_learner <- Lrnr_glm_fast$new(family = gaussian())\nmean_learner <- Lrnr_mean$new()\ncontin_learner_lib <- Stack$new(\n  enet_contin_learner, lasso_contin_learner, fglm_contin_learner, mean_learner\n)\nsl_contin_learner <- Lrnr_sl$new(learners = contin_learner_lib)\n\n# SL learners used for binary data (nuisance parameters G and E in this case)\nenet_binary_learner <- Lrnr_glmnet$new(\n  alpha = 0.5, family = \"binomial\", nfolds = 3\n)\nlasso_binary_learner <- Lrnr_glmnet$new(\n  alpha = 1, family = \"binomial\", nfolds = 3\n)\nfglm_binary_learner <- Lrnr_glm_fast$new(family = binomial())\nbinary_learner_lib <- Stack$new(\n  enet_binary_learner, lasso_binary_learner, fglm_binary_learner, mean_learner\n)\nsl_binary_learner <- Lrnr_sl$new(learners = binary_learner_lib)\n\n# create list for treatment and outcome mechanism regressions\nlearner_list <- list(\n  Y = sl_contin_learner,\n  A = sl_binary_learner\n)"},{"path":"causal-mediation-analysis.html","id":"targeted-estimation-of-the-natural-indirect-effect","chapter":"7 Causal Mediation Analysis","heading":"7.6.1 Targeted Estimation of the Natural Indirect Effect","text":"demonstrate calculation NIE , starting instantiating “Spec”\nobject encodes exactly learners use nuisance parameters\n\\(e(\\mid Z, W)\\) \\(\\psi_Z\\). pass Spec object tmle3\nfunction, alongside data, node list (created ), learner list\nindicating machine learning algorithms use estimating nuisance\nparameters based \\(\\) \\(Y\\).Based output, conclude indirect effect treatment\nmediators node_list$Z washb_NIE$summary$tmle_est.","code":"\ntmle_spec_NIE <- tmle_NIE(\n  e_learners = Lrnr_cv$new(lasso_binary_learner, full_fit = TRUE),\n  psi_Z_learners = Lrnr_cv$new(lasso_contin_learner, full_fit = TRUE),\n  max_iter = 1\n)\nwashb_NIE <- tmle3(\n  tmle_spec_NIE, washb_data, node_list, learner_list\n)\nwashb_NIE"},{"path":"causal-mediation-analysis.html","id":"targeted-estimation-of-the-natural-direct-effect","chapter":"7 Causal Mediation Analysis","heading":"7.6.2 Targeted Estimation of the Natural Direct Effect","text":"analogous procedure applies estimation NDE, replacing \nSpec object NIE tmle_spec_NDE define learners NDE\nnuisance parameters:, can draw conclusion direct effect treatment\n(paths involving mediators, node_list$Z) \nwashb_NDE$summary$tmle_est. Note , together, estimates \nnatural direct indirect effects approximately recover average\ntreatment effect, , based estimates NDE NIE, \nATE roughly sum washb_NDE$summary$tmle_est \nwashb_NIE$summary$tmle_est.","code":"\ntmle_spec_NDE <- tmle_NDE(\n  e_learners = Lrnr_cv$new(lasso_binary_learner, full_fit = TRUE),\n  psi_Z_learners = Lrnr_cv$new(lasso_contin_learner, full_fit = TRUE),\n  max_iter = 1\n)\nwashb_NDE <- tmle3(\n  tmle_spec_NDE, washb_data, node_list, learner_list\n)\nwashb_NDE"},{"path":"causal-mediation-analysis.html","id":"exercises-1","chapter":"7 Causal Mediation Analysis","heading":"7.7 Exercises","text":"Exercise 7.1  Examine WASH Benefits dataset choose different set potential\nmediators effect treatment weight--height Z-score. Using\nnewly chosen set mediators (single mediator), estimate natural\ndirect indirect effects. Provide interpretation estimates.Exercise 7.2  Assess whether additivity natural direct indirect effects holds.\nUsing natural direct indirect effects estimated , sum\nrecover ATE?Exercise 7.3  Evaluate whether assumptions required identification natural\ndirect indirect effects plausible WASH Benefits example. \nparticular, position evaluation terms empirical diagnostics \ntreatment mediator positivity.","code":""},{"path":"r6.html","id":"r6","chapter":"8 A Primer on the R6 Class System","heading":"8 A Primer on the R6 Class System","text":"central goal Targeted Learning statistical paradigm estimate\nscientifically relevant parameters realistic (usually nonparametric) models.tlverse designed using basic OOP principles R6 OOP framework.\n’ve tried make easy use tlverse packages without worrying\nmuch OOP, helpful intuition tlverse \nstructured. , briefly outline key concepts OOP. Readers\nfamiliar OOP basics invited skip section.","code":""},{"path":"r6.html","id":"classes-fields-and-methods","chapter":"8 A Primer on the R6 Class System","heading":"8.1 Classes, Fields, and Methods","text":"key concept OOP object, collection data functions\ncorresponds conceptual unit. Objects two main types \nelements:fields, can thought nouns, information object,\nandmethods, can thought verbs, actions object can\nperform.Objects members classes, define specific fields \nmethods . Classes can inherit elements classes (sometimes called\nbase classes) – accordingly, classes similar, exactly \n, can share parts definitions.Many different implementations OOP exist, variations \nconcepts implemented used. R several different implementations,\nincluding S3, S4, reference classes, R6. tlverse uses R6\nimplementation. R6, methods fields class object accessed using\n$ operator. thorough introduction R6, see https://adv-r.hadley.nz/r6.html, Hadley Wickham’s Advanced\nR (Wickham 2014).","code":""},{"path":"r6.html","id":"object-oriented-programming-python-and-r","chapter":"8 A Primer on the R6 Class System","heading":"8.2 Object Oriented Programming: Python and R","text":"OO concepts (classes inherentence) baked Python first\npublished version (version 0.9 1991). contrast, R gets OO “approach”\npredecessor, S, first released 1976. first 15 years, S\nsupport classes, , suddenly, S got two OO frameworks bolted \nrapid succession: informal classes S3 1991, formal classes \nS4 1998. process continues, new OO frameworks periodically\nreleased, try improve lackluster OO support R, reference\nclasses (R5, 2010) R6 (2014). , R6 behaves like Python\nclasses (also like OOP focused languages like C++ Java), including\nmethod definitions part class definitions, allowing objects \nmodified reference.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Baker, Monya. 2016. “Reproducibility Crisis? Nature Survey Lifts Lid Researchers View Crisis Rocking Science Think Help.” Nature 533 (7604): 452–55.Benkeser, David, Mark J van der Laan. 2016. “Highly Adaptive Lasso Estimator.” 2016 IEEE International Conference Data Science Advanced Analytics (DSAA). IEEE. https://doi.org/10.1109/dsaa.2016.93.Breiman, Leo. 1996. “Stacked Regressions.” Machine Learning 24 (1): 49–64.Buckheit, Jonathan B, David L Donoho. 1995. “Wavelab Reproducible Research.” Wavelets Statistics, 55–81. Springer.Coyle, Jeremy R, Nima S Hejazi, Mark J van der Laan. 2020. hal9001: Scalable Highly Adaptive Lasso. https://doi.org/10.5281/zenodo.3558313.Dı́az, Iván, Mark J van der Laan. 2011. “Super Learner Based Conditional Density Estimation Application Marginal Structural Models.” International Journal Biostatistics 7 (1): 1–20.———. 2012. “Population Intervention Causal Effects Based Stochastic Interventions.” Biometrics 68 (2): 541–49.———. 2018. “Stochastic Treatment Regimes.” Targeted Learning Data Science: Causal Inference Complex Longitudinal Studies, 167–80. Springer Science & Business Media.Dudoit, Sandrine, Mark J van der Laan. 2005. “Asymptotics Cross-Validated Risk Estimation Estimator Selection Performance Assessment.” Statistical Methodology 2 (2): 131–54.Haneuse, Sebastian, Andrea Rotnitzky. 2013. “Estimation Effect Interventions Modify Received Treatment.” Statistics Medicine 32 (30): 5260–77.Hejazi, Nima S, David C Benkeser, Mark J van der Laan. 2020. haldensify: Highly Adaptive Lasso Conditional Density Estimation. https://github.com/nhejazi/haldensify. https://doi.org/10.5281/zenodo.3698329.Hejazi, Nima S, Jeremy R Coyle, Mark J van der Laan. 2020. “hal9001: Scalable Highly Adaptive Lasso Regression R.” Journal Open Source Software. https://doi.org/10.21105/joss.02526.Luby, Stephen P, Mahbubur Rahman, Benjamin F Arnold, Leanne Unicomb, Sania Ashraf, Peter J Winch, Christine P Stewart, et al. 2018. “Effects Water Quality, Sanitation, Handwashing, Nutritional Interventions Diarrhoea Child Growth Rural Bangladesh: Cluster Randomised Controlled Trial.” Lancet Global Health 6 (3): e302–e315.Munafò, Marcus R, Brian Nosek, Dorothy VM Bishop, Katherine S Button, Christopher D Chambers, Nathalie Percie Du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J Ware, John PA Ioannidis. 2017. “Manifesto Reproducible Science.” Nature Human Behaviour 1 (1): 0021.Nature Editorial. 2015a. “Scientists Fool — Can Stop.” Nature 526 (7572).———. 2015b. “Let’s Think Cognitive Bias.” Nature 526 (7572).Nosek, Brian , Charles R Ebersole, Alexander C DeHaven, David T Mellor. 2018. “Preregistration Revolution.” Proceedings National Academy Sciences 115 (11): 2600–2606.Pearl, Judea. 2009. Causality: Models, Reasoning, Inference. Cambridge University Press.Peng, Roger. 2015. “Reproducibility Crisis Science: Statistical Counterattack.” Significance 12 (3): 30–32.Polley, Eric C, Mark J van der Laan. 2010. “Super Learner Prediction.” Bepress.Pullenayegum, Eleanor M, Robert W Platt, Melanie Barwick, Brian M Feldman, Martin Offringa, Lehana Thabane. 2016. “Knowledge Translation Biostatistics: Survey Current Practices, Preferences, Barriers Dissemination Uptake New Statistical Methods.” Statistics Medicine 35 (6): 805–18.Stark, Philip B, Andrea Saltelli. 2018. “Cargo-Cult Statistics Scientific Crisis.” Significance 15 (4): 40–43.Stromberg, Arnold, others. 2004. “Write Statistical Software? Case Robust Statistical Methods.” Journal Statistical Software 10 (5): 1–8.Szucs, Denes, John Ioannidis. 2017. “Null Hypothesis Significance Testing Unsuitable Research: Reassessment.” Frontiers Human Neuroscience 11: 390.van der Laan, Mark J, Eric C Polley, Alan E Hubbard. 2007. “Super Learner.” Statistical Applications Genetics Molecular Biology 6 (1).van der Laan, Mark J, Richard JCM Starmans. 2014. “Entering Era Data Science: Targeted Learning Integration Statistics Computational Data Analysis.” Advances Statistics 2014.Wickham, Hadley. 2014. Advanced R. Chapman; Hall/CRC.Young, Jessica G, Miguel Hernán, James M Robins. 2014. “Identification, Estimation Approximation Risk Interventions Depend Natural Value Treatment Using Observational Data.” Epidemiologic Methods 3 (1): 1–19.","code":""}]
